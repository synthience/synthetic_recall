# __init__.py

```py
# synthians_memory_core/__init__.py

"""
Synthians Memory Core - A Unified, Efficient Memory System
Incorporates HPC-QuickRecal, Hyperbolic Geometry, Emotional Intelligence,
Memory Assemblies, and Adaptive Thresholds.
"""

__version__ = "1.0.0"

# Core components
from .synthians_memory_core import SynthiansMemoryCore
from .memory_structures import MemoryEntry, MemoryAssembly
from .hpc_quickrecal import UnifiedQuickRecallCalculator, QuickRecallMode, QuickRecallFactor
from .geometry_manager import GeometryManager, GeometryType
from .emotional_intelligence import EmotionalAnalyzer, EmotionalGatingService
from .memory_persistence import MemoryPersistence
from .adaptive_components import ThresholdCalibrator

__all__ = [
    "SynthiansMemoryCore",
    "MemoryEntry",
    "MemoryAssembly",
    "UnifiedQuickRecallCalculator",
    "QuickRecallMode",
    "QuickRecallFactor",
    "GeometryManager",
    "GeometryType",
    "EmotionalAnalyzer",
    "EmotionalGatingService",
    "MemoryPersistence",
    "ThresholdCalibrator",
]

```

# adaptive_components.py

```py
# synthians_memory_core/adaptive_components.py

import time
import math
from collections import deque
from typing import Dict, Any, Optional

from .custom_logger import logger # Use the shared custom logger

class ThresholdCalibrator:
    """Dynamically calibrates similarity thresholds based on feedback."""

    def __init__(self, initial_threshold: float = 0.75, learning_rate: float = 0.05, window_size: int = 50):
        self.threshold = initial_threshold
        self.learning_rate = learning_rate
        self.feedback_history = deque(maxlen=window_size)
        self.stats = {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0} # Added tn for completeness
        logger.info("ThresholdCalibrator", "Initialized", {"initial": initial_threshold, "lr": learning_rate, "window": window_size})

    def record_feedback(self, similarity_score: float, was_relevant: bool):
        """Record feedback for a retrieved memory."""
        is_above_threshold = similarity_score >= self.threshold

        self.feedback_history.append({
            "score": similarity_score,
            "relevant": was_relevant,
            "predicted_relevant": is_above_threshold,
            "threshold_at_time": self.threshold
        })

        # Update stats based on prediction vs actual relevance
        if is_above_threshold:
            if was_relevant: self.stats['tp'] += 1
            else: self.stats['fp'] += 1
        else:
            if was_relevant: self.stats['fn'] += 1
            else: self.stats['tn'] += 1 # Correctly predicted irrelevant

        # Adjust threshold immediately based on this feedback
        self.adjust_threshold()

    def adjust_threshold(self) -> float:
        """Adjust the similarity threshold based on recent feedback."""
        if len(self.feedback_history) < 10: # Need minimum feedback
            return self.threshold

        # Calculate Precision and Recall from recent history (last N items)
        recent_feedback = list(self.feedback_history)
        recent_tp = sum(1 for f in recent_feedback if f["predicted_relevant"] and f["relevant"])
        recent_fp = sum(1 for f in recent_feedback if f["predicted_relevant"] and not f["relevant"])
        recent_fn = sum(1 for f in recent_feedback if not f["predicted_relevant"] and f["relevant"])

        precision = recent_tp / max(1, recent_tp + recent_fp)
        recall = recent_tp / max(1, recent_tp + recent_fn)

        adjustment = 0.0
        # If precision is low (too many irrelevant items retrieved), increase threshold
        if precision < 0.6 and recall > 0.5: # Avoid penalizing if recall is also low
            adjustment = self.learning_rate * (1.0 - precision) # Stronger increase for lower precision
        # If recall is low (too many relevant items missed), decrease threshold
        elif recall < 0.6 and precision > 0.5: # Avoid penalizing if precision is also low
             adjustment = -self.learning_rate * (1.0 - recall) # Stronger decrease for lower recall

        # Apply adjustment with diminishing returns near bounds
        current_threshold = self.threshold
        if adjustment > 0:
            # Less adjustment as we approach 1.0
            adjustment *= (1.0 - current_threshold)
        else:
             # Less adjustment as we approach 0.0
             adjustment *= current_threshold

        new_threshold = current_threshold + adjustment
        new_threshold = max(0.1, min(0.95, new_threshold)) # Keep within reasonable bounds

        if abs(new_threshold - self.threshold) > 0.001:
            logger.info("ThresholdCalibrator", f"Adjusted threshold: {self.threshold:.3f} -> {new_threshold:.3f}",
                        {"adjustment": adjustment, "precision": precision, "recall": recall})
            self.threshold = new_threshold

        return self.threshold

    def get_current_threshold(self) -> float:
        """Return the current similarity threshold."""
        return self.threshold

    def get_statistics(self) -> dict:
        """Return statistics about calibration performance."""
        total = self.stats['tp'] + self.stats['fp'] + self.stats['fn'] + self.stats['tn']
        precision = self.stats['tp'] / max(1, self.stats['tp'] + self.stats['fp'])
        recall = self.stats['tp'] / max(1, self.stats['tp'] + self.stats['fn'])
        f1 = 2 * precision * recall / max(0.001, precision + recall)

        return {
            "threshold": self.threshold,
            "feedback_count": len(self.feedback_history),
            "true_positives": self.stats['tp'],
            "false_positives": self.stats['fp'],
            "false_negatives": self.stats['fn'],
            "true_negatives": self.stats['tn'],
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }

# Note: AdaptiveBatchScheduler might be overkill if batching is handled externally
# or if the primary interaction pattern doesn't benefit significantly from adaptive batching.
# Keeping ThresholdCalibrator as it's directly related to retrieval relevance.

```

# api\__init__.py

```py


```

# api\client\__init__.py

```py


```

# api\client\client.py

```py
# synthians_memory_core/api/client/client.py

import sys
import json
import asyncio
import numpy as np
from typing import Dict, Any, List, Optional, Union
import aiohttp
import argparse
from datetime import datetime

class SynthiansClient:
    """A simple client for testing the Synthians Memory Core API."""
    
    def __init__(self, base_url: str = "http://localhost:5010"):
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def health_check(self) -> Dict[str, Any]:
        """Check if the server is healthy."""
        async with self.session.get(f"{self.base_url}/health") as response:
            return await response.json()
    
    async def get_stats(self) -> Dict[str, Any]:
        """Get system statistics."""
        async with self.session.get(f"{self.base_url}/stats") as response:
            return await response.json()
    
    async def process_memory(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Process and store a new memory."""
        payload = {
            "content": content,
            "metadata": metadata or {}
        }
        async with self.session.post(
            f"{self.base_url}/process_memory", json=payload
        ) as response:
            return await response.json()
    
    async def retrieve_memories(self, query: str, top_k: int = 5, 
                               user_emotion: Optional[Dict[str, Any]] = None,
                               cognitive_load: float = 0.5,
                               threshold: Optional[float] = None,
                               metadata_filter: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Retrieve relevant memories."""
        payload = {
            "query": query,
            "top_k": top_k,
            "user_emotion": user_emotion,
            "cognitive_load": cognitive_load,
        }
        if threshold is not None:
            payload["threshold"] = threshold
        if metadata_filter is not None:
            payload["metadata_filter"] = metadata_filter
        async with self.session.post(
            f"{self.base_url}/retrieve_memories", json=payload
        ) as response:
            return await response.json()
    
    async def generate_embedding(self, text: str) -> Dict[str, Any]:
        """Generate embedding for text."""
        payload = {"text": text}
        async with self.session.post(
            f"{self.base_url}/generate_embedding", json=payload
        ) as response:
            return await response.json()
    
    async def calculate_quickrecal(self, text: str = None, embedding: List[float] = None, 
                                 context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Calculate QuickRecal score."""
        payload = {
            "text": text,
            "embedding": embedding,
            "context": context or {}
        }
        async with self.session.post(
            f"{self.base_url}/calculate_quickrecal", json=payload
        ) as response:
            return await response.json()
    
    async def analyze_emotion(self, text: str) -> Dict[str, Any]:
        """Analyze emotional content of text."""
        payload = {"text": text}
        async with self.session.post(
            f"{self.base_url}/analyze_emotion", json=payload
        ) as response:
            return await response.json()
    
    async def provide_feedback(self, memory_id: str, similarity_score: float, 
                             was_relevant: bool) -> Dict[str, Any]:
        """Provide feedback on memory retrieval."""
        payload = {
            "memory_id": memory_id,
            "similarity_score": similarity_score,
            "was_relevant": was_relevant
        }
        async with self.session.post(
            f"{self.base_url}/provide_feedback", json=payload
        ) as response:
            return await response.json()
    
    async def detect_contradictions(self, threshold: float = 0.75) -> Dict[str, Any]:
        """Detect potential contradictions in memories."""
        async with self.session.post(
            f"{self.base_url}/detect_contradictions?threshold={threshold}"
        ) as response:
            return await response.json()
    
    async def get_memory_by_id(self, memory_id: str) -> Dict[str, Any]:
        """Retrieve a specific memory by its ID."""
        async with self.session.get(
            f"{self.base_url}/memory/{memory_id}"
        ) as response:
            return await response.json()
    
    async def process_transcription(self, text: str, audio_metadata: Dict[str, Any] = None, 
                                  importance: float = None) -> Dict[str, Any]:
        """Process a transcription with audio features."""
        payload = {
            "text": text,
            "audio_metadata": audio_metadata or {},
        }
        if importance is not None:
            payload["importance"] = importance
            
        async with self.session.post(
            f"{self.base_url}/process_transcription", json=payload
        ) as response:
            return await response.json()


async def run_tests(client: SynthiansClient):
    """Run a series of tests to verify API functionality."""
    print("Running API tests...\n")
    
    try:
        print("1. Health Check Test")
        health = await client.health_check()
        print(f"Health check result: {json.dumps(health, indent=2)}\n")
        
        print("2. Stats Test")
        stats = await client.get_stats()
        print(f"Stats result: {json.dumps(stats, indent=2)}\n")
        
        print("3. Embedding Generation Test")
        embed_resp = await client.generate_embedding("Testing the embedding generation API")
        if embed_resp["success"]:
            embed_dim = len(embed_resp["embedding"])
            print(f"Successfully generated embedding with dimension {embed_dim}\n")
        else:
            print(f"Failed to generate embedding: {embed_resp.get('error')}\n")
        
        print("4. QuickRecal Calculation Test")
        qr_resp = await client.calculate_quickrecal(text="Testing the QuickRecal API")
        print(f"QuickRecal result: {json.dumps(qr_resp, indent=2)}\n")
        
        print("5. Emotion Analysis Test")
        emotion_resp = await client.analyze_emotion("I am feeling very happy today")
        print(f"Emotion analysis result: {json.dumps(emotion_resp, indent=2)}\n")
        
        print("6. Memory Processing Test")
        mem_resp = await client.process_memory(
            content="This is a test memory created at " + datetime.now().isoformat(),
            metadata={"source": "test_client", "importance": 0.8}
        )
        print(f"Memory processing result: {json.dumps(mem_resp, indent=2)}\n")
        
        if mem_resp.get("success"):
            memory_id = mem_resp.get("memory_id")
            
            print("7. Memory Retrieval Test")
            retrieve_resp = await client.retrieve_memories("test memory", top_k=3)
            print(f"Memory retrieval result: {json.dumps(retrieve_resp, indent=2)}\n")
            
            print("8. Feedback Test")
            feedback_resp = await client.provide_feedback(
                memory_id=memory_id,
                similarity_score=0.85,
                was_relevant=True
            )
            print(f"Feedback result: {json.dumps(feedback_resp, indent=2)}\n")
        
        print("9. Contradiction Detection Test")
        contradict_resp = await client.detect_contradictions(threshold=0.7)
        print(f"Contradiction detection result: {json.dumps(contradict_resp, indent=2)}\n")
        
        print("All tests completed.")
    except Exception as e:
        print(f"Test failed with error: {str(e)}")


async def main():
    parser = argparse.ArgumentParser(description="Synthians Memory Core API Client")
    parser.add_argument("--url", default="http://localhost:5010", help="API server URL")
    parser.add_argument("--action", choices=["test", "health", "stats", "add", "retrieve", "embedding", "quickrecal", "emotion"], 
                       default="test", help="Action to perform")
    parser.add_argument("--query", help="Query for memory retrieval")
    parser.add_argument("--content", help="Content for memory processing or analysis")
    parser.add_argument("--metadata", help="JSON metadata string for memory processing")
    parser.add_argument("--top_k", type=int, default=5, help="Number of results to return for memory retrieval")
    parser.add_argument("--cognitive_load", type=float, default=0.5, help="Cognitive load for memory retrieval")
    parser.add_argument("--threshold", type=float, help="Threshold for memory retrieval")
    
    args = parser.parse_args()
    
    async with SynthiansClient(base_url=args.url) as client:
        if args.action == "test":
            await run_tests(client)
        
        elif args.action == "health":
            result = await client.health_check()
            print(json.dumps(result, indent=2))
        
        elif args.action == "stats":
            result = await client.get_stats()
            print(json.dumps(result, indent=2))
        
        elif args.action == "add" and args.content:
            metadata = {}
            if args.metadata:
                try:
                    metadata = json.loads(args.metadata)
                except json.JSONDecodeError:
                    print("Error: metadata must be valid JSON")
                    return
            
            result = await client.process_memory(content=args.content, metadata=metadata)
            print(json.dumps(result, indent=2))
        
        elif args.action == "retrieve" and args.query:
            result = await client.retrieve_memories(
                query=args.query, 
                top_k=args.top_k,
                cognitive_load=args.cognitive_load,
                threshold=args.threshold if hasattr(args, 'threshold') and args.threshold is not None else None
            )
            print(json.dumps(result, indent=2))
        
        elif args.action == "embedding" and args.content:
            result = await client.generate_embedding(text=args.content)
            print(json.dumps(result, indent=2))
        
        elif args.action == "quickrecal" and args.content:
            result = await client.calculate_quickrecal(text=args.content)
            print(json.dumps(result, indent=2))
        
        elif args.action == "emotion" and args.content:
            result = await client.analyze_emotion(text=args.content)
            print(json.dumps(result, indent=2))
        
        else:
            print("Invalid action or missing required arguments")
            parser.print_help()


if __name__ == "__main__":
    asyncio.run(main())

```

# api\client\test_metadata.py

```py
import asyncio
import json
import sys
import time
from datetime import datetime
from typing import Dict, Any, List, Optional

# Import the client class directly from client module
from synthians_memory_core.api.client.client import SynthiansClient

async def test_metadata_synthesis():
    """Test the metadata synthesis capabilities of the memory system."""
    print("\n=== Testing Metadata Synthesis ===\n")
    
    async with SynthiansClient() as client:
        # 1. Process a memory with specific emotional content
        print("\n1. Creating memory with emotional content...")
        happy_memory = await client.process_memory(
            content="I am feeling incredibly happy and joyful today. It's a wonderful day and everything is going great!",
            metadata={
                "source": "metadata_test",
                "importance": 0.9,
                "test_type": "positive_emotion"
            }
        )
        print(f"Happy memory result: {json.dumps(happy_memory, indent=2)}")
        
        # 2. Process a memory with negative emotional content
        print("\n2. Creating memory with negative emotional content...")
        sad_memory = await client.process_memory(
            content="I'm feeling quite sad and disappointed today. Things aren't going well and I'm frustrated.",
            metadata={
                "source": "metadata_test",
                "importance": 0.7,
                "test_type": "negative_emotion"
            }
        )
        print(f"Sad memory result: {json.dumps(sad_memory, indent=2)}")
        
        # 3. Process a memory with technical content
        print("\n3. Creating memory with technical/complex content...")
        tech_memory = await client.process_memory(
            content="The quantum computational paradigm leverages superposition and entanglement to perform calculations that would be infeasible on classical computers. The fundamental unit is the qubit, which can exist in multiple states simultaneously.",
            metadata={
                "source": "metadata_test",
                "importance": 0.8,
                "test_type": "complex_content"
            }
        )
        print(f"Technical memory result: {json.dumps(tech_memory, indent=2)}")
        
        # 4. Retrieve memories and check if metadata is preserved
        print("\n4. Retrieving memories to verify metadata...")
        # First try with default parameters
        retrieve_resp = await client.retrieve_memories(
            "test metadata synthesis", 
            top_k=5
        )
        print(f"Default retrieval results: {json.dumps(retrieve_resp, indent=2)}")
        
        # Try again with a lowered threshold to bypass ThresholdCalibrator
        print("\n4b. Retrieving with lowered threshold...")
        retrieve_with_threshold = await client.retrieve_memories(
            "test metadata synthesis", 
            top_k=5,
            threshold=0.4  # Explicitly lower the threshold well below our ~0.66 scores
        )
        print(f"Retrieval with threshold=0.4: {json.dumps(retrieve_with_threshold, indent=2)}")
        
        # Try with exact memory IDs to force retrieval
        print("\n4c. Retrieving by exact memory IDs...")
        memory_ids = [
            happy_memory.get("memory_id"),
            sad_memory.get("memory_id"),
            tech_memory.get("memory_id")
        ]
        # Filter out any None values
        memory_ids = [mid for mid in memory_ids if mid]
        
        if memory_ids:
            memory_by_id = await client.retrieve_memory_by_id(memory_ids[0])
            print(f"Retrieved by ID: {json.dumps(memory_by_id, indent=2)}")
            
            # Try direct query of each test type
            print("\n4d. Retrieving with direct test type queries...")
            for test_type in ["positive_emotion", "negative_emotion", "complex_content"]:
                test_query = await client.retrieve_memories(
                    test_type,  # Use the test_type as the query
                    top_k=1,
                    threshold=0.4,
                    user_emotion=None  # Bypass emotional gating
                )
                print(f"Query '{test_type}' results: {json.dumps(test_query, indent=2)}")
        
        # 5. Verify key metadata fields in each memory
        print("\n5. Validating metadata fields...")
        memories = retrieve_resp.get("memories", [])
        
        validation_results = []
        for memory in memories:
            metadata = memory.get("metadata", {})
            validation = {
                "id": memory.get("id"),
                "metadata_schema_version": metadata.get("metadata_schema_version"),
                "has_timestamp": "timestamp" in metadata,
                "has_timestamp_iso": "timestamp_iso" in metadata,
                "has_time_of_day": "time_of_day" in metadata,
                "has_dominant_emotion": "dominant_emotion" in metadata,
                "has_emotional_intensity": "emotional_intensity" in metadata,
                "has_complexity_estimate": "complexity_estimate" in metadata,
                "has_embedding_metadata": all(key in metadata for key in ["embedding_valid", "embedding_dim"])
            }
            validation_results.append(validation)
        
        print(f"Validation results: {json.dumps(validation_results, indent=2)}")
        
        # Summary
        print("\n=== Metadata Synthesis Test Summary ===\n")
        if validation_results:
            success = all(result.get("has_timestamp") and 
                         result.get("has_dominant_emotion") and 
                         result.get("has_complexity_estimate") 
                         for result in validation_results)
            if success:
                print("✅ SUCCESS: All memories have proper metadata synthesis")
            else:
                print("❌ FAILURE: Some memories are missing key metadata fields")
        else:
            print("❓ INCONCLUSIVE: No memories were retrieved for validation")

def main():
    """Run the metadata synthesis test."""
    asyncio.run(test_metadata_synthesis())

if __name__ == "__main__":
    main()

```

# api\server.py

```py
# synthians_memory_core/api/server.py

import asyncio
import os
import time
import logging
import numpy as np
from typing import Dict, Any, List, Optional, Union
from fastapi import FastAPI, HTTPException, BackgroundTasks, Path, Depends, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from contextlib import asynccontextmanager
import uvicorn
import json
from datetime import datetime
import sys
import importlib.util
import subprocess

# Import the unified memory core
from synthians_memory_core import SynthiansMemoryCore
from synthians_memory_core.custom_logger import logger
from synthians_memory_core.emotion_analyzer import EmotionAnalyzer
from synthians_memory_core.utils.transcription_feature_extractor import TranscriptionFeatureExtractor
from synthians_memory_core.interruption import InterruptionAwareMemoryHandler
from synthians_memory_core.memory_core.trainer_integration import TrainerIntegrationManager, SequenceEmbeddingsResponse, UpdateQuickRecalScoreRequest

# Optional: Import sentence_transformers for embedding generation if not moved to GeometryManager
from sentence_transformers import SentenceTransformer

# Define request/response models using Pydantic
class ProcessMemoryRequest(BaseModel):
    """Request model for processing a new memory."""
    content: str
    embedding: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None
    analyze_emotion: Optional[bool] = Field(default=True, description="Whether to analyze emotions in the content")

class ProcessMemoryResponse(BaseModel):
    """Response model for memory processing."""
    success: bool
    memory_id: Optional[str] = None
    quickrecal_score: Optional[float] = None
    embedding: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class RetrieveMemoriesRequest(BaseModel):
    query: str
    query_embedding: Optional[List[float]] = None
    top_k: int = 5
    user_emotion: Optional[Union[Dict[str, Any], str]] = None
    cognitive_load: float = 0.5
    threshold: Optional[float] = None

class RetrieveMemoriesResponse(BaseModel):
    success: bool
    memories: List[Dict[str, Any]] = []
    error: Optional[str] = None

class GenerateEmbeddingRequest(BaseModel):
    text: str

class GenerateEmbeddingResponse(BaseModel):
    success: bool
    embedding: Optional[List[float]] = None
    dimension: Optional[int] = None
    error: Optional[str] = None

class QuickRecalRequest(BaseModel):
    embedding: Optional[List[float]] = None
    text: Optional[str] = None
    context: Optional[Dict[str, Any]] = None

class QuickRecalResponse(BaseModel):
    success: bool
    quickrecal_score: Optional[float] = None
    factors: Optional[Dict[str, float]] = None
    error: Optional[str] = None

class EmotionRequest(BaseModel):
    text: str

class EmotionResponse(BaseModel):
    success: bool
    emotions: Optional[Dict[str, float]] = None
    dominant_emotion: Optional[str] = None
    error: Optional[str] = None

class FeedbackRequest(BaseModel):
    memory_id: str
    similarity_score: float
    was_relevant: bool

class FeedbackResponse(BaseModel):
    success: bool
    new_threshold: Optional[float] = None
    error: Optional[str] = None

# Models for the transcription endpoint
class TranscriptionRequest(BaseModel):
    """Request model for processing transcription data."""
    text: str = Field(..., description="The transcribed text")
    audio_metadata: Optional[Dict[str, Any]] = Field(None, description="Optional metadata about the audio source")
    embedding: Optional[List[float]] = Field(None, description="Optional pre-computed embedding for the transcription")
    memory_id: Optional[str] = Field(None, description="Optional memory ID if updating an existing memory")
    importance: Optional[float] = Field(None, description="Optional importance score for the memory (0-1)")
    force_update: bool = Field(False, description="Force update if memory ID exists")

class TranscriptionResponse(BaseModel):
    """Response model for processed transcription data."""
    success: bool = Field(..., description="Whether the operation was successful")
    memory_id: Optional[str] = Field(None, description="ID of the created/updated memory")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Extracted metadata from the transcription")
    embedding: Optional[List[float]] = Field(None, description="Embedding generated for the transcription")
    error: Optional[str] = Field(None, description="Error message if operation failed")

class GetMemoryResponse(BaseModel):
    """Response model for memory retrieval."""
    success: bool
    memory: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# App lifespan for initialization/cleanup
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup app resources."""
    # Startup Logic
    logger.info("API", "Starting Synthians Memory Core API server...")
    
    # Set startup time
    app.state.startup_time = time.time()
    
    # Run GPU setup script to detect GPU and install appropriate FAISS package
    try:
        logger.info("API", "Checking for GPU availability and setting up FAISS...")
        # Get the path to gpu_setup.py
        current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        gpu_setup_path = os.path.join(current_dir, "gpu_setup.py")
        
        if os.path.exists(gpu_setup_path):
            logger.info("API", f"Running GPU setup script from: {gpu_setup_path}")
            # Run the setup script as a subprocess
            result = subprocess.run([sys.executable, gpu_setup_path], 
                                    capture_output=True, text=True, check=False)
            
            if result.returncode == 0:
                logger.info("API", f"GPU setup completed successfully: {result.stdout.strip()}")
            else:
                logger.warning("API", f"GPU setup failed: {result.stderr.strip()}")
                logger.info("API", "Continuing with CPU-only FAISS")
        else:
            logger.warning("API", f"GPU setup script not found at {gpu_setup_path}")
    except Exception as e:
        logger.error("API", f"Error during GPU setup: {str(e)}")
        logger.info("API", "Continuing with CPU-only FAISS")
    
    # Create core instance on startup
    app.state.memory_core = SynthiansMemoryCore()
    await app.state.memory_core.initialize()
    
    # Initialize emotion analysis model
    try:
        logger.info("API", "Initializing emotion analyzer...")
        # Use the new EmotionAnalyzer class
        app.state.emotion_analyzer = EmotionAnalyzer()
        logger.info("API", "Emotion analyzer initialized")
    except Exception as e:
        logger.error("API", f"Failed to initialize emotion analyzer: {str(e)}")
        app.state.emotion_analyzer = None
    
    # Initialize transcription feature extractor
    try:
        logger.info("API", "Initializing transcription feature extractor...")
        # Create the extractor with the emotion_analyzer
        app.state.transcription_extractor = TranscriptionFeatureExtractor(
            emotion_analyzer=app.state.emotion_analyzer
        )
        logger.info("API", "Transcription feature extractor initialized")
    except Exception as e:
        logger.error("API", f"Failed to initialize transcription feature extractor: {str(e)}")
        app.state.transcription_extractor = None
        
    # Initialize trainer integration manager
    try:
        logger.info("API", "Initializing trainer integration manager...")
        app.state.trainer_integration = TrainerIntegrationManager(
            memory_core=app.state.memory_core
        )
        logger.info("API", "Trainer integration manager initialized")
    except Exception as e:
        logger.error("API", f"Failed to initialize trainer integration manager: {str(e)}")
        app.state.trainer_integration = None
    
    # Initialize embedding model
    try:
        model_name = os.environ.get("EMBEDDING_MODEL", "all-mpnet-base-v2")
        logger.info("API", f"Loading embedding model: {model_name}")
        
        # Try to load the model, download if not available
        try:
            app.state.embedding_model = SentenceTransformer(model_name)
            logger.info("API", f"Embedding model {model_name} loaded successfully")
        except Exception as model_error:
            # If the model doesn't exist, it might need to be downloaded
            if "No such file or directory" in str(model_error) or "not found" in str(model_error).lower():
                logger.warning("API", f"Model {model_name} not found locally, attempting to download...")
                from sentence_transformers import util as st_util
                # Force download from Hugging Face
                app.state.embedding_model = SentenceTransformer(model_name, use_auth_token=None)
                logger.info("API", f"Successfully downloaded and loaded model {model_name}")
            else:
                # Re-raise if it's not a file-not-found error
                raise
    except Exception as e:
        logger.error("API", f"Failed to load embedding model: {str(e)}")
        app.state.embedding_model = None
    
    # Complete initialization
    logger.info("API", "Synthians Memory Core API server started")
    
    # Yield control to FastAPI
    yield
    
    # Shutdown Logic
    logger.info("API", "Shutting down Synthians Memory Core API server...")
    # Clean up resources
    try:
        if hasattr(app.state, 'memory_core'):
            await app.state.memory_core.cleanup()
    except Exception as e:
        logger.error("API", f"Error during cleanup: {str(e)}")
    
    logger.info("API", "Synthians Memory Core API server shut down")

# Create the FastAPI app with lifespan
app = FastAPI(
    title="Synthians Memory Core API",
    description="Unified API for memory, embeddings, QuickRecal, and emotion analysis",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict to your frontend domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Helper Functions ---

# Generate embedding using the loaded model
async def generate_embedding(text: str) -> np.ndarray:
    """Generate embedding for text using the sentence transformer model."""
    if not text:
        logger.warning("generate_embedding", "Empty text provided for embedding generation")
        # Return a zero vector of appropriate dimension
        embedding_dim = app.state.memory_core.config.get('embedding_dim', 768)
        return np.zeros(embedding_dim, dtype=np.float32)
    
    try:
        # Use the embedding model from app state
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            None, lambda: app.state.embedding_model.encode(text)
        )
        return embedding
    except Exception as e:
        logger.error("generate_embedding", f"Error generating embedding: {str(e)}")
        # Return a zero vector as fallback
        embedding_dim = app.state.memory_core.config.get('embedding_dim', 768)
        return np.zeros(embedding_dim, dtype=np.float32)

# --- API Endpoints ---

@app.get("/")
async def root():
    return {"message": "Synthians Memory Core API"}

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    try:
        uptime = time.time() - app.state.startup_time
        # Use _memories instead of memories to match the updated attribute name
        memory_count = len(app.state.memory_core._memories)
        assembly_count = len(app.state.memory_core.assemblies)
        return {
            "status": "healthy",
            "uptime_seconds": uptime,
            "memory_count": memory_count,
            "assembly_count": assembly_count,
            "version": "1.0.0"  # Add version information
        }
    except Exception as e:
        logger.error("health_check", f"Health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/stats")
async def get_stats():
    """Get system statistics."""
    try:
        uptime = time.time() - app.state.startup_time
        # Get vector index stats
        vector_index_stats = {
            "count": app.state.memory_core.vector_index.count(),
            "id_mappings": len(app.state.memory_core.vector_index.id_to_index),
            "index_type": app.state.memory_core.vector_index.config.get('index_type', 'Unknown')
        }
        
        return {
            "success": True,  # Add success field
            "api_server": {
                "uptime_seconds": uptime,
                "memory_count": len(app.state.memory_core._memories),
                "embedding_dim": app.state.memory_core.config.get('embedding_dim', 768),
                "geometry": app.state.memory_core.config.get('geometry', 'hyperbolic'),
                "model": os.environ.get('EMBEDDING_MODEL', 'sentence-transformers/all-mpnet-base-v2')
            },
            "memory": {
                "total_memories": len(app.state.memory_core._memories),
                "total_assemblies": len(app.state.memory_core.assemblies),
                "storage_path": app.state.memory_core.config.get('storage_path', '/app/memory/stored/synthians'),
                "threshold": app.state.memory_core.config.get('contradiction_threshold', 0.75),
            },
            "vector_index": vector_index_stats
        }
    except Exception as e:
        logger.error("get_stats", f"Error retrieving stats: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/process_memory", response_model=ProcessMemoryResponse)
async def process_memory(request: ProcessMemoryRequest, background_tasks: BackgroundTasks):
    """Process and store a new memory."""
    try:
        logger.info("process_memory", "Processing new memory request")
        # Validate input
        if not request.content and not request.embedding and not request.metadata:
            raise HTTPException(status_code=400, detail="No memory content provided")
            
        # Tracking for current request (all fields start as None)
        embedding = None
        generated_text = None
        memory_id = None
        emotion_data = None
        
        # Handle case where embedding is provided but in dict format
        if request.embedding is not None:
            if isinstance(request.embedding, dict):
                logger.warning("process_memory", f"Received embedding as dict type, attempting to extract vector")
                try:
                    # Try common dict formats
                    if 'embedding' in request.embedding and isinstance(request.embedding['embedding'], list):
                        embedding = request.embedding['embedding']
                        logger.info("process_memory", "Successfully extracted embedding from dict['embedding']")
                    elif 'vector' in request.embedding and isinstance(request.embedding['vector'], list):
                        embedding = request.embedding['vector']
                        logger.info("process_memory", "Successfully extracted embedding from dict['vector']")
                    elif 'value' in request.embedding and isinstance(request.embedding['value'], list):
                        embedding = request.embedding['value']
                        logger.info("process_memory", "Successfully extracted embedding from dict['value']")
                    else:
                        keys = list(request.embedding.keys()) if hasattr(request.embedding, 'keys') else 'unknown'
                        logger.error("process_memory", f"Could not extract embedding from dict with keys: {keys}")
                        embedding = None
                except Exception as e:
                    logger.error("process_memory", f"Error extracting embedding from dict: {str(e)}")
                    embedding = None
            else:
                # Normal list embedding
                embedding = request.embedding
                
        # Step 1: Generate embedding if needed
        if request.content and (embedding is None) and hasattr(app.state, 'embedding_model'):
            try:
                # Generate embedding
                logger.info("process_memory", "Generating embedding from text")
                loop = asyncio.get_event_loop()
                embedding_list = await loop.run_in_executor(
                    None, 
                    lambda: app.state.embedding_model.encode([request.content])
                )
                # Convert numpy array to Python list to avoid array boolean issues
                if embedding_list is not None and len(embedding_list) > 0:
                    embedding = embedding_list[0].tolist()
                    logger.info("process_memory", f"Generated embedding with {len(embedding)} dimensions")
                else:
                    embedding = None
                    logger.warning("process_memory", "Failed to generate embedding - empty result")
            except Exception as embed_error:
                logger.error("process_memory", f"Embedding generation error: {str(embed_error)}")
                embedding = None
                
        # Step 2: Perform emotion analysis if requested
        if request.analyze_emotion and request.content:
            try:
                logger.info("process_memory", "Performing emotion analysis")
                
                # Use our EmotionAnalyzer directly for the analysis
                if hasattr(app.state, 'emotion_analyzer') and app.state.emotion_analyzer is not None:
                    # Use the emotion analyzer
                    logger.debug("process_memory", "Using emotion analyzer for analysis")
                    emotion_data = await app.state.emotion_analyzer.analyze(request.content)
                else:
                    # Fallback: Call the analyze_emotion endpoint
                    logger.debug("process_memory", "Using analyze_emotion endpoint fallback")
                    emotion_response = await analyze_emotion(request.content)
                    if emotion_response.success:
                        emotion_data = {
                            "emotions": emotion_response.emotions,
                            "dominant_emotion": emotion_response.dominant_emotion
                        }
                
                logger.info("process_memory", f"Emotion analysis complete: {emotion_data.get('dominant_emotion') if emotion_data else 'None'}")
            except Exception as emotion_error:
                logger.error("process_memory", f"Emotion analysis error: {str(emotion_error)}")
                # Continue without emotion data
                
        # Step 3: Process the memory through the core
        try:
            # Prepare metadata with emotion data if available
            metadata = request.metadata or {}
            
            # Add timestamp to metadata
            metadata['timestamp'] = time.time()
            
            # Add emotion data to metadata if available
            if emotion_data:
                metadata['emotional_context'] = emotion_data
            
            # If we don't have an embedding at this point but have content, create a zero-embedding
            # This is a fallback to ensure the memory core can process the request
            if (embedding is None) and request.content:
                logger.warning("process_memory", "No embedding generated or provided. Creating zero-embedding as fallback.")
                # Create a zero-embedding with the default dimension
                embedding_dim = app.state.memory_core.config.get('embedding_dim', 768)
                embedding = [0.0] * embedding_dim
            
            # Validate embedding for NaN/Inf values and handle dimension mismatches
            if embedding is not None:
                try:
                    # Check for NaN/Inf values
                    if any(not np.isfinite(val) for val in embedding):
                        logger.warning("process_memory", "Found NaN/Inf values in embedding. Replacing with zeros.")
                        embedding = [0.0 if not np.isfinite(val) else val for val in embedding]
                    
                    # Ensure correct dimensionality
                    expected_dim = app.state.memory_core.config.get('embedding_dim', 768)
                    actual_dim = len(embedding)
                    
                    if actual_dim != expected_dim:
                        logger.warning("process_memory", f"Dimension mismatch: expected {expected_dim}, got {actual_dim}. Aligning to expected dimension.")
                        if actual_dim < expected_dim:
                            # Pad with zeros if too small
                            embedding = embedding + [0.0] * (expected_dim - actual_dim)
                        else:
                            # Truncate if too large
                            embedding = embedding[:expected_dim]
                except Exception as val_error:
                    logger.error("process_memory", f"Error validating embedding: {str(val_error)}")
                    # Continue with original embedding
            
            # Call the memory core to process the memory
            logger.info("process_memory", "Calling memory core to process memory")
            
            result = await app.state.memory_core.process_new_memory(
                content=request.content,
                embedding=embedding,
                metadata=metadata
            )
            
            memory_id = result.id if result else None
            quickrecal_score = result.quickrecal_score if result else None
            logger.info("process_memory", f"Memory processed successfully with ID: {memory_id}")
            
            # Return response with results
            return ProcessMemoryResponse(
                success=True,
                memory_id=memory_id,
                quickrecal_score=quickrecal_score,
                embedding=embedding,
                metadata=metadata
            )
            
        except Exception as core_error:
            logger.error("process_memory", f"Memory core processing error: {str(core_error)}")
            raise HTTPException(status_code=500, detail=f"Memory processing failed: {str(core_error)}")
    
    except Exception as e:
        logger.error("process_memory", f"Process memory error: {str(e)}")
        import traceback
        logger.error("process_memory", traceback.format_exc())
        
        return ProcessMemoryResponse(
            success=False,
            error=str(e)
        )


@app.post("/retrieve_memories", response_model=RetrieveMemoriesResponse)
async def retrieve_memories(request: RetrieveMemoriesRequest):
    """Retrieve relevant memories."""
    try:
        # Add debug logging
        logger.info("retrieve_memories", f"Received request: query='{request.query}', top_k={request.top_k}, threshold={request.threshold}")
        logger.debug(f"API retrieve_memories: Received request with threshold={request.threshold} (type: {type(request.threshold)})") # Log received value with type
        
        # Convert user_emotion from dict to string if needed
        user_emotion_str = None
        if request.user_emotion:
            if isinstance(request.user_emotion, dict) and 'dominant_emotion' in request.user_emotion:
                user_emotion_str = request.user_emotion['dominant_emotion']
            elif isinstance(request.user_emotion, str):
                user_emotion_str = request.user_emotion
        
        # Retrieve memories with updated parameters - fully keyword-based to avoid positional argument confusion
        retrieve_result = await app.state.memory_core.retrieve_memories(
            query=request.query,
            top_k=request.top_k,
            threshold=request.threshold,  # Use threshold from request if provided
            user_emotion=user_emotion_str,
            metadata_filter=request.metadata_filter if hasattr(request, 'metadata_filter') else None,
            search_strategy=request.search_strategy if hasattr(request, 'search_strategy') else None
        )
        
        # Add detailed response debugging
        memories = retrieve_result.get('memories', [])
        logger.debug(f"API endpoint: Retrieved {len(memories)} memories from core")
        if memories:
            logger.debug(f"API endpoint: First memory ID = {memories[0].get('id')}")
        
        response = RetrieveMemoriesResponse(
            success=retrieve_result.get('success', False),
            memories=memories,
            error=retrieve_result.get('error')
        )
        
        # Final API response check
        logger.debug(f"API endpoint: Final response will contain {len(response.memories)} memories")
        
        return response
    except Exception as e:
        logger.error("retrieve_memories", f"Error: {str(e)}")
        import traceback
        logger.error("retrieve_memories", traceback.format_exc())
        return RetrieveMemoriesResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate_embedding", response_model=GenerateEmbeddingResponse)
async def embedding_endpoint(request: GenerateEmbeddingRequest):
    """Generate embedding for text."""
    try:
        embedding = await generate_embedding(request.text)
        return GenerateEmbeddingResponse(
            success=True,
            embedding=embedding.tolist(),
            dimension=len(embedding)
        )
    except Exception as e:
        logger.error("generate_embedding", f"Error: {str(e)}")
        return GenerateEmbeddingResponse(
            success=False,
            error=str(e)
        )

@app.post("/calculate_quickrecal", response_model=QuickRecalResponse)
async def calculate_quickrecal(request: QuickRecalRequest):
    """Calculate QuickRecal score for an embedding or text."""
    try:
        # Generate embedding if text is provided but embedding is not
        embedding = None
        if request.embedding is None and request.text is not None:
            # Generate embedding directly
            embedding = await generate_embedding(request.text)
        elif request.embedding is not None:
            embedding = np.array(request.embedding, dtype=np.float32)
        else:
            return QuickRecalResponse(
                success=False,
                error="Either embedding or text must be provided"
            )
        
        if embedding is None:
            return QuickRecalResponse(
                success=False,
                error="Failed to generate embedding"
            )
            
        # Prepare context with text if provided
        context = request.context or {'timestamp': time.time()}
        if request.text:
            context['text'] = request.text
            
        # Calculate QuickRecal score - use synchronous method to avoid asyncio issues
        try:
            if hasattr(app.state.memory_core.quick_recal, 'calculate'):
                quickrecal_score = await app.state.memory_core.quick_recal.calculate(embedding, context=context)
            else:
                logger.warning("calculate_quickrecal", "No calculate method found, using fallback")
                quickrecal_score = 0.5  # Default fallback score
        except RuntimeError as re:
            if "asyncio.run()" in str(re):
                # Handle asyncio runtime error by using synchronous version
                logger.warning("calculate_quickrecal", f"Asyncio runtime error: {str(re)}. Using synchronous method.")
                if hasattr(app.state.memory_core.quick_recal, 'calculate_sync'):
                    quickrecal_score = app.state.memory_core.quick_recal.calculate_sync(embedding, context=context)
                else:
                    logger.error("calculate_quickrecal", "No synchronous fallback method available.")
                    quickrecal_score = 0.5  # Default fallback score
            else:
                raise re
        
        # Get factor scores if available
        factors = None
        if hasattr(app.state.memory_core.quick_recal, 'get_last_factor_scores'):
            factors = app.state.memory_core.quick_recal.get_last_factor_scores()
        
        return QuickRecalResponse(
            success=True,
            quickrecal_score=quickrecal_score,
            factors=factors
        )
    except Exception as e:
        logger.error("calculate_quickrecal", f"Error: {str(e)}")
        return QuickRecalResponse(
            success=False,
            error=str(e)
        )

@app.post("/analyze_emotion", response_model=EmotionResponse)
async def analyze_emotion(request: EmotionRequest):
    """Analyze emotional content of text."""
    try:
        # Get text from the request
        text = request.text
            
        # Ensure text is a string
        if not isinstance(text, str):
            return EmotionResponse(
                success=False,
                error="Text must be a string"
            )
        
        # Use our EmotionAnalyzer if available
        if hasattr(app.state, 'emotion_analyzer') and app.state.emotion_analyzer is not None:
            # Get analysis results from the analyzer
            result = await app.state.emotion_analyzer.analyze(text)
            
            return EmotionResponse(
                success=True,
                emotions=result.get("emotions", {}),
                dominant_emotion=result.get("dominant_emotion", "neutral")
            )
        else:
            # Fallback to keyword-based detection if analyzer isn't available
            logger.warning("analyze_emotion", "Emotion analyzer not available, using keyword fallback")
            
            # Simple keyword-based emotion detection
            emotion_keywords = {
                "joy": ["happy", "joy", "delighted", "glad", "pleased", "excited", "thrilled"],
                "sadness": ["sad", "unhappy", "depressed", "down", "miserable", "upset", "disappointed"],
                "anger": ["angry", "mad", "furious", "annoyed", "irritated", "enraged", "frustrated"],
                "fear": ["afraid", "scared", "frightened", "terrified", "anxious", "worried", "nervous"],
                "surprise": ["surprised", "amazed", "astonished", "shocked", "stunned"],
                "disgust": ["disgusted", "repulsed", "revolted", "sickened"],
                "neutral": ["ok", "fine", "neutral", "average", "normal"]
            }
            
            text = text.lower()
            emotion_scores = {emotion: 0.1 for emotion in emotion_keywords}  # Base score
            
            # Simple keyword matching
            for emotion, keywords in emotion_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        emotion_scores[emotion] += 0.15  # Increment score for each match
            
            # Find the dominant emotion
            dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
            
            return EmotionResponse(
                success=True,
                emotions=emotion_scores,
                dominant_emotion=dominant_emotion
            )
            
    except Exception as e:
        logger.error("analyze_emotion", f"Error analyzing emotions: {str(e)}")
        import traceback
        logger.error("analyze_emotion", traceback.format_exc())
        
        return EmotionResponse(
            success=False,
            error=str(e)
        )

@app.post("/provide_feedback", response_model=FeedbackResponse)
async def provide_feedback(request: FeedbackRequest):
    """Provide feedback on memory retrieval relevance."""
    try:
        if not app.state.memory_core.threshold_calibrator:
            return FeedbackResponse(
                success=False,
                error="Adaptive thresholding is not enabled"
            )
        
        await app.state.memory_core.provide_feedback(
            memory_id=request.memory_id,
            similarity_score=request.similarity_score,
            was_relevant=request.was_relevant
        )
        
        new_threshold = app.state.memory_core.threshold_calibrator.get_current_threshold()
        
        return FeedbackResponse(
            success=True,
            new_threshold=new_threshold
        )
    except Exception as e:
        logger.error("provide_feedback", f"Error: {str(e)}")
        return FeedbackResponse(
            success=False,
            error=str(e)
        )

@app.post("/detect_contradictions")
async def detect_contradictions(threshold: float = 0.75):
    """Detect potential causal contradictions in memories."""
    try:
        contradictions = await app.state.memory_core.detect_contradictions(threshold=threshold)
        return {
            "success": True,
            "contradictions": contradictions,
            "count": len(contradictions)
        }
    except Exception as e:
        logger.error("detect_contradictions", f"Error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/process_transcription", response_model=TranscriptionResponse)
async def process_transcription(request: TranscriptionRequest, background_tasks: BackgroundTasks):
    """Process a transcription and store it in the memory system with rich metadata."""
    try:
        logger.info("process_transcription", "Processing transcription request")
        
        # Validate input
        if not request.text or not isinstance(request.text, str) or len(request.text.strip()) == 0:
            logger.error("process_transcription", "Invalid or empty transcription text")
            return TranscriptionResponse(
                success=False,
                error="Transcription text cannot be empty"
            )
            
        # Tracking for current request
        embedding = None
        extracted_metadata = None
        memory_id = None
        
        # Step 1: Generate embedding if needed
        if request.embedding is None and hasattr(app.state, 'embedding_model'):
            try:
                logger.info("process_transcription", "Generating embedding from transcription")
                loop = asyncio.get_event_loop()
                embedding_list = await loop.run_in_executor(
                    None, 
                    lambda: app.state.embedding_model.encode([request.text])
                )
                # Convert numpy array to Python list to avoid array boolean issues
                if embedding_list is not None and len(embedding_list) > 0:
                    embedding = embedding_list[0].tolist()
                    logger.info("process_transcription", f"Generated embedding with {len(embedding)} dimensions")
                else:
                    embedding = None
                    logger.warning("process_transcription", "Failed to generate embedding - empty result")
            except Exception as embed_error:
                logger.error("process_transcription", f"Embedding generation error: {str(embed_error)}")
                # Continue with None embedding if it fails
        else:
            embedding = request.embedding
        
        # Step 2: Extract features using the TranscriptionFeatureExtractor
        if hasattr(app.state, 'transcription_extractor') and app.state.transcription_extractor is not None:
            try:
                logger.info("process_transcription", "Extracting features from transcription")
                
                # Use our extractor to get rich metadata
                audio_metadata = request.audio_metadata or {}
                extracted_metadata = await app.state.transcription_extractor.extract_features(
                    transcript=request.text,
                    meta=audio_metadata
                )
                
                logger.info("process_transcription", 
                         f"Extracted {len(extracted_metadata)} features including" +
                         f" dominant_emotion={extracted_metadata.get('dominant_emotion', 'none')}," +
                         f" keywords={len(extracted_metadata.get('keywords', []))} keywords")
            except Exception as extract_error:
                logger.error("process_transcription", f"Feature extraction error: {str(extract_error)}")
                # Continue with empty metadata if extraction fails
                extracted_metadata = {
                    "input_modality": "spoken",
                    "source": "transcription",
                    "error": str(extract_error)
                }
        else:
            logger.warning("process_transcription", "No transcription feature extractor available")
            extracted_metadata = {
                "input_modality": "spoken",
                "source": "transcription"
            }
        
        # Step 3: Process the memory through the core
        try:
            # Prepare final metadata
            metadata = extracted_metadata or {}
            
            # Set importance if provided
            if request.importance is not None:
                metadata["importance"] = max(0.0, min(1.0, request.importance))
            
            # Add timestamp to metadata
            metadata["timestamp"] = time.time()
            
            # Call memory core to process the memory
            logger.info("process_transcription", "Calling memory core to process transcription memory")
            result = await app.state.memory_core.process_memory(
                content=request.text,
                embedding=embedding,
                memory_id=request.memory_id,
                metadata=metadata,
                memory_type="transcription",
                force_update=request.force_update
            )
            
            memory_id = result.get("memory_id")
            logger.info("process_transcription", f"Transcription processed with ID: {memory_id}")
            
            # Return success response
            return TranscriptionResponse(
                success=True,
                memory_id=memory_id,
                metadata=metadata,
                embedding=embedding
            )
            
        except Exception as core_error:
            logger.error("process_transcription", f"Memory core processing error: {str(core_error)}")
            raise HTTPException(status_code=500, detail=f"Memory processing failed: {str(core_error)}")
    
    except Exception as e:
        logger.error("process_transcription", f"Process transcription error: {str(e)}")
        import traceback
        logger.error("process_transcription", traceback.format_exc())
        
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

# --- Additional Memory Management Endpoints ---

@app.get("/api/memories/{memory_id}", response_model=GetMemoryResponse, tags=["Memory Management"])
async def get_memory(memory_id: str = Path(..., title="Memory ID", description="The unique ID of the memory to retrieve")):
    """Retrieve a specific memory entry by its ID."""
    try:
        memory = await app.state.memory_core.get_memory_by_id_async(memory_id)
        
        if memory is None:
            logger.warning("API", f"Memory not found: {memory_id}")
            return GetMemoryResponse(success=False, error=f"Memory with ID '{memory_id}' not found")
        
        # Use the MemoryEntry's to_dict method for proper serialization
        memory_dict = memory.to_dict()
        
        logger.info("API", f"Retrieved memory: {memory_id}")
        return GetMemoryResponse(success=True, memory=memory_dict)
    except Exception as e:
        logger.error("API", f"Error retrieving memory: {str(e)}")
        return GetMemoryResponse(success=False, error=f"Internal error: {str(e)}")

# --- Optional: Assembly Management Endpoints (Basic for MVP) ---

@app.get("/assemblies")
async def list_assemblies():
    """List all memory assemblies."""
    try:
        assembly_info = []
        async with app.state.memory_core._lock:
            for assembly_id, assembly in app.state.memory_core.assemblies.items():
                assembly_info.append({
                    "assembly_id": assembly_id,
                    "name": assembly.name,
                    "memory_count": len(assembly.memories),
                    "last_activation": assembly.last_activation
                })
        return {
            "success": True,
            "assemblies": assembly_info,
            "count": len(assembly_info)
        }
    except Exception as e:
        logger.error("list_assemblies", f"Error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

@app.get("/assemblies/{assembly_id}")
async def get_assembly(assembly_id: str):
    """Get details for a specific assembly."""
    try:
        async with app.state.memory_core._lock:
            if assembly_id not in app.state.memory_core.assemblies:
                return {
                    "success": False,
                    "error": "Assembly not found"
                }
            
            assembly = app.state.memory_core.assemblies[assembly_id]
            memory_ids = list(assembly.memories)
            
            # Get memory details (limited to first 10 for brevity)
            memories = []
            for mem_id in memory_ids[:10]:
                if mem_id in app.state.memory_core._memories:
                    memory = app.state.memory_core._memories[mem_id]
                    memories.append({
                        "id": memory.id,
                        "content": memory.content,
                        "quickrecal_score": memory.quickrecal_score
                    })
            
            return {
                "success": True,
                "assembly_id": assembly_id,
                "name": assembly.name,
                "memory_count": len(assembly.memories),
                "last_activation": assembly.last_activation,
                "sample_memories": memories,
                "total_memories": len(memory_ids)
            }
    except Exception as e:
        logger.error("get_assembly", f"Error: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

# --- Trainer Integration Endpoints ---

@app.post("/api/memories/get_sequence_embeddings", response_model=SequenceEmbeddingsResponse)
async def get_sequence_embeddings(
    topic: Optional[str] = None,
    user: Optional[str] = None,
    emotion: Optional[str] = None,
    min_importance: Optional[float] = None,
    limit: int = 100,
    min_quickrecal_score: Optional[float] = None,
    start_timestamp: Optional[str] = None,
    end_timestamp: Optional[str] = None,
    sort_by: str = "timestamp"
):
    """Retrieve a sequence of memory embeddings, ordered by timestamp or quickrecal score.
    
    This endpoint enables the Trainer to obtain sequential memory embeddings
    for training its predictive models and building semantic time series.
    """
    logger.info("API", f"Retrieving sequence embeddings with topic={topic}, limit={limit}, sort_by={sort_by}")
    
    if app.state.trainer_integration is None:
        logger.error("API", "Trainer integration manager not initialized")
        raise HTTPException(status_code=500, detail="Trainer integration not available")
    
    try:
        sequence = await app.state.trainer_integration.get_sequence_embeddings(
            topic=topic,
            user=user,
            emotion=emotion,
            min_importance=min_importance,
            limit=limit,
            min_quickrecal_score=min_quickrecal_score,
            start_timestamp=start_timestamp,
            end_timestamp=end_timestamp,
            sort_by=sort_by
        )
        return sequence
    except Exception as e:
        logger.error("API", f"Error retrieving sequence embeddings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve sequence embeddings: {str(e)}")

@app.post("/api/memories/update_quickrecal_score")
async def update_quickrecal_score(request: UpdateQuickRecalScoreRequest):
    """Update a memory's quickrecal score based on surprise feedback from the Trainer.
    
    This endpoint allows the Trainer to inform the Memory Core about surprising or
    unexpected memories, which can boost their recall priority and track narrative surprise.
    
    Surprise is recorded in the memory's metadata for future reference and pattern analysis.
    """
    logger.info("API", f"Updating quickrecal score for memory {request.memory_id} with delta {request.delta}")
    
    if app.state.trainer_integration is None:
        logger.error("API", "Trainer integration manager not initialized")
        raise HTTPException(status_code=500, detail="Trainer integration not available")
    
    try:
        result = await app.state.trainer_integration.update_quickrecal_score(request)
        return result
    except Exception as e:
        logger.error("API", f"Error updating quickrecal score: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to update quickrecal score: {str(e)}")

# Run the server when the module is executed directly
if __name__ == "__main__":
    import os
    import uvicorn
    
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "5010"))
    
    print(f"Starting Synthians Memory Core API server at {host}:{port}")
    
    uvicorn.run(app, host=host, port=port)

```

# custom_logger.py

```py
# synthians_memory_core/custom_logger.py

import logging
import os
import time
from typing import Dict, Any, Optional

# Set up logging
log_level = os.getenv("LOG_LEVEL", "INFO")
numeric_level = getattr(logging, log_level.upper(), None)
if not isinstance(numeric_level, int):
    numeric_level = logging.INFO

logging.basicConfig(
    level=numeric_level,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

class Logger:
    """
    A simplified logger compatible with both the original interface
    (context, message, data) and standard logging calls (message, *args, **kwargs).
    """

    def __init__(self, name="SynthiansMemory"):
        self.logger = logging.getLogger(name)

    def _log(self, level: int, context_or_msg: str, msg: Optional[str] = None, data: Optional[Dict[str, Any]] = None, **kwargs):
        """Internal log handler."""
        exc_info = kwargs.pop('exc_info', None) # Extract standard exc_info kwarg

        # Determine how the method was called
        if msg is not None:
            # Called likely with (context, message, data)
            log_message = f"[{context_or_msg}] {msg}"
            if data:
                 log_message += f" | Data: {data}"
        else:
            # Called likely with standard (message, *args) or (message, data={})
            # Treat the first argument as the main message
            log_message = context_or_msg
            if data: # If data was passed as the third positional arg (legacy)
                 log_message += f" | Data: {data}"
            elif kwargs: # Or if data was passed as kwargs (more standard)
                 # Filter out standard logging kwargs if any snuck in
                 log_kwargs = {k: v for k, v in kwargs.items() if k not in ['level', 'name', 'pathname', 'lineno', 'funcName', 'exc_text', 'stack_info']}
                 if log_kwargs:
                      log_message += f" | Data: {log_kwargs}"

        self.logger.log(level, log_message, exc_info=exc_info)

    def info(self, context_or_msg: str, msg: Optional[str] = None, data: Optional[Dict[str, Any]] = None, **kwargs):
        """Log info message"""
        self._log(logging.INFO, context_or_msg, msg, data, **kwargs)

    def warning(self, context_or_msg: str, msg: Optional[str] = None, data: Optional[Dict[str, Any]] = None, **kwargs):
        """Log warning message"""
        self._log(logging.WARNING, context_or_msg, msg, data, **kwargs)

    def error(self, context_or_msg: str, msg: Optional[str] = None, data: Optional[Dict[str, Any]] = None, **kwargs):
        """Log error message, accepting exc_info"""
        self._log(logging.ERROR, context_or_msg, msg, data, **kwargs)

    def debug(self, context_or_msg: str, msg: Optional[str] = None, data: Optional[Dict[str, Any]] = None, **kwargs):
        """Log debug message"""
        self._log(logging.DEBUG, context_or_msg, msg, data, **kwargs)

# Create a singleton logger instance
logger = Logger()
```

# docs\api\API_REFERENCE.md

```md
# Synthians Cognitive Architecture: API Reference

This document provides a reference for the APIs exposed by the Synthians Memory Core service.

**Date:** 2025-03-30
**Version:** 1.0.0

## Table of Contents

1.  [Synthians Memory Core API](#synthians-memory-core-api)
2.  [Common Error Handling](#common-error-handling)

---

## 1. Synthians Memory Core API

**Base URL:** `http://localhost:5010` (Default)

This service manages persistent memory storage, retrieval, scoring, embedding generation, emotion analysis, and related functionalities for the Synthians system.

---

### Root

*   **Method:** `GET`
*   **Path:** `/`
*   **Description:** Basic endpoint indicating the API is running.
*   **Response (Success):**
    \`\`\`json
    {
      "message": "Synthians Memory Core API"
    }
    \`\`\`

---

### Health Check

*   **Method:** `GET`
*   **Path:** `/health`
*   **Description:** Checks the health status of the Memory Core service, including uptime and basic counts.
*   **Response (Success):**
    \`\`\`json
    {
      "status": "healthy",
      "uptime_seconds": 1234.56,         // Example value
      "memory_count": 500,              // Example value
      "assembly_count": 50,             // Example value
      "version": "1.0.0"
    }
    \`\`\`
*   **Response (Error):**
    \`\`\`json
    {
      "status": "unhealthy",
      "error": "Description of the error"
    }
    \`\`\`

---

### Get Statistics

*   **Method:** `GET`
*   **Path:** `/stats`
*   **Description:** Retrieves detailed statistics about the Memory Core system, including memory counts, vector index status, geometry configuration, and API server details.
*   **Response (Success):**
    \`\`\`json
    {
      "success": true,
      "api_server": {
        "uptime_seconds": 1234.56,        // Example value
        "memory_count": 500,             // Example value - In-memory cache count
        "embedding_dim": 768,
        "geometry": "hyperbolic",        // Current geometry setting
        "model": "all-mpnet-base-v2"      // Configured embedding model
      },
      "memory": {
        "total_memories": 500,           // Example value - Total indexed memories
        "total_assemblies": 50,          // Example value
        "storage_path": "/app/memory/stored/synthians",
        "threshold": 0.75                // Configured default threshold (may differ from active retrieval thresholds which can be adaptive or request-specific)
      },
      "vector_index": {
        "count": 500,                    // Example value
        "id_mappings": 500,              // Example value
        "index_type": "Cosine"           // e.g., L2, IP, Cosine
      }
    }
    \`\`\`
*   **Response (Error):**
    \`\`\`json
    {
      "success": false,
      "error": "Description of the error retrieving stats"
    }
    \`\`\`

---

### Process Memory

*   **Method:** `POST`
*   **Path:** `/process_memory`
*   **Description:** Processes and stores a new memory entry. Generates embedding if not provided, calculates QuickRecal score, performs emotion analysis (optional), synthesizes metadata, and saves the memory. Handles potential embedding dimension mismatches.
*   **Request Model:** (`ProcessMemoryRequest`)
    \`\`\`json
    {
      "content": "string", // The text content of the memory
      "embedding": "Optional[List[float]]", // Optional pre-computed embedding
      "metadata": "Optional[Dict[str, Any]]", // Optional base metadata
      "analyze_emotion": "Optional[bool]" // Default: true. Set to false to skip emotion analysis.
    }
    \`\`\`
*   **Response Model:** (`ProcessMemoryResponse`)
    \`\`\`json
    {
      "success": true,
      "memory_id": "string", // Unique ID assigned to the memory
      "quickrecal_score": "float", // Calculated relevance score
      "embedding": "List[float]", // The embedding used/generated (potentially aligned)
      "metadata": "Dict[str, Any]", // Enriched metadata after synthesis
      "error": null // Or error string on failure
    }
    \`\`\`

---

### Retrieve Memories

*   **Method:** `POST`
*   **Path:** `/retrieve_memories`
*   **Description:** Retrieves relevant memories based on a query string. Generates query embedding, performs vector search, applies emotional gating, and uses adaptive thresholding (if enabled).
*   **Request Model:** (`RetrieveMemoriesRequest`)
    \`\`\`json
    {
      "query": "string", // The search query text
      "query_embedding": "Optional[List[float]]", // Pre-computed embedding vector; rarely needed as the system will automatically generate an embedding from the query text
      "top_k": "int", // Default: 5. Max number of results to return.
      "user_emotion": "Optional[Union[Dict[str, Any], str]]", // e.g., {"dominant_emotion": "joy"} or "joy". Used for emotional gating.
      "cognitive_load": "float", // Default: 0.5. Influences emotional gating strictness.
      "threshold": "Optional[float]", // Explicit similarity threshold override (0.0-1.0). If None, uses adaptive threshold.
      "metadata_filter": "Optional[Dict[str, Any]]", // Filter memories by metadata fields (e.g., {"source": "user", "day_of_week": "monday"}). Supports nested keys with dots (e.g., "details.project").
      "search_strategy": "Optional[str]" // Determines the retrieval approach (e.g., "vector", "hybrid", "metadata"). If not specified, uses the system default.
    }
    \`\`\`
*   **Response Model:** (`RetrieveMemoriesResponse`)
    \`\`\`json
    {
      "success": true,
      "memories": [
        {
          "id": "string",
          "content": "string",
          "embedding": "List[float]",
          "timestamp": "float", // Unix timestamp
          "quickrecal_score": "float",
          "metadata": "Dict[str, Any]", // Includes synthesized metadata
          "similarity": "float", // Similarity score to the query
          "emotional_resonance": "Optional[float]", // Score from emotional gating (if applied)
          "final_score": "Optional[float]" // Combined score after gating (if applied)
          // ... other MemoryEntry fields serialized by to_dict()
        }
        // ... more memories up to top_k
      ],
      "error": null // Or error string on failure
    }
    \`\`\`

---

### Generate Embedding

*   **Method:** `POST`
*   **Path:** `/generate_embedding`
*   **Description:** Generates an embedding vector for the given text using the server's configured Sentence Transformer model.
*   **Request Model:** (`GenerateEmbeddingRequest`)
    \`\`\`json
    {
      "text": "string" // The text to embed
    }
    \`\`\`
*   **Response Model:** (`GenerateEmbeddingResponse`)
    \`\`\`json
    {
      "success": true,
      "embedding": "List[float]", // The generated embedding vector
      "dimension": "int", // The dimension of the embedding
      "error": null
    }
    \`\`\`

---

### Calculate QuickRecal Score

*   **Method:** `POST`
*   **Path:** `/calculate_quickrecal`
*   **Description:** Calculates the QuickRecal score for a given text or embedding, considering context factors. Generates embedding if only text is provided.
*   **Request Model:** (`QuickRecalRequest`)
    \`\`\`json
    {
      "embedding": "Optional[List[float]]", // Pre-computed embedding
      "text": "Optional[string]", // Text to generate embedding from if embedding not provided
      "context": "Optional[Dict[str, Any]]" // Context factors (e.g., timestamp, relevance, importance, metadata)
    }
    \`\`\`
*   **Response Model:** (`QuickRecalResponse`)
    \`\`\`json
    {
      "success": true,
      "quickrecal_score": "float", // The calculated score (0.0-1.0)
      "factors": "Optional[Dict[str, float]]", // Scores of individual contributing factors (e.g., recency, emotion)
      "error": null
    }
    \`\`\`

---

### Analyze Emotion

*   **Method:** `POST`
*   **Path:** `/analyze_emotion`
*   **Description:** Analyzes the emotional content of the given text using the server's `EmotionAnalyzer` (transformer model or keyword fallback).
*   **Request Model:** (`EmotionRequest`)
    \`\`\`json
    {
      "text": "string" // The text to analyze
    }
    \`\`\`
*   **Response Model:** (`EmotionResponse`)
    \`\`\`json
    {
      "success": true,
      "emotions": "Dict[str, float]", // Scores for different emotions (e.g., {"joy": 0.8, "sadness": 0.1})
      "dominant_emotion": "string", // The emotion with the highest score
      "error": null
    }
    \`\`\`

---

### Provide Feedback

*   **Method:** `POST`
*   **Path:** `/provide_feedback`
*   **Description:** Provides feedback on the relevance of a retrieved memory, used by the `ThresholdCalibrator` to adjust the adaptive similarity threshold.
*   **Request Model:** (`FeedbackRequest`)
    \`\`\`json
    {
      "memory_id": "string", // ID of the memory the feedback is about
      "similarity_score": "float", // The similarity score assigned during retrieval
      "was_relevant": "bool" // True if the user found it relevant, False otherwise
    }
    \`\`\`
*   **Response Model:** (`FeedbackResponse`)
    \`\`\`json
    {
      "success": true,
      "new_threshold": "Optional[float]", // The current adaptive threshold after adjustment
      "error": null
    }
    \`\`\`

---

### Detect Contradictions

*   **Method:** `POST`
*   **Path:** `/detect_contradictions`
*   **Description:** Attempts to detect potential contradictions among stored memories based on semantic similarity and content analysis (currently basic keyword checks for opposition).
*   **Query Parameter:** `threshold` (float, default: 0.75) - Similarity threshold for considering memories potentially contradictory.
*   **Response (Success):**
    \`\`\`json
    {
      "success": true,
      "contradictions": [
        {
           "memory_a_id": "string",
           "memory_a_content": "string",
           "memory_b_id": "string",
           "memory_b_content": "string",
           "similarity": "float",
           "overlap_ratio": "float" // Ratio of common words
        }
        // ... more potential contradictions
      ],
      "count": "int" // Number of contradiction pairs found
    }
    \`\`\`
*   **Response (Error):**
    \`\`\`json
    {
      "success": false,
      "error": "Description of the error"
    }
    \`\`\`

---

### Process Transcription

*   **Method:** `POST`
*   **Path:** `/process_transcription`
*   **Description:** Processes transcribed text, enriches it with features extracted from audio metadata (e.g., pauses, speaking rate, interruption info), performs emotion analysis, and stores it as a memory.
*   **Request Model:** (`TranscriptionRequest`)
    \`\`\`json
    {
      "text": "string", // The transcribed text
      "audio_metadata": "Optional[Dict[str, Any]]", // e.g., {"duration_sec": 5.2, "was_interrupted": true}
      "embedding": "Optional[List[float]]", // Optional pre-computed embedding
      "memory_id": "Optional[string]", // For updating an existing memory
      "importance": "Optional[float]", // Optional importance score (0-1)
      "force_update": "bool" // Default: false. Force update if memory_id exists.
    }
    \`\`\`
*   **Response Model:** (`TranscriptionResponse`)
    \`\`\`json
    {
      "success": true,
      "memory_id": "string", // ID of the created/updated memory
      "metadata": "Dict[str, Any]", // Enriched metadata including extracted audio features
      "embedding": "List[float]", // The embedding used/generated
      "error": null
    }
    \`\`\`

---

### Get Memory by ID

*   **Method:** `GET`
*   **Path:** `/api/memories/{memory_id}`
*   **Description:** Retrieves a specific memory entry by its unique identifier. Returns the complete memory object including content, embedding, and all metadata.
*   **Path Parameter:** `memory_id` (string) - The unique ID of the memory.
*   **Response Model:** (`GetMemoryResponse`)
    \`\`\`json
    {
      "success": true,
      "memory": { // Full MemoryEntry dictionary representation
        "id": "string",
        "content": "string",
        "embedding": "List[float]",
        "timestamp": "string", // ISO format UTC
        "quickrecal_score": "float",
        "quickrecal_updated": "Optional[string]", // ISO format UTC
        "metadata": "Dict[str, Any]",
        "access_count": "int",
        "last_access_time": "string", // ISO format UTC
        "hyperbolic_embedding": "Optional[List[float]]"
      },
      "error": null
    }
    \`\`\`
*   **Response (Not Found):**
    \`\`\`json
    {
      "success": false,
      "memory": null,
      "error": "Memory with ID '{memory_id}' not found"
    }
    \`\`\`

---

### List Assemblies

*   **Method:** `GET`
*   **Path:** `/assemblies`
*   **Description:** Lists basic information about all known memory assemblies.
*   **Response (Success):**
    \`\`\`json
    {
      "success": true,
      "assemblies": [
        {
          "assembly_id": "string",
          "name": "string",
          "memory_count": "int", // Number of memories in the assembly
          "last_activation": "string" // ISO format UTC timestamp
        }
        // ... more assemblies
      ],
      "count": "int" // Total number of assemblies
    }
    \`\`\`

---

### Get Assembly Details

*   **Method:** `GET`
*   **Path:** `/assemblies/{assembly_id}`
*   **Description:** Retrieves detailed information about a specific memory assembly, including a sample of its member memories.
*   **Path Parameter:** `assembly_id` (string) - The unique ID of the assembly.
*   **Response (Success):**
    \`\`\`json
    {
      "success": true,
      "assembly_id": "string",
      "name": "string",
      "memory_count": "int",
      "last_activation": "string", // ISO format UTC
      "sample_memories": [ // Limited sample (e.g., first 10) for brevity
        {
          "id": "string",
          "content": "string",
          "quickrecal_score": "float"
        }
        // ... up to 10 sample memories
      ],
      "total_memories": "int" // Total number of memories in the assembly
    }
    \`\`\`
*   **Response (Not Found):**
    \`\`\`json
    {
      "success": false,
      "error": "Assembly not found"
    }
    \`\`\`

---

### Get Sequence Embeddings (Trainer Integration)

*   **Method:** `POST`
*   **Path:** `/api/memories/get_sequence_embeddings`
*   **Description:** Retrieves a sequence of memory embeddings, ordered and filtered, suitable for feeding into a sequence trainer (e.g., Neural Memory Server).
*   **Request Model:** (Implicit, uses query parameters)
    *   **Query Parameters:** `topic`, `user`, `emotion`, `min_importance`, `limit`, `min_quickrecal_score`, `start_timestamp`, `end_timestamp`, `sort_by` (timestamp or quickrecal_score)
*   **Response Model:** (`SequenceEmbeddingsResponse`)
    \`\`\`json
    {
      "embeddings": [
        {
          "id": "string",
          "embedding": "List[float]",
          "timestamp": "string", // ISO format UTC
          "quickrecal_score": "Optional[float]",
          "emotion": "Optional[Dict[str, float]]",
          "dominant_emotion": "Optional[string]",
          "importance": "Optional[float]",
          "topic": "Optional[string]",
          "user": "Optional[string]"
        }
        // ... more embeddings up to limit
      ]
    }
    \`\`\`

---

### Update QuickRecal Score (Trainer Integration)

*   **Method:** `POST`
*   **Path:** `/api/memories/update_quickrecal_score`
*   **Description:** Allows an external system (like the Trainer or Orchestrator) to update a memory's QuickRecal score based on feedback, such as prediction surprise. Records the reason and context in the memory's metadata.
*   **Request Model:** (`UpdateQuickRecalScoreRequest`)
    \`\`\`json
    {
      "memory_id": "string", // ID of the memory to update
      "delta": "float", // Amount to change score by (+ve or -ve)
      "predicted_embedding": "Optional[List[float]]", // Embedding predicted by the trainer
      "reason": "Optional[string]", // e.g., "NM Surprise Loss: 0.65"
      "embedding_delta": "Optional[List[float]]" // Pre-calculated delta between actual and predicted
    }
    \`\`\`
*   **Response (Success):**
    \`\`\`json
    {
      "status": "success",
      "memory_id": "string",
      "previous_score": "float",
      "new_score": "float", // Score after applying delta (clamped 0-1)
      "delta": "float"
    }
    \`\`\`
*   **Response (Error):**
    \`\`\`json
    {
      "status": "error",
      "message": "Description of failure (e.g., memory not found, update failed)"
    }
    \`\`\`

---

## 2. Common Error Handling

API endpoints generally return errors using the standard FastAPI `HTTPException` mechanism, resulting in JSON responses like:

\`\`\`json
{
  "detail": "Description of the error"
}

Specific endpoints might return structured error responses with a "success": false field and an "error" field:

json
CopyInsert
{
  "success": false,
  "error": "Detailed error message",
  "status_code": 400 // Optional HTTP status code
}
Common HTTP Status Codes:

200 OK: Request successful.
400 Bad Request: Invalid input parameters or payload format (e.g., malformed JSON, missing required fields).
404 Not Found: Requested resource (e.g., memory_id, assembly_id) not found.
500 Internal Server Error: An unexpected error occurred during processing on the server (e.g., embedding generation failure, persistence error).
503 Service Unavailable: A required internal component (e.g., vector index, emotion model) failed to initialize or is unavailable.
```

# docs\api\client_usage.md

```md
# Memory Core Python Client Usage Guide

*This document provides examples and best practices for using the asynchronous Python client (`SynthiansClient`) to interact with the Synthians Memory Core API.*

## 1. Installation & Setup

Ensure the client class is accessible within your project.

\`\`\`python
import asyncio
import numpy as np
import json # Added for pretty printing
from synthians_memory_core.api.client.client import SynthiansClient

# Initialize the client within an async context
async def main():
    # Use default localhost and port unless configured otherwise
    client_instance = SynthiansClient(base_url="http://localhost:5010")
    async with client_instance as client:
        # --- Use client methods here ---
        print("Client initialized.")
        # Example calls (uncomment to run)
        # await store_example(client)
        # await retrieve_example(client)
        # await get_by_id_example(client, "some_memory_id") # Replace with a real ID
        # await embedding_example(client)
        # await emotion_example(client)
        # await quickrecal_example(client)
        # await feedback_example(client, "some_memory_id") # Replace with a real ID
        # await contradict_example(client)
        # await transcription_example(client)
        # await safe_call(client)
        print("Client operations finished.")

# To run the examples:
# if __name__ == "__main__":
#     asyncio.run(main())
\`\`\`

## 2. Basic Operations

### Storing a Memory

The server handles embedding generation if not provided. Metadata is enriched server-side.

\`\`\`python
async def store_example(client: SynthiansClient):
    # Store with content only (embedding generated server-side)
    response1 = await client.process_memory(
        content="This is an important memory about project Alpha."
    )
    if response1.get("success"):
        print(f"Stored memory 1 with ID: {response1['memory_id']}")
        return response1['memory_id'] # Return ID for potential use later
    else:
        print(f"Failed to store memory 1: {response1.get('error')}")
        return None

    # Store with custom metadata
    response2 = await client.process_memory(
        content="Meeting notes regarding the Q3 roadmap.",
        metadata={
            "source": "meeting_notes",
            "project": "RoadmapQ3",
            "attendees": ["Alice", "Bob"]
        }
    )
    if response2.get("success"):
        print(f"Stored memory 2 with ID: {response2['memory_id']}")
        print(f"  -> Returned metadata: {response2.get('metadata')}") # Note enriched metadata
        return response2['memory_id']
    else:
        print(f"Failed to store memory 2: {response2.get('error')}")
        return None
\`\`\`

### Retrieving Memories

Retrieve memories based on a text query. Emotional gating and adaptive thresholding are applied server-side if configured.

\`\`\`python
async def retrieve_example(client: SynthiansClient):
    # Basic retrieval by query
    response1 = await client.retrieve_memories(
        query="project Alpha roadmap",
        top_k=3
    )
    if response1.get("success"):
        print(f"
Retrieved {len(response1['memories'])} memories for 'project Alpha roadmap':")
        for i, memory in enumerate(response1['memories']):
            print(f"  {i+1}. ID: {memory.get('id')}, Score: {memory.get('similarity'):.4f}, Content: {memory.get('content', '')[:60]}...")
    else:
        print(f"Retrieval failed: {response1.get('error')}")

    # Retrieve with metadata filtering
    response2 = await client.retrieve_memories(
        query="meeting notes",
        top_k=5,
        metadata_filter={
            "source": "meeting_notes",
            "project": "RoadmapQ3"
        }
    )
    if response2.get("success"):
        print(f"
Retrieved {len(response2['memories'])} memories matching metadata filter:")
        for memory in response2['memories']:
             print(f"  - ID: {memory.get('id')}, Source: {memory.get('metadata', {}).get('source')}")
    else:
        print(f"Metadata retrieval failed: {response2.get('error')}")


    # Retrieve with emotional context (provide dominant emotion)
    response3 = await client.retrieve_memories(
        query="important decision",
        user_emotion={"dominant_emotion": "focused"}, # Or just "focused"
        cognitive_load=0.3, # Lower value allows more results through gating
        top_k=3
    )
    if response3.get("success"):
        print(f"
Retrieved {len(response3['memories'])} memories with 'focused' emotion:")
        # Check 'emotional_resonance' or 'final_score' if available
        for memory in response3['memories']:
            print(f"  - ID: {memory.get('id')}, Resonance: {memory.get('emotional_resonance', 'N/A')}")
    else:
        print(f"Emotional retrieval failed: {response3.get('error')}")

    # Retrieve with explicit threshold override
    response4 = await client.retrieve_memories(
        query="roadmap",
        threshold=0.1, # Very low threshold for broad recall
        top_k=10
    )
    if response4.get("success"):
         print(f"
Retrieved {len(response4['memories'])} memories with low threshold (0.1):")
    else:
         print(f"Low threshold retrieval failed: {response4.get('error')}")

\`\`\`

### Retrieving a Specific Memory by ID

\`\`\`python
async def get_by_id_example(client: SynthiansClient, memory_id: str):
    if not memory_id:
        print("Cannot retrieve by ID: memory_id is missing.")
        return

    response = await client.get_memory_by_id(memory_id) # Corrected method name
    if response.get("success") and response.get("memory"):
        print(f"
Successfully retrieved memory by ID {memory_id}:")
        # Use json.dumps for readable output of the memory dict
        print(json.dumps(response["memory"], indent=2, default=str)) # Use default=str for non-serializable types like datetime
    elif not response.get("success") and "not found" in response.get("error", "").lower(): # Check error message for 404
         print(f"
Memory with ID {memory_id} not found.")
    else:
        print(f"
Failed to retrieve memory by ID {memory_id}: {response.get('error')}")

\`\`\`

## 3. Utility Endpoints

### Generating Embeddings

\`\`\`python
async def embedding_example(client: SynthiansClient):
    response = await client.generate_embedding("Generate an embedding for this sentence.")
    if response.get("success"):
        embedding = response.get("embedding")
        dimension = response.get("dimension")
        print(f"
Generated embedding (Dimension: {dimension}): {str(embedding)[:100]}...") # Print snippet
    else:
        print(f"
Failed to generate embedding: {response.get('error')}")
\`\`\`

### Analyzing Emotion

\`\`\`python
async def emotion_example(client: SynthiansClient):
    response = await client.analyze_emotion("This is a surprisingly complex and intriguing challenge!")
    if response.get("success"):
        print(f"
Emotion Analysis Result:")
        print(f"  Dominant: {response.get('dominant_emotion')}")
        print(f"  Scores: {response.get('emotions')}")
    else:
        print(f"
Failed to analyze emotion: {response.get('error')}")
\`\`\`

### Calculating QuickRecal Score

\`\`\`python
async def quickrecal_example(client: SynthiansClient):
    # Calculate score for text (embedding generated server-side)
    response1 = await client.calculate_quickrecal(
        text="Calculate the relevance score for this piece of text.",
        context={"importance": 0.7, "source": "user_query"}
    )
    if response1.get("success"):
        print(f"
QuickRecal Score (from text): {response1.get('quickrecal_score'):.4f}")
        print(f"  Factors: {response1.get('factors')}")
    else:
        print(f"
Failed QuickRecal calculation (text): {response1.get('error')}")

    # Calculate score for pre-computed embedding
    embedding_resp = await client.generate_embedding("Some other text")
    if embedding_resp.get("success"):
        embedding = embedding_resp.get("embedding")
        response2 = await client.calculate_quickrecal(embedding=embedding)
        if response2.get("success"):
             print(f"
QuickRecal Score (from embedding): {response2.get('quickrecal_score'):.4f}")
        else:
             print(f"
Failed QuickRecal calculation (embedding): {response2.get('error')}")

\`\`\`

## 4. Advanced Features

### Providing Feedback

Used to tune the adaptive retrieval threshold.

\`\`\`python
async def feedback_example(client: SynthiansClient, memory_id: str):
    if not memory_id:
        print("Cannot provide feedback: memory_id is missing.")
        return

    # Example: Assume a memory was retrieved with score 0.82 and user found it relevant
    response = await client.provide_feedback(
        memory_id=memory_id,
        similarity_score=0.82,
        was_relevant=True
    )
    if response.get("success"):
        print(f"
Feedback provided successfully. New threshold: {response.get('new_threshold'):.4f}")
    else:
        print(f"
Failed to provide feedback: {response.get('error')}")

\`\`\`

### Detecting Contradictions

\`\`\`python
async def contradict_example(client: SynthiansClient):
    # Add potentially contradictory memories first
    await client.process_memory("Statement A implies outcome X.")
    await client.process_memory("Statement B prevents outcome X.")
    await asyncio.sleep(1.0) # Allow indexing time

    response = await client.detect_contradictions(threshold=0.7)
    if response.get("success"):
        print(f"
Contradiction Detection Found: {response.get('count')} potential contradictions.")
        # Pretty print the list of contradictions
        print(json.dumps(response.get("contradictions", []), indent=2))
    else:
         print(f"
Contradiction detection failed: {response.get('error')}")
\`\`\`

### Processing Transcriptions

Enriches transcriptions with audio features before storing.

\`\`\`python
async def transcription_example(client: SynthiansClient):
    # Assuming client has process_transcription method
    if not hasattr(client, 'process_transcription'):
        print("
Skipping transcription example: client.process_transcription not implemented.")
        return

    response = await client.process_transcription(
        text="Okay, let me think... The first point is about integration, and the second... involves the API.",
        audio_metadata={
            "duration_sec": 6.8,
            "speaking_rate": 2.5, # Words per second, example
            "pause_count": 3,
            "longest_pause_ms": 800,
            "was_interrupted": False
        },
        importance=0.8 # Optional importance score
    )
    if response.get("success"):
        print("
Transcription processed successfully:")
        # Pretty print the response
        print(json.dumps(response, indent=2))
    else:
        print(f"
Failed to process transcription: {response.get('error')}")

\`\`\`

## 5. Error Handling

The client methods return dictionaries. Check the `"success"` key (usually boolean) or look for an `"error"` key. Handle potential `aiohttp` exceptions.

\`\`\`python
import aiohttp # Import for exception handling

async def safe_call(client: SynthiansClient):
    try:
        response = await client.health_check()
        if response.get("status") == "healthy":
            print("Server is healthy.")
        # Handle structured error response from health check
        elif "error" in response:
            print(f"Health check failed: {response['error']} (Status: {response.get('status')})")
        else:
            print(f"Health check returned unexpected response: {response}")

        # Example of handling potential failure during retrieval
        retrieve_response = await client.retrieve_memories("nonexistent query for testing")
        if not retrieve_response.get("success"):
             print(f"Retrieval Error: {retrieve_response.get('error')}")
        else:
             print("Retrieval successful (may return 0 memories).")

    except aiohttp.ClientConnectorError as e:
        print(f"Connection Error: Could not connect to the server at {client.base_url}. Is it running? ({e})")
    except aiohttp.ClientResponseError as e:
        print(f"HTTP Error: Received status {e.status} from server. Message: {e.message}")
    except asyncio.TimeoutError:
        print(f"Request Timeout: The request to {client.base_url} timed out.")
    except Exception as e:
        # Catch other unexpected client-side or parsing errors
        print(f"An unexpected error occurred: {e}")

\`\`\`

## 6. Best Practices

1.  **Use Async Context Manager:** Always use `async with SynthiansClient(...) as client:` to ensure the `aiohttp.ClientSession` is properly managed and closed.
2.  **Check Responses:** Robustly check the `"success"` or `"error"` keys in the returned dictionary before assuming an operation succeeded. Handle potential `None` returns or missing keys.
3.  **Rate Limiting:** Be mindful of server load. Avoid sending extremely high volumes of requests without appropriate delays or batching strategies (client doesn't implement batching itself). Use `asyncio.sleep()` if needed.
4.  **Metadata:** Use meaningful and structured metadata when storing memories to improve filtering, context, and retrieval relevance.
5.  **Thresholds:** Understand the impact of the `threshold` parameter in `retrieve_memories`. Lower values increase recall but may decrease precision. Use the feedback mechanism if adaptive thresholding is enabled on the server.
6.  **Error Logging:** Implement robust logging on the client-side to capture API errors, unexpected responses, and connection issues. Use the specific `aiohttp` exceptions for better diagnostics.
7.  **Embedding Handling:** Be aware that the server handles embedding generation and dimension alignment. Provide pre-computed embeddings only if necessary and ensure they are valid lists of floats.

```

# docs\api\README.md

```md
# API Reference & Client Documentation

This directory contains documentation for the HTTP API exposed by the Synthians Memory Core service and guidelines for using the Python client.

## Contents

*   [API Reference](./API_REFERENCE.md): Comprehensive reference for all HTTP API endpoints exposed by the Synthians Memory Core (`http://localhost:5010`), including request/response models and parameters. Details cover memory processing, retrieval, embedding generation, QuickRecal scoring, emotion analysis, feedback mechanisms, and integration points for the Neural Memory / Orchestrator.
*   [Client Usage](./client_usage.md): Guidelines and code examples for using the asynchronous Python client (`SynthiansClient`) to interact with the Memory Core API. Demonstrates basic operations, utility endpoints, advanced features like feedback and contradiction detection, and error handling best practices.

## Technical Details

*   **Framework:** The API is built using FastAPI.
*   **Data Format:** Uses JSON for all request and response bodies. Pydantic models define the structure (see `synthians_memory_core/api/server.py`).
*   **Error Handling:** Follows standard HTTP status codes. Errors often include a `"detail"` field (FastAPI default) or a structured response with `"success": false` and `"error": "message"`.
*   **Asynchronous:** The server and client are designed for asynchronous operations using `asyncio`.
*   **Authentication:** Currently, no specific authentication is implemented in the provided code. Access control would need to be added (e.g., API keys, JWT) for production environments.
*   **Client:** The `SynthiansClient` library simplifies interaction by handling `aiohttp` requests, session management, and basic response parsing within an async context manager.

```

# docs\architechture-changes.md

```md
# Synthians Architecture Changes & Evolution

*This document tracks significant architectural shifts and decisions during the development of the Synthians Cognitive Architecture, focusing on the memory system.*

---

## 2025-03-30: Documentation Refresh & Consistency Pass

*   **Context:** Following significant architectural stabilization and bug fixing, a pass was made to update and align all core documentation (`README.md`, `ARCHITECTURE.md`, `API_REFERENCE.md`, `client_usage.md`, placeholder component docs) with the current codebase.
*   **Key Changes:**
    1.  **Updated API Docs:** `API_REFERENCE.md` and `client_usage.md` were comprehensively updated to reflect the actual FastAPI endpoints, Pydantic models, asynchronous client methods (`SynthiansClient`), and recent features (e.g., `metadata_filter`, `update_quickrecal_score` integration endpoint).
    2.  **Architecture Doc Alignment:** `ARCHITECTURE.md` was updated to accurately depict the Bi-Hemispheric flow, component responsibilities (Memory Core, Neural Memory, CCE), and the refined cognitive cycle involving surprise feedback.
    3.  **Component Placeholders:** Ensured placeholder docs (`core/`, `trainer/`, `orchestrator/`, `testing/`) reflect the latest component names and intended functionality (e.g., `UnifiedQuickRecallCalculator`, `IndexIDMap`, `SurpriseDetector`).
    4.  **READMEs Updated:** Top-level `README.md` and section `README.md` files were updated for clarity and navigation.
*   **Impact:** Core documentation now provides a much more accurate and consistent representation of the system's current state, improving developer understanding and maintainability.

---

## 2025-03-27T23:05:09Z - Lucidia Agent

Okay, let's break down the implications of successfully integrating the Titans Neural Memory module, as implemented according to the paper, into your `synthians_trainer_server`. This moves beyond simple prediction to a more dynamic form of memory.

**Core Shift:** You're moving from a model that *predicts* the next state based on a learned function (like a standard RNN/LSTM where only the hidden state changes at test time) to a model whose *internal parameters* (`M`) are actively *updated* at test time based on new inputs and an associative loss. It's learning to memorize *during* inference.

**Key Implications:**

1.  **True Test-Time Adaptation & Memorization:**
    *   **What:** The memory module (`M`) literally changes its weights with each relevant input via the `update_step` (gradient descent + momentum + decay).
    *   **Why:** This directly implements the paper's core idea – "learning to memorize at test time." It's not just updating a state vector; it's refining its internal associative mapping (`M(k) -> v`) on the fly.
    *   **Impact:** The system can continuously adapt to new information encountered *after* initial training. It explicitly encodes new key-value associations into its parameters, offering a form of ongoing learning and potentially better handling of dynamic environments or distribution shifts compared to static models.

2.  **Shift from Prediction to Associative Recall & Update:**
    *   **What:** The primary functions become `retrieve(query)` (associative recall without changing weights) and `update_memory(input)` (memorization by changing weights). Direct prediction of the *next embedding* is less explicit; retrieval provides related information based on a query.
    *   **Why:** The model's loss (`||M(k) - v||²`) drives it to associate keys with values, not necessarily to predict the *next* value in a sequence directly from the *previous* one in the same way the old model did.
    *   **Impact:** The orchestrator (`ContextCascadeEngine`) needs different logic. Instead of asking "predict next," it might:
        *   Get current embedding `x_t` from `SynthiansMemoryCore`.
        *   Call `/update_memory` with `x_t` to memorize the current step (updating `M`).
        *   Generate a query `q_t` (maybe from `x_t` or context).
        *   Call `/retrieve` with `q_t` to get relevant associative memory `y_t`.
        *   Use `y_t` (and maybe `x_t`) to inform the next action or a separate prediction head.

3.  **More Sophisticated "Surprise" Metric:**
    *   **What:** The gradient `∇ℓ` used in the `update_step` directly represents how much the memory model's parameters needed to change to correctly associate the current key `k_t` with value `v_t`. This is the paper's "surprise."
    *   **Why:** It measures the error in the associative memory's *current* understanding. The momentum term `S_t` carries this surprise forward.
    *   **Impact:** This gradient norm (or related metrics) can be sent back to the `SynthiansMemoryCore` via the orchestrator to update `quickrecal_score`, providing a more grounded measure of novelty or unexpectedness based on the memory's internal learning process.

4.  **Potential for Enhanced Long-Term Context Handling:**
    *   **What:** Information is encoded into the *parameters* of `M`, not just a fixed-size state vector. The forgetting gate (`alpha_t`) helps manage capacity.
    *   **Why:** Unlike RNN hidden states which can saturate or overwrite information, updating weights allows for potentially storing more information over longer sequences, distributed across the parameters. The forgetting gate provides a mechanism to discard less relevant history encoded in the weights.
    *   **Impact:** Theoretically better performance on tasks requiring recall over very long contexts (as claimed in the paper, >2M tokens), surpassing limitations of fixed RNN states and quadratic Transformer costs.

5.  **Increased Computational Cost at Test Time:**
    *   **What:** Every `update_memory` call involves a forward pass, a loss calculation, a backward pass (gradient calculation w.r.t `M`), and parameter updates.
    *   **Why:** This is inherent to the "learning at test time" approach using gradient descent.
    *   **Impact:** Inference (a retrieve + update cycle) will be significantly slower per step than the previous model's simple forward pass. The parallelization technique mentioned in the paper (Section 3.2) becomes crucial for practical speed, but our current implementation is sequential.

6.  **Complex Training Dynamics (Outer vs. Inner Loop):**
    *   **What:** You now have two sets of parameters: the *outer* parameters (`WK`, `WV`, `WQ`, gates) trained via traditional backprop on a task loss, and the *inner* memory parameters (`M`) which evolve during the test-time `update_step` but are *reset* for the outer loop training gradient calculation.
    *   **Why:** The outer loop learns *how to learn/memorize effectively* (by tuning projections and gates), while the inner loop *performs* the memorization.
    *   **Impact:** Requires careful implementation of the outer training loop (`train_outer_step`) and managing the state reset. Tuning the gates (`alpha_t`, `theta_t`, `eta_t`) and the outer learning rate becomes critical for balancing memorization and generalization.

7.  **Explicit Role Definition:**
    *   **What:** The `synthians_trainer_server` now clearly embodies the adaptive, associative, long-term memory role. `SynthiansMemoryCore` remains the structured, indexed, episodic/semantic store.
    *   **Why:** Aligns with the paper's concept of distinct but interconnected memory systems.
    *   **Impact:** Simplifies conceptual understanding. The orchestrator mediates between the fast-lookup `MemoryCore` and the dynamically learning `NeuralMemoryModule`.

**In Summary:**

Getting this working means your "trainer" server transforms from a sequence predictor into a **dynamic, test-time adaptive associative memory**. It gains the ability to continuously learn and encode new associations directly into its parameters during operation. This offers potential for superior long-context handling and adaptation but comes at the cost of increased per-step computational complexity during inference and requires a more sophisticated training setup (outer loop). The interaction with `SynthiansMemoryCore` becomes richer, with the Neural Memory handling dynamic patterns and the Core handling structured storage and retrieval, potentially linked via surprise feedback.

## Implementation Considerations

### Optimization Opportunities

1. **Inference Speed Optimization:**
   * Consider implementing the paper's parallelization technique (Section 3.2) to enable parallel update steps
   * Profile forward/backward operations to identify bottlenecks
   * For large memory models, investigate quantization of memory parameters

2. **Memory Efficiency:**
   * Monitor memory usage patterns during extended operation
   * Implement mechanisms to selectively reset memory weights when they saturate (monitor gradient norms)
   * Consider scheduled alpha/forgetting gate adjustments based on context length

3. **Outer Loop Training:**
   * Start with simple task losses before implementing complex meta-learning objectives
   * Carefully track outer vs. inner parameter gradients to prevent interference
   * Consider curriculum learning for outer loop parameters (start with short contexts)

### Integration with Orchestrator

1. **New Call Pattern:**
   \`\`\`python
   # Previous pattern (simplified)
   previous_memory_state = [...]
   prediction, new_memory = trainer_server.predict_next_embedding(curr_embedding, previous_memory_state)
   
   # New pattern (simplified)
   # 1. First memorize current embedding (updates internal weights)
   trainer_server.update_memory(curr_embedding)
   
   # 2. Then retrieve relevant memory using a query
   query = generate_query(curr_embedding, context)
   memory_retrieval = trainer_server.retrieve(query)
   \`\`\`

2. **Surprise Metric Integration:**
   * Expose a gradient norm metric from `/update_memory` endpoint 
   * Feed this value directly into `quickrecal_score` calculation
   * Consider sliding window normalization of gradient norms

3. **Fallback Mechanisms:**
   * Implement retrieval confidence scoring
   * Provide graceful degradation when memory is unconfident
   * Consider hybrid approaches: use traditional prediction heads alongside memory retrieval

### Monitoring & Debugging

1. **Key Metrics to Track:**
   * Gate values (α, θ, η) throughout operation
   * Gradient norms for inner memory updates
   * Weight change magnitude after each update step
   * Memory parameter saturation (if weights grow too large)

2. **Visualization Tools:**
   * Create embeddings projector for the internal key/value spaces
   * Track key-to-value mapping consistency over time
   * Visualize memory association strength through operation

### Future Extensions

1. **Multi-Head Memory:**
   * Consider extending to multiple parallel memory modules specializing in different association types
   * Implement attention mechanism over multiple memory retrievals

2. **Hierarchical Memory:**
   * Create layered memory modules with different timescales
   * Fast-changing short-term memory feeding into slower-changing long-term memory

3. **Memory Reflection:**
   * Periodically perform "reflection" steps where memory retrieves from itself
   * Use these to consolidate and reorganize internal representation patterns

---

## 2025-03-27T23:04:02Z: Neural Memory Integration - Lucidia Agent

### Summary of Changes

Successfully integrated the Titans Neural Memory module into the `synthians_trainer_server` by fixing critical TensorFlow/Keras implementation issues. The module now properly supports save/load state functionality and correctly registers trainable variables for dynamic updates at test time.

### Key Technical Fixes

1. **Fixed MemoryMLP Layer Registration**
   * Moved layer creation from `build()` to `__init__()` method to ensure proper variable tracking
   * Changed layers from private list (`_layers`) to explicit instance attributes (`self.hidden_layers`, `self.output_layer`)
   * Ensured TensorFlow's variable tracking system correctly identifies trainable weights
   * Resolved "MemoryMLP has NO trainable variables!" errors that prevented gradient updates

2. **Fixed TensorFlow Model Save/Load State**
   * Corrected architecture violation where model was being rebuilt in-place with `__init__()`
   * Implemented proper state loading that respects TensorFlow architectural constraints
   * Created a separate model initialization approach for loading models with different configs
   * Added comprehensive error handling for shape mismatches during weight loading
   * Fixed momentum state variable handling to ensure gradient updates work correctly

3. **Enhanced Gradient Tracking**
   * Added explicit `tape.watch()` calls for trainable variables
   * Fixed gradient calculation in both inner and outer update loops
   * Implemented proper handling of `None` gradients during training
   * Added resilience measures to detect and rebuild missing variables

4. **API Endpoint Improvements**
   * Fixed tensor shape handling in `/retrieve`, `/update_memory`, and `/train_outer` endpoints
   * Improved error messages and validation
   * Enhanced the state persistence endpoints (`/save` and `/load`)

### Impact

* All 9/9 API tests now pass successfully
* The neural memory module can now properly learn at test time as described in the Titans paper
* Gradient updates flow correctly through both inner and outer optimization loops
* State can be reliably saved and loaded across model instances

### Future Considerations

1. **Performance Optimization**
   * Current implementation processes batch examples sequentially in the training loop
   * Could be optimized for parallel processing of examples

2. **Memory Efficiency**
   * Consider optimizing for large embedding dimensions
   * Implement memory-efficient update strategies for high-dimensional embeddings

3. **Metrics Collection**
   * Add tracking for gradient norms, gate values, and memory usage
   * Implement visualization tools for memory behavior analysis
```

# docs\ARCHITECTURE.md

```md
# Synthians Cognitive Architecture (March 2025)

## 1. Overview

This document describes the architecture of the Synthians cognitive system, implementing a **Bi-Hemispheric Cognitive Architecture**. This model separates persistent, indexed memory storage/retrieval (Memory Core) from dynamic, sequence-aware associative memory processing (Neural Memory Server), orchestrated by the Context Cascade Engine (CCE).

The system aims to enable adaptive memory recall, continuous learning from experience via test-time adaptation, contextual awareness through attention mechanisms, and robust handling of complex data like embeddings and emotional context.

**Core Principles:**

*   **Memory is weighted, not just chronological** (QuickRecal)
*   **Emotion shapes recall** (Emotional Gating)
*   **Surprise signals significance** (Neural Memory Loss/Grad → QuickRecal Boost)
*   **Ideas cluster and connect** (Assemblies & Attention)
*   **Presence emerges from adaptive memory** (Test-Time Learning & Variants)

## 2. System Components

The system comprises three main microservices:

1.  **Synthians Memory Core (`synthians_memory_core`):** The primary repository for structured memories (content, metadata, embeddings). Handles storage, indexing (FAISS with `IndexIDMap`), retrieval, relevance scoring (HPC-QuickRecal), metadata synthesis, emotional analysis/gating, and persistence. Analogous to a searchable, adaptive library.
2.  **Neural Memory Server (`synthians_trainer_server`):** Implements an adaptive, associative memory inspired by the Titans paper, capable of test-time learning on sequences of embeddings. Handles K/V/Q projections, associative retrieval, and test-time weight updates, providing surprise metrics. Analogous to learning temporal patterns and associations.
3.  **Context Cascade Engine (`orchestrator`):** Acts as the central orchestrator, managing the information flow between the Memory Core and the Neural Memory Server. Implements the core cognitive cycle and different attention-based variants (MAC, MAG, MAL).

**Diagram:**

\`\`\`text
+--------------------------+        +--------------------------+        +-----------------------------+
|                          |        |                          |        |                             |
|  Synthians Memory Core   |<-(5)---|  Context Cascade Engine  |---(2,4,6)->|   Neural Memory Server      |
|  (Storage/Retrieval)     |-------|       (Orchestrator)     |-------|   (Associative/Predictive)  |
|  (FAISS, QuickRecal,     |  (1)   |   (Handles Variants &    |  (3,7) |   (Test-Time Learning,      |
|   Emotion, Persistence)  |        |    Sequence History)     |        |    Surprise Metrics)        |
+--------------------------+        +--------------------------+        +-----------------------------+
       |         ^                                                           |         ^
       |         | (Filesystem, JSON)                                        |         | (TensorFlow, State Files)
       v         |                                                           v         |
+-----------------+-------+                                           +-----------------+----------+
| Memory Persistence &    |                                           | Neural Memory Module (M) & |
|   Vector Index          |                                           |   Momentum State (S)       |
+-------------------------+                                           +----------------------------+

Key Steps (Refactored & Functional Flow):
1. Input -> CCE -> Memory Core (/process_memory) -> Get x_t, memory_id, initial_qr
2. CCE -> Neural Memory (/get_projections) -> Get k_t, v_t, q_t
3. CCE -> Variant Pre-Update (MAG: /calculate_gates; MAL: calc v'_t)
4. CCE -> Neural Memory (/update_memory w/ variant mods) -> Get loss, grad_norm
5. CCE -> Memory Core (/api/memories/update_quickrecal_score) -> Apply boost -> **FUNCTIONAL**
6. CCE -> Neural Memory (/retrieve) -> Get y_t_raw, q_t
7. CCE -> Variant Post-Retrieval (MAC: calc attended_y_t) -> Get y_t_final
8. CCE -> Update Sequence History (ts, id, x, k, v, q, y_final)
\`\`\`

## 3. Key Components Deep Dive

### 3.1. Synthians Memory Core (`synthians_memory_core` package)

*   **Role:** Long-term, indexed, searchable memory.
*   **Key Classes:**
    *   `SynthiansMemoryCore`: Main orchestrating class.
    *   `MemoryEntry` / `MemoryAssembly`: Data structures.
    *   `MemoryVectorIndex`: FAISS `IndexIDMap` wrapper for fast, ID-keyed vector search. Handles GPU/CPU, persistence, migration, integrity checks.
    *   `MemoryPersistence`: Asynchronous JSON-based storage for `MemoryEntry` and `MemoryAssembly` objects. Manages `memory_index.json`.
    *   `UnifiedQuickRecallCalculator`: Calculates memory relevance (`quickrecal_score`) based on multiple factors (recency, emotion, similarity, importance, surprise feedback, etc.).
    *   `MetadataSynthesizer`: Enriches memories with derived metadata (temporal, emotional, cognitive, embedding stats).
    *   `GeometryManager`: Centralized handling of embedding validation (NaN/Inf), normalization, dimension alignment (padding/truncation), and geometric distance/similarity calculations (Euclidean, Hyperbolic).
    *   `EmotionAnalyzer` / `EmotionalGatingService`: Processes text for emotion; filters/re-ranks retrieved memories based on emotional context and cognitive load.
    *   `ThresholdCalibrator`: Dynamically adjusts retrieval similarity thresholds.
    *   `TrainerIntegrationManager`: Handles API calls related to the trainer feedback loop.
*   **Status:** Core functionality implemented and stabilized. Vector index uses robust `IndexIDMap`. Surprise feedback loop is functional.

### 3.2. Neural Memory Server (`synthians_trainer_server` package)

*   **Role:** Adaptive associative memory, learning temporal patterns.
*   **Key Classes:**
    *   `NeuralMemoryModule`: TensorFlow/Keras model implementing Titans-style test-time learning (inner loop updates `M`, outer loop trains projections/gates).
    *   `MemoryMLP`: The internal MLP (M) storing associations.
    *   `http_server.py`: FastAPI server exposing NM functionality.
    *   `MetricsStore`: Collects operational metrics (updates, boosts, retrievals) and generates diagnostic reports.
    *   *(Surprise Calculation)*: The `/update_memory` endpoint calculates associative error (`loss`) and gradient magnitude (`grad_norm`), returning them as surprise metrics.
*   **Status:** Implemented with test-time learning. API supports variant interactions (`/get_projections`, `/calculate_gates`, modified `/update_memory`). Auto-initializes. TF/NumPy compatibility issues resolved via lazy loading.

### 3.3. Context Cascade Engine (`orchestrator` package)

*   **Role:** Orchestrates the cognitive cycle, manages history, and applies variant logic.
*   **Key Classes:**
    *   `ContextCascadeEngine`: Main orchestrator implementing the refactored flow.
    *   `SequenceContextManager`: Manages deque-based history `(ts, id, x, k, v, q, y_final)` for attention.
    *   `titans_variants.py`: Base class and specific logic for MAC, MAG, MAL variants, interacting with NM server API and `SequenceContextManager`.
*   **Status:** Implements the corrected flow for all variants. Dynamically configures attention based on NM server response. Manages history. Initiates QuickRecal boost feedback.

## 4. Core Concepts & Strategies

### 4.1. Embedding Handling

*   **Centralized Management:** `GeometryManager` handles validation, alignment, normalization, and distance/similarity calculations based on configured `embedding_dim` and `geometry_type`.
*   **Validation:** Checks for `None`, `NaN`, `Inf`. Invalid vectors are typically replaced with zero vectors and warnings logged.
*   **Dimension Alignment:** Handles mismatches (e.g., 384 vs 768) using configured `alignment_strategy` ('truncate' or 'pad') via `align_vectors`. Alignment occurs at API boundaries and before vector index operations.
*   **Normalization:** L2 normalization is typically applied before storage and similarity calculations.
*   **See:** `docs/core/embedding_handling.md`

### 4.2. Vector Indexing (FAISS `IndexIDMap`)

*   **Implementation:** `MemoryVectorIndex` uses `faiss.IndexIDMap` wrapping a base `IndexFlatL2` or `IndexFlatIP`.
*   **ID Management:** String `memory_id`s are hashed to unique `int64` numeric IDs for use with `add_with_ids`. `id_to_index` maps `string_id -> numeric_id`. `search` uses reverse mapping.
*   **Persistence:** The `.faiss` index file now stores vectors and numeric IDs together. `.mapping.json` serves as a backup for the string->numeric map.
*   **Integrity & Repair:** `verify_index_integrity` checks consistency. `migrate_to_idmap` converts legacy indices. `recreate_mapping` recovers the string->numeric map from backup or filesystem scan.
*   **See:** `docs/core/vector_index.md`

### 4.3. Titans Variants & Attention

*   **Orchestration:** The CCE manages the variant-specific flow.
*   **MAC (Post-Retrieval):** Enhances retrieved `y_t` using attention over historical `(k_i, y_i)`.
*   **MAG (Pre-Update):** Calculates attention over historical `k_i` to determine external gates (`alpha, theta, eta`) passed to `/update_memory`.
*   **MAL (Pre-Update):** Calculates attention over historical `(k_i, v_i)` to create `v'_t`, which replaces `v_t` in the `/update_memory` call.
*   **History:** `SequenceContextManager` stores `(ts, id, x, k, v, q, y_final)` required for attention.
*   **See:** `docs/orchestrator/titans_variants.md`

### 4.4. TensorFlow Lazy Loading

*   **Problem:** TensorFlow importing NumPy early caused version conflicts with `fix_numpy.py`.
*   **Solution:** A `_get_tf()` helper function in `titans_variants.py` delays `import tensorflow` until it's first needed, allowing NumPy downgrade to occur first. Code using TF calls `_get_tf()` instead of direct import.

### 4.5. Surprise Feedback Loop

*   **Mechanism:** NM Server's `/update_memory` returns `loss` and `grad_norm`. CCE calculates a `boost` value. CCE calls Memory Core's `/api/memories/update_quickrecal_score` endpoint with the target `memory_id` and `boost` value. The Memory Core service then updates the `quickrecal_score` and associated metadata for that specific memory entry.
*   **Impact:** Connects the adaptive learning of the Neural Memory directly to the relevance ranking within the Memory Core, allowing surprising or hard-to-associate memories to gain significance.
*   **Status:** Fully functional and tested end-to-end.

## 5. Current Status & Known Gaps

*   **Status:** Core architecture implemented. Bi-hemispheric loop with surprise feedback is functional. Vector index is robust using `IndexIDMap`. Retrieval pipeline is stabilized. Basic variant flows are implemented in CCE.
*   **Known Gaps:**
    *   **Variant Testing:** Dedicated integration tests needed for MAC, MAG, MAL effects.
    *   **Performance:** NM test-time update lacks parallelization.
    *   **Outer Loop Training:** NM `/train_outer` needs significant development for effective use.
    *   **Component Deep Dives:** Documentation for QuickRecal factors, Metadata Synthesizer pipeline, etc., needs more detail.
    *   **Configuration:** Ensure all key parameters are exposed via `CONFIGURATION_GUIDE.md`.
    *   **Test Teardown:** Investigate remaining background task cancellation warnings during test shutdown.

---

*(This document reflects the state as of late March 2025 and should be updated alongside major architectural changes.)*

```

# docs\archive\api_updates.md

```md
# API Updates for Phase 4

**Author:** Lucidia Core Team
**Date:** 2025-03-28
**Status:** Planned

## Overview

This document outlines the necessary API changes to complete Phase 4 of the Lucidia cognitive system. These changes enable the proper functioning of the Titans Architecture Variants (MAC, MAG, MAL) by exposing neural projections, supporting variant-specific parameters, and maintaining backward compatibility.

> *"The interface evolves to support the growing cognitive capabilities."*

## Neural Memory API Changes

### 1. New Endpoint: `/get_projections`

A new endpoint to calculate key, value, and query projections for an input embedding without updating memory.

#### Request Model

\`\`\`python
class GetProjectionsRequest(BaseModel):
    input_embedding: List[float]
\`\`\`

#### Response Model

\`\`\`python
class GetProjectionsResponse(BaseModel):
    status: str
    key_projection: Optional[List[float]] = None
    value_projection: Optional[List[float]] = None
    query_projection: Optional[List[float]] = None
\`\`\`

#### Implementation

\`\`\`python
@app.post("/get_projections")
async def get_projections(request: GetProjectionsRequest) -> GetProjectionsResponse:
    """Calculate key, value, and query projections for an input embedding without updating memory."""
    try:
        embedding = request.input_embedding
        embedding_np = np.array(embedding)
        
        # Get projections from neural memory module
        k_t, v_t, q_t = neural_memory_module.get_projections(embedding_np)
        
        return GetProjectionsResponse(
            status="success",
            key_projection=k_t.tolist(),
            value_projection=v_t.tolist(),
            query_projection=q_t.tolist()
        )
    except Exception as e:
        logger.error(f"Error in get_projections: {e}")
        return GetProjectionsResponse(status="error")
\`\`\`

### 2. New Endpoint: `/calculate_gates`

A new endpoint to calculate gate values based on attention output (for MAG variant).

#### Request Model

\`\`\`python
class CalculateGatesRequest(BaseModel):
    attention_output: List[float]
\`\`\`

#### Response Model

\`\`\`python
class CalculateGatesResponse(BaseModel):
    status: str
    alpha_t: Optional[float] = None
    theta_t: Optional[float] = None
    eta_t: Optional[float] = None
\`\`\`

#### Implementation

\`\`\`python
@app.post("/calculate_gates")
async def calculate_gates(request: CalculateGatesRequest) -> CalculateGatesResponse:
    """Calculate gate values based on attention output."""
    try:
        attention_output = np.array(request.attention_output)
        alpha_t, theta_t, eta_t = neural_memory_module.calculate_gates(attention_output)
        
        return CalculateGatesResponse(
            status="success",
            alpha_t=float(alpha_t),
            theta_t=float(theta_t),
            eta_t=float(eta_t)
        )
    except Exception as e:
        logger.error(f"Error in calculate_gates: {e}")
        return CalculateGatesResponse(status="error")
\`\`\`

### 3. Enhanced `/update_memory` Endpoint

Expand the existing endpoint to accept MAG gates and MAL modified projections.

#### Updated Request Model

\`\`\`python
class UpdateMemoryRequest(BaseModel):
    input_embedding: List[float]
    # MAG parameters (optional)
    alpha_t: Optional[float] = None
    theta_t: Optional[float] = None
    eta_t: Optional[float] = None
    # MAL parameters (optional)
    key_projection: Optional[List[float]] = None
    value_projection: Optional[List[float]] = None
\`\`\`

#### Updated Response Model

\`\`\`python
class UpdateMemoryResponse(BaseModel):
    status: str
    loss: Optional[float] = None
    grad_norm: Optional[float] = None
    key_projection: Optional[List[float]] = None
    value_projection: Optional[List[float]] = None
\`\`\`

#### Implementation Changes

\`\`\`python
@app.post("/update_memory")
async def update_memory(request: UpdateMemoryRequest) -> UpdateMemoryResponse:
    """Update neural memory with input embedding and optional custom parameters."""
    try:
        embedding = request.input_embedding
        embedding_np = np.array(embedding)
        
        # Handle MAG variant (external gates)
        external_gates = None
        if request.alpha_t is not None and request.theta_t is not None and request.eta_t is not None:
            external_gates = {
                "alpha_t": request.alpha_t,
                "theta_t": request.theta_t,
                "eta_t": request.eta_t
            }
        
        # Handle MAL variant (external key/value projections)
        key_projection = None
        value_projection = None
        if request.key_projection is not None:
            key_projection = np.array(request.key_projection)
        if request.value_projection is not None:
            value_projection = np.array(request.value_projection)
        
        # Update memory with appropriate parameters
        result = neural_memory_module.update_step(
            embedding_np,
            external_gates=external_gates,
            key_projection=key_projection,
            value_projection=value_projection
        )
        
        # Get projections for response
        k_t, v_t, _ = neural_memory_module.get_projections(embedding_np)
        
        return UpdateMemoryResponse(
            status="success",
            loss=float(result["loss"]),
            grad_norm=float(result["grad_norm"]),
            key_projection=k_t.tolist(),
            value_projection=v_t.tolist()
        )
    except Exception as e:
        logger.error(f"Error updating memory: {e}")
        return UpdateMemoryResponse(status="error")
\`\`\`

### 4. Enhanced `/retrieve` Endpoint

Update the existing endpoint to include query projection in the response.

#### Response Model Update

\`\`\`python
class RetrieveResponse(BaseModel):
    status: str
    retrieved_embedding: Optional[List[float]] = None
    query_projection: Optional[List[float]] = None  # New field
\`\`\`

#### Implementation Changes

\`\`\`python
@app.post("/retrieve")
async def retrieve(request: RetrieveRequest) -> RetrieveResponse:
    """Retrieve from neural memory using an input embedding."""
    try:
        embedding = request.input_embedding
        embedding_np = np.array(embedding)
        
        # Process query through neural memory module
        retrieved, q_t = neural_memory_module.retrieve(embedding_np, return_query=True)
        
        return RetrieveResponse(
            status="success",
            retrieved_embedding=retrieved.tolist(),
            query_projection=q_t.tolist()  # Include query projection
        )
    except Exception as e:
        logger.error(f"Error retrieving from memory: {e}")
        return RetrieveResponse(status="error")
\`\`\`

### 5. New Endpoint: `/config`

A new endpoint to retrieve configuration parameters, particularly for attention mechanism setup.

#### Response Model

\`\`\`python
class ConfigResponse(BaseModel):
    status: str
    key_dim: int
    value_dim: int
    query_dim: int
    recommended_attention_heads: int
    momentum_settings: Dict[str, float]
\`\`\`

#### Implementation

\`\`\`python
@app.get("/config")
async def get_config() -> ConfigResponse:
    """Get Neural Memory configuration parameters."""
    try:
        return ConfigResponse(
            status="success",
            key_dim=neural_memory_module.key_dim,
            value_dim=neural_memory_module.value_dim,
            query_dim=neural_memory_module.query_dim,
            recommended_attention_heads=4,  # Default recommendation
            momentum_settings={
                "alpha": neural_memory_module.alpha,
                "theta": neural_memory_module.theta,
                "eta": neural_memory_module.eta
            }
        )
    except Exception as e:
        logger.error(f"Error getting config: {e}")
        return ConfigResponse(status="error")
\`\`\`

## Neural Memory Module Changes

### 1. New Projection Helper Method

\`\`\`python
def get_projections(self, x_t):
    """Calculate key, value, and query projections for an input embedding.
    
    Args:
        x_t: Input embedding
        
    Returns:
        Tuple of (key_projection, value_projection, query_projection)
    """
    # Ensure input is properly shaped for TensorFlow
    x_t = self._prepare_input(x_t)
    
    # Calculate projections
    k_t = self.key_projection(x_t)
    v_t = self.value_projection(x_t)
    q_t = self.query_projection(x_t)
    
    return k_t.numpy(), v_t.numpy(), q_t.numpy()
\`\`\`

### 2. Enhanced Update Step Method

\`\`\`python
def update_step(self, x_t, external_gates=None, key_projection=None, value_projection=None):
    """Update memory with input embedding and optional external parameters.
    
    Args:
        x_t: Input embedding
        external_gates: Dict with keys 'alpha_t', 'theta_t', 'eta_t' for MAG variant
        key_projection: Optional external key projection for MAL variant
        value_projection: Optional external value projection for MAL variant
    
    Returns:
        Dict with loss and grad_norm
    """
    # Use provided projections if available, otherwise calculate them
    if key_projection is None or value_projection is None:
        k_t, v_t, q_t = self.get_projections(x_t)
        
        if key_projection is None:
            key_projection = k_t
        if value_projection is None:
            value_projection = v_t
    else:
        # Ensure query projection for metrics
        _, _, q_t = self.get_projections(x_t)
    
    # Use external gates if provided (MAG variant)
    alpha_t = self.alpha
    theta_t = self.theta  
    eta_t = self.eta
    
    if external_gates is not None:
        alpha_t = external_gates['alpha_t']
        theta_t = external_gates['theta_t']
        eta_t = external_gates['eta_t']
    
    # Perform update with potentially modified parameters
    with tf.GradientTape() as tape:
        # Forward pass through memory MLP
        tape.watch(self.memory_mlp.trainable_variables)
        pred_v = self.memory_mlp(q_t, training=True)
        
        # Calculate loss
        loss = 0.5 * tf.reduce_sum(tf.square(pred_v - value_projection))
    
    # Get gradients and update memory
    grads = tape.gradient(loss, self.memory_mlp.trainable_variables)
    grad_norm = self._calculate_grad_norm(grads)
    
    # Update momentum with gradient scaling and decay
    self._update_momentum(grads, theta_t, eta_t)
    
    # Apply momentum and forgetting to memory weights
    self._update_memory_weights(alpha_t)
    
    return {"loss": loss.numpy(), "grad_norm": grad_norm.numpy()}
\`\`\`

### 3. Enhanced Retrieve Method

\`\`\`python
def retrieve(self, x_t, return_query=False):
    """Retrieve from memory using input embedding.
    
    Args:
        x_t: Input embedding
        return_query: Whether to return the query projection
        
    Returns:
        Retrieved embedding or tuple of (retrieved_embedding, query_projection)
    """
    # Ensure input is properly shaped
    x_t = self._prepare_input(x_t)
    
    # Calculate query projection
    q_t = self.query_projection(x_t)
    
    # Forward pass through memory MLP
    y_t = self.memory_mlp(q_t, training=False)
    
    if return_query:
        return y_t.numpy(), q_t.numpy()
    else:
        return y_t.numpy()
\`\`\`

### 4. New Gate Calculation Method

\`\`\`python
def calculate_gates(self, attention_output):
    """Calculate gate values based on attention output for MAG variant.
    
    Args:
        attention_output: Output from attention mechanism
        
    Returns:
        Tuple of (alpha_t, theta_t, eta_t)
    """
    # Ensure input is properly shaped
    attention_output = self._prepare_input(attention_output)
    
    # Simple linear transformation and sigmoid activation
    # This is a placeholder implementation - actual gate calculation
    # might be more sophisticated based on specific MAG design
    gate_layer = tf.keras.layers.Dense(3, activation="sigmoid")
    gates = gate_layer(attention_output)
    
    # Extract individual gates (default range 0-1)
    alpha_t = gates[0, 0].numpy()  # Forget rate
    theta_t = gates[0, 1].numpy()  # Inner learning rate
    eta_t = gates[0, 2].numpy()    # Momentum decay
    
    # Scale to appropriate ranges based on default values
    alpha_t = alpha_t * 0.1        # Scale to 0-0.1 range
    theta_t = theta_t * 0.5        # Scale to 0-0.5 range
    eta_t = 0.9 + (eta_t * 0.09)   # Scale to 0.9-0.99 range
    
    return alpha_t, theta_t, eta_t
\`\`\`

## Neural Memory Client Changes

### 1. New Get Projections Method

\`\`\`python
async def get_projections(self, embedding):
    """Get key, value, and query projections for an input embedding."""
    try:
        response = await self.post(
            "/get_projections",
            {"input_embedding": embedding.tolist() if hasattr(embedding, 'tolist') else embedding}
        )
        return (
            np.array(response["key_projection"]),
            np.array(response["value_projection"]),
            np.array(response["query_projection"])
        )
    except Exception as e:
        logger.error(f"Error getting projections: {e}")
        return None, None, None
\`\`\`

### 2. New Calculate Gates Method

\`\`\`python
async def calculate_gates(self, attention_output):
    """Calculate gate values based on attention output."""
    try:
        response = await self.post(
            "/calculate_gates",
            {"attention_output": attention_output.tolist() if hasattr(attention_output, 'tolist') else attention_output}
        )
        return {
            "alpha_t": response["alpha_t"],
            "theta_t": response["theta_t"],
            "eta_t": response["eta_t"]
        }
    except Exception as e:
        logger.error(f"Error calculating gates: {e}")
        return None
\`\`\`

### 3. Enhanced Update Memory Method

\`\`\`python
async def update_memory(self, params):
    """Update neural memory with input embedding and optional parameters."""
    try:
        response = await self.post("/update_memory", params)
        return response
    except Exception as e:
        logger.error(f"Error updating memory: {e}")
        return {"status": "error", "error": str(e)}
\`\`\`

### 4. New Get Config Method

\`\`\`python
async def get_config(self):
    """Get Neural Memory configuration parameters."""
    try:
        response = await self.get("/config")
        return response
    except Exception as e:
        logger.error(f"Error getting config: {e}")
        return {"status": "error", "error": str(e)}
\`\`\`

## Context Cascade Engine Changes

### 1. Dynamic Attention Configuration

\`\`\`python
async def _initialize_attention_module(self):
    """Initialize attention module with dynamic configuration from Neural Memory server."""
    try:
        # Get configuration from Neural Memory server
        config_response = await self.neural_memory_client.get_config()
        
        if config_response["status"] == "success":
            # Calculate appropriate parameters
            key_dim = config_response["key_dim"]
            num_heads = config_response["recommended_attention_heads"]
            
            # Configure per-head dimension
            per_head_dim = max(key_dim // num_heads, 8)  # Ensure at least 8 dimensions per head
            
            # Create attention module
            self.attention_module = MultiHeadAttentionModule(
                num_heads=num_heads,
                key_dim=per_head_dim,
                use_layer_norm=True,
                use_residual=True
            )
            
            logger.info(f"Initialized attention module with {num_heads} heads, "
                       f"{per_head_dim} dimensions per head")
        else:
            # Fallback to default configuration
            logger.warning("Failed to get config from Neural Memory server. "
                          "Using default attention configuration.")
            self.attention_module = MultiHeadAttentionModule(
                num_heads=4,
                key_dim=32,
                use_layer_norm=True,
                use_residual=True
            )
    except Exception as e:
        logger.error(f"Error initializing attention module: {e}")
        # Fallback to default configuration
        self.attention_module = MultiHeadAttentionModule(
            num_heads=4,
            key_dim=32,
            use_layer_norm=True,
            use_residual=True
        )
\`\`\`

## MetricsStore Fix

### Fix for format_diagnostics_as_table Method

\`\`\`python
def format_diagnostics_as_table(self):
    """Format diagnostics data as a markdown table for display."""
    if not self.diagnostics:
        return "No diagnostics data available."
    
    # Ensure data_points exists with a default empty list
    if "data_points" not in self.diagnostics:
        self.diagnostics["data_points"] = []
    
    # Process data points
    data_points = self.diagnostics["data_points"]
    if not data_points:
        return "No data points in diagnostics."
    
    # Create table header
    headers = list(data_points[0].keys())
    table = "| " + " | ".join(headers) + " |\n"
    table += "| " + " | ".join(["---" for _ in headers]) + " |\n"
    
    # Add data rows
    for point in data_points:
        table += "| " + " | ".join([str(point.get(h, "")) for h in headers]) + " |\n"
    
    return table
\`\`\`

## Backward Compatibility Considerations

1. **NONE Variant Support**: The refactored flow must continue to work with the "NONE" variant, which represents the original Phase 3 implementation without attention mechanisms.

2. **Default Gate Values**: When no external gates are provided (non-MAG variants), the system should use the default gate values from the Neural Memory configuration.

3. **Optional Parameters**: All new parameters in API requests should be optional to maintain compatibility with existing clients.

4. **Error Handling**: Enhanced error handling is needed to gracefully handle clients that do not send the expected parameters or handle the enhanced responses.

## Testing Strategy

### 1. API Endpoint Tests

Create comprehensive tests for each new and modified endpoint:

\`\`\`python
async def test_get_projections_endpoint():
    """Test the /get_projections endpoint."""
    client = NeuralMemoryClient(...)
    
    # Test with valid embedding
    embedding = np.random.random(128).astype(np.float32)
    k_t, v_t, q_t = await client.get_projections(embedding)
    
    assert k_t is not None and len(k_t) > 0
    assert v_t is not None and len(v_t) > 0
    assert q_t is not None and len(q_t) > 0
    
    # Test with invalid embedding (e.g., NaN values)
    embedding_with_nan = np.array([np.nan] * 128).astype(np.float32)
    k_t, v_t, q_t = await client.get_projections(embedding_with_nan)
    
    # Should handle NaN gracefully
    assert k_t is not None
\`\`\`

### 2. Integration Tests

Create tests that verify the complete flow for each variant:

\`\`\`python
async def test_mag_variant_integration():
    """Test the complete MAG variant flow."""
    # Set environment variable
    os.environ["TITANS_VARIANT"] = "MAG"
    
    # Initialize components
    memory_client = MemoryCoreClient(...)
    neural_memory_client = NeuralMemoryClient(...)
    cce = ContextCascadeEngine(...)
    
    # Process a sequence of inputs
    embeddings = [np.random.random(128).astype(np.float32) for _ in range(5)]
    results = []
    
    for embedding in embeddings:
        result = await cce.process_new_input(embedding)
        results.append(result)
    
    # Verify gate values are being applied
    # (This would require instrumenting the neural_memory_module to expose actual gate values used)
\`\`\`

## Conclusion

The API updates outlined in this document provide the necessary foundation for completing Phase 4 of the Lucidia cognitive system. These changes enable the Titans Architecture Variants to function correctly, with proper timing and information flow between components.

The enhanced API maintains backward compatibility while adding the flexibility needed for the attention-based variants. The addition of configuration endpoints and improved diagnostics will facilitate easier integration, monitoring, and debugging.

Implementing these changes will resolve the critical MAG/MAL timing issue identified in the codebase review, allowing all variants to function as designed and completing the Phase 4 implementation.

---

**Related Documentation:**
- [Phase 4 Implementation](phase_4_implementation.md)
- [Attention](attention.md)
- [Titans Variant Refactor](titans_variant_refactor.md)

```

# docs\archive\architecture_overview.md

```md
# Bi-Hemispheric Architecture Overview

## Introduction

The Synthians Memory Core implements a Bi-Hemispheric Cognitive Architecture that separates memory storage/retrieval from sequence prediction/surprise detection, mimicking how the brain's hemispheres handle different aspects of cognition. This document provides a technical overview of the architecture, component interactions, and the information flow between them.

## System Components

### 1. Memory Core

The Memory Core serves as the primary memory storage and retrieval system, similar to the brain's hippocampus and temporal lobes.

**Key Responsibilities:**
- Storing and indexing memory entries with associated embeddings and metadata
- Retrieval of memories based on semantic similarity and quickrecal scores
- Memory assembly management and persistence
- Emotional gating of memory retrieval based on emotional context
- Maintaining memory importance through quickrecal scores

**Key Classes:**
- `SynthiansMemoryCore`: Main interface for all memory operations
- `MemoryEntry`: Individual memory representation with embedding and metadata
- `MemoryAssembly`: Collection of related memories with a composite embedding
- `MemoryPersistence`: Handles saving and loading memories and assemblies
- `EmotionalGatingService`: Applies emotional context to memory retrieval

### 2. Trainer Server

The Trainer Server handles sequence prediction and surprise detection, similar to the brain's frontal lobes and predictive capabilities.

**Key Responsibilities:**
- Predicting the next embedding in a sequence using neural mechanisms
- Calculating surprise when expectations don't match reality
- Training on memory sequences to improve predictions
- Maintaining a stateless architecture that relies on explicit memory state passing

**Key Classes:**
- `SynthiansTrainer`: Neural model for sequence prediction
- `SurpriseDetector`: Detects and analyzes surprise in embedding sequences
- `HPCQRFlowManager`: Manages the QuickRecal factors for memory importance
- `NeuralMemoryModule`: Provides key-value-query projections and memory update mechanisms

### 3. Context Cascade Engine (Orchestrator)

The Context Cascade Engine connects the Memory Core and Trainer Server, orchestrating the flow of information between them and implementing the full cognitive cycle.

**Key Responsibilities:**
- Processing new memories through the complete cognitive pipeline
- Managing the interplay between prediction and memory storage
- Feeding surprise feedback to enhance memory retrieval
- Handling error states and coordinating between components
- Orchestrating variant-specific processing pathways (Titans variants)

**Key Classes:**
- `ContextCascadeEngine`: Main orchestrator class with modular processing methods
- `GeometryManager`: Shared utility for consistent vector operations across components
- `TitansVariantBase`: Base class for all variant-specific processing
- `SequenceContextManager`: Manages historical context for attention-based operations

## Titans Architecture Variants

The system supports multiple cognitive processing variants through the Titans Architecture framework. Each variant implements different attention mechanisms and memory update strategies:

### NONE Variant (Default)

The standard cognitive flow without additional attention mechanisms.

**Key Characteristics:**
- Direct memory storage and retrieval
- Standard Neural Memory updates without attention-based modifications
- Baseline for comparison with other variants

### MAC Variant (Memory-Attended Content)

Enhances retrieved content using attention mechanisms over historical memory.

**Key Characteristics:**
- Processes input through standard Neural Memory update
- Applies attention between query and historical keys to modify retrieved output
- Post-retrieval enhancement of memory content

**Processing Flow:**
1. Standard memory update and retrieval
2. Apply attention between current query and historical keys
3. Create attended_y_t by combining retrieved and historical values
4. Return the attention-modified retrieved embedding

### MAG Variant (Memory-Attended Gates)

Modifies Neural Memory update gate values using attention mechanisms.

**Key Characteristics:**
- Calculates Neural Memory gate values (α, θ, η) using attention
- These gates control forgetting rate, learning rate, and momentum decay
- Pre-update influence on memory storage

**Processing Flow:**
1. Calculate projections from input
2. Apply attention between query and historical keys
3. Compute gate values from attention output
4. Update Neural Memory with custom gates
5. Standard memory retrieval

### MAL Variant (Memory-Attended Learning)

Modifies the value projection used in Neural Memory updates using attention.

**Key Characteristics:**
- Modifies the value projection (v_t) before Neural Memory update
- Uses attention between current query and historical keys/values
- Creates an enhanced representation for memory storage

**Processing Flow:**
1. Calculate projections from input
2. Apply attention between query and historical keys/values
3. Calculate modified value projection (v_prime) by combining original and attended values
4. Update Neural Memory with modified value projection
5. Standard memory retrieval

## Information Flow

### Refactored Cognitive Cycle

1. **Input Processing:**
   - New memory content and optional embedding arrive at the Context Cascade Engine
   - The Engine forwards the memory to the Memory Core for storage
   - Memory ID and embedding (x_t) are returned

2. **Projections and Variant Pre-Processing:**
   - The Engine obtains key, value, and query projections (k_t, v_t, q_t) from Neural Memory
   - For MAG: Calculate attention-based gates
   - For MAL: Calculate modified value projection

3. **Neural Memory Update:**
   - For NONE/MAC: Standard update with input embedding
   - For MAG: Include calculated gates in update
   - For MAL: Use modified value projection in update
   - Loss and gradient norm metrics are returned

4. **QuickRecal Feedback:**
   - Surprise metrics (loss, grad_norm) are used to calculate QuickRecal boost
   - Memory Core updates the memory's QuickRecal score accordingly

5. **Retrieval and Post-Processing:**
   - Neural Memory retrieves relevant embedding based on input
   - For MAC: Apply attention over historical context to modify retrieved embedding

6. **History Update:**
   - All context (embeddings, projections, results) is added to sequence history
   - This enriches the historical context for future attention operations

## Embedding Handling and Dimension Alignment

The system includes robust handling for embedding-related challenges:

### Dimension Mismatches

The system gracefully handles dimension mismatches between embeddings (e.g., 384 vs 768 dimensions):

- **Vector Alignment Utility**: Automatically aligns vectors to the same dimension for comparison operations
- **Normalization Methods**: Safe normalization with dimension handling (padding/truncation as needed)
- **Validation**: Detection and handling of malformed embeddings (NaN/Inf values)

**Implementation Details:**
- The `_align_vectors_for_comparison` method handles dimension mismatches
- The `_normalize_embedding` methods in multiple classes handle padding or truncation
- The `_validate_embedding` method checks for NaN/Inf values and provides fallbacks

### Embedding Conversion

The system includes utility methods to handle various embedding formats:

- `_to_list`: Safely converts numpy arrays, TensorFlow tensors, and other array-like objects to Python lists
- `_to_numpy`: Ensures consistent numpy array format for internal processing

## TensorFlow Lazy Loading

To prevent NumPy version conflicts, the system implements lazy loading for TensorFlow:

\`\`\`python
# Global variable for TensorFlow instance
_tf = None

def _get_tf():
    """Lazy-load TensorFlow only when needed."""
    global _tf
    if _tf is None:
        try:
            import tensorflow as tf
            _tf = tf
        except ImportError as e:
            logger.error(f"Failed to import TensorFlow: {e}")
    return _tf
\`\`\`

**Benefits:**
- Prevents NumPy version conflicts by deferring TensorFlow imports
- Allows the `fix_numpy.py` script to downgrade NumPy before TensorFlow is imported
- Keeps TensorFlow isolated to only those components that require it
- Enables all variants to function correctly regardless of NumPy version requirements

## Stateless Design Pattern

A key refinement in the architecture is the stateless design of the Trainer Server:

1. **No Global State:**
   - The Trainer Server maintains no session or global state
   - Each prediction request must include all necessary context

2. **Memory State Passing:**
   - The `previous_memory_state` parameter contains the state from the last prediction
   - This state includes sequence history, surprise metrics, and momentum
   - The response includes a new `memory_state` to be passed in the next request

3. **Orchestrator State Management:**
   - The Context Cascade Engine manages the memory state between requests
   - It stores the state returned by the Trainer and passes it in the next prediction

4. **Benefits:**
   - Improved scalability through horizontal scaling of the Trainer Server
   - Enhanced reliability as state is not dependent on server uptime
   - Simplified debugging and state inspection
   - Easier deployment and migration without state loss

## Memory Assemblies

Memory Assemblies represent related memories that are grouped together for enhanced retrieval and semantic organization.

1. **Creation and Composition:**
   - Assemblies can be created with initial memories or built incrementally
   - Each assembly maintains a composite embedding representing its semantic center
   - When memories are added, the composite embedding is updated

2. **Retrieval Benefits:**
   - Assemblies improve recall by activating related memories
   - They provide context for ambiguous queries
   - They enable higher-level semantic organization beyond individual memories

3. **Dynamic Updates:**
   - Assemblies can evolve over time as new memories are added
   - The system can merge similar assemblies or split diverging ones
   - Assembly strength is determined by member coherence and usage patterns

## Implementation and Integration Guidelines

1. **Component Communication:**
   - All inter-component communication uses well-defined APIs
   - The Context Cascade Engine handles all orchestration
   - Components should not directly communicate with each other

2. **Error Handling:**
   - Each component implements comprehensive error handling
   - The orchestrator manages overall system stability
   - Graceful degradation is provided when components are unavailable

3. **Configuration:**
   - Each component has its own configuration
   - The orchestrator manages system-wide settings
   - Environment variables like `TITANS_VARIANT` control high-level behavior

4. **Monitoring and Diagnostics:**
   - Each component provides health and performance metrics
   - The `lucidia_think_trace` tool offers system-wide diagnostics
   - Logging is standardized across components for easy aggregation

```

# docs\archive\bihemispheric_architecture.md

```md
# Bi-Hemispheric Cognitive Architecture

## Overview

The Bi-Hemispheric Cognitive Architecture implements a neural system inspired by human brain hemispheric specialization, creating a bidirectional flow between memory storage/retrieval and sequential prediction. This architecture enables Lucidia to develop a more nuanced understanding of sequential patterns and adapt memory retrieval based on prediction accuracy and surprise detection.

## Key Components

### 1. Memory Core (Left Hemisphere)

Responsible for storing, indexing, retrieving, and enriching memories:

- **Memory Storage**: Persists embeddings and metadata to disk
- **Vector Indexing**: Enables fast similarity-based retrieval using FAISS
- **Metadata Enrichment**: Adds contextual information to memories
- **Emotional Analysis**: Detects emotions in content and uses them for retrieval
- **HPC-QR**: Hippocampal-inspired Quick Recall scoring system

### 2. Trainer Server (Right Hemisphere)

Focuses on pattern recognition and sequence prediction:

- **Sequence Prediction**: Predicts the next embedding based on current input
- **Memory State Tracking**: Maintains internal memory state to track context
- **Surprise Analysis**: Detects unexpected patterns in embedding sequences

### 3. Context Cascade Engine (Corpus Callosum)

Orchestrates the bidirectional flow between the two hemispheres:

- **Prediction Integration**: Feeds predictions from the Trainer into Memory Core retrieval
- **Surprise Detection**: Identifies when reality diverges from predictions
- **Memory Enhancement**: Updates memory importance based on surprise signals
- **State Management**: Tracks the Trainer's memory state across interactions

## Neural Pathway Flow

1. **Input Processing**: New input is processed and embedded by the Memory Core
2. **Prediction**: Context Cascade Engine sends current embedding to Trainer for next embedding prediction
3. **Reality Check**: When new input arrives, it's compared against the prediction
4. **Surprise Detection**: Difference between prediction and reality is quantified
5. **Feedback Loop**: Surprising memories get importance boosts in Memory Core
6. **Retrieval Enhancement**: Future retrievals prioritize memories that were surprising

## Key Innovations

1. **Vector Alignment**: System handles embedding dimension mismatches (384 vs 768) seamlessly
2. **Surprise Metrics**: Measures both prediction error and context shifts
3. **Adaptive Thresholds**: Surprise detection adapts to current narrative volatility
4. **Memory State Continuity**: Maintains continuity of the prediction model's internal state
5. **Quickrecal Boosting**: Automatically enhances the retrieval priority of surprising memories

## Architecture Diagram

\`\`\`
┌───────────────────┐              ┌─────────────────────┐
│   Memory Core     │              │   Trainer Server    │
│  (Left Hemisphere)│              │  (Right Hemisphere) │
│                   │              │                     │
│ ┌───────────────┐ │              │ ┌─────────────────┐ │
│ │   GeometryMgr │ │              │ │GeometryMgr (ref)│ │
│ └───────────────┘ │              │ └─────────────────┘ │
│ ┌───────────────┐ │              │ ┌─────────────────┐ │
│ │  VectorIndex  │ │              │ │SequencePredictor│ │
│ └───────────────┘ │              │ └─────────────────┘ │
│ ┌───────────────┐ │              │ ┌─────────────────┐ │
│ │   MetadataSyn │ │              │ │ SurpriseDetector│ │
│ └───────────────┘ │              │ └─────────────────┘ │
└────────┬──────────┘              └──────────┬──────────┘
         │                                     │
         │        ┌──────────────────┐        │
         │        │ Context Cascade  │        │
         └────────┤     Engine      ├────────┘
                  │ (Corpus Callosum)│
                  └──────────────────┘
\`\`\`

## Implementation Notes

- The system is designed to handle embedding dimension mismatches, a critical requirement for systems using different embedding models
- The GeometryManager is shared across components to ensure vector operations are consistent
- All communication between components uses asynchronous HTTP calls with proper timeouts and error handling
- Memory state is preserved between calls to maintain prediction continuity
- The system adapts to the current context's volatility when determining surprise thresholds

```

# docs\archive\index_repair_implementation.md

```md
# Memory Index Repair Implementation Details

## Technical Summary

This document provides a detailed overview of the implementation for fixing inconsistencies between the FAISS vector count and ID mapping in the Synthians Memory Core system.

## Key Changes

### 1. Enhanced Vector Extraction in Migration Process

The core issue was resolved by adding a robust "sequential extraction" strategy to the `migrate_to_idmap` method. This strategy handles the case where vectors exist in the FAISS index but no ID mappings are available.

**Key Implementation:**

\`\`\`python
# Special case handling for orphaned vectors (vectors without ID mappings)
if original_count > 0 and len(old_id_to_index) == 0:
    # 1. Search for real memory IDs in filesystem
    # 2. Generate synthetic IDs if needed
    # 3. Extract vectors sequentially using index.reconstruct
    # 4. Build a new consistent mapping
\`\`\`

This approach solved a critical issue where the system would fail to extract vectors during migration when mappings were missing, leading to a loss of vector data.

### 2. Improved Mapping Reconstruction

The `recreate_mapping` method was enhanced to include a more robust recovery strategy:

1. First attempts to restore mappings from backup files
2. If backup is unavailable, tries to reconstruct from memory files
3. Includes a last-resort fallback that generates sequential mappings

### 3. Repair Logic in SynthiansMemoryCore

Updated the `repair_index` method to:

1. Check initial consistency state before attempting repairs
2. Consider an already-consistent index as a successful outcome
3. Determine overall success based on both repair operation and final consistency state

\`\`\`python
# Determine overall success: either repair succeeded or the index is now consistent
overall_success = success or is_consistent_after
\`\`\`

### 4. Enhanced Error Handling

Added more detailed error handling and logging throughout the repair process:

1. Comprehensive tracebacks for debugging
2. Clear status messages for each repair stage
3. Improved diagnostics for troubleshooting

## Implementation Benefits

1. **Reliability**: The system can now recover from previously unrecoverable index inconsistencies
2. **Data Preservation**: Vector data is preserved even when ID mappings are lost
3. **Automatic Recovery**: Repairs happen automatically during system startup
4. **Better Diagnostics**: Enhanced logging and error reporting

## Testing Results

The implementation was successfully tested with a real-world case where:

1. The FAISS index contained 56 vectors
2. The ID mapping dictionary was empty (0 entries)

Test logs showed a successful recovery:

\`\`\`
Vector index inconsistency detected! FAISS count: 56, Mapping count: 0
Using sequential extraction for index with no ID mappings
Extracted 56 vectors using sequential extraction
Successfully migrated 56 vectors to IndexIDMap
\`\`\`

## PowerShell Considerations

When running repair scripts or chaining commands in a PowerShell environment, remember to use semicolons (`;`) instead of the `&&` operator for command chaining, as per system requirements.

```

# docs\archive\index_repair_system.md

```md
# Memory Index Repair System

## Overview

The Memory Index Repair System is a critical enhancement to the Synthians Memory Core that ensures consistency between the FAISS vector index and memory ID mappings. This document explains the implementation details, repair strategies, and recovery mechanisms.

## Problem Statement

When using FAISS with `IndexIDMap` for memory retrieval, inconsistencies can occur between:
1. The number of vectors stored in the FAISS index
2. The number of memory ID mappings maintained in the system

These inconsistencies can cause several issues:
- Failed memory retrievals
- Incorrect similarity scores
- Inability to update or delete memories properly
- System instability during scale-up

## Key Components

### 1. Auto-Detection System

The system automatically detects inconsistencies during:
- Startup initialization
- Index loading
- Before critical operations (search, add)

The detection logic is implemented in `verify_index_integrity()` which returns:
- A boolean indicating consistency status
- Detailed diagnostics about the index state

### 2. Repair Strategies

The system implements multiple repair strategies:

#### a. ID Mapping Recreation

When the FAISS index contains vectors but the ID mapping is missing or corrupt:

1. First tries to recover from backup mapping files
2. If no backup exists, scans memory directories to obtain memory IDs
3. If neither option works, generates synthetic IDs for the vectors

#### b. Index Migration

When the index needs to be upgraded to use `IndexIDMap` for improved ID management:

1. Standard Migration: Uses existing ID mappings to extract vectors and rebuild
2. Sequential Extraction: For orphaned vectors (vectors without mappings), extracts vectors from the index sequentially and assigns new IDs
3. Direct Access: For CPU indices, can directly access vector data for migration

#### c. Full Rebuild (Last Resort)

If other repair strategies fail, the system can perform a more drastic rebuild by:
- Generating synthetic ID mappings for all vectors in the index
- Creating a fresh backup mapping file

### 3. Recovery Workflow

The recovery process follows this sequence:

1. Detect inconsistency through integrity check
2. Evaluate best repair strategy based on diagnostics
3. Attempt repair using selected strategy
4. Verify success through post-repair integrity check
5. Update the mapping backup file

## Implementation Details

### Enhanced Migrate to IndexIDMap

The `migrate_to_idmap()` method has been enhanced to handle various edge cases:

\`\`\`python
def migrate_to_idmap(self, force_cpu: bool = True) -> bool:
    # ... existing code ...
    
    # Special case: If we have vectors but no ID mapping, we need a special approach
    if original_count > 0 and len(old_id_to_index) == 0:
        # Implements sequential extraction for indices with missing mappings
        # Attempts to find real memory IDs from files
        # Falls back to synthetic ID generation if necessary
    
    # ... standard migration approaches ...
\`\`\`

### Recreate Mapping Enhancement

The `recreate_mapping()` method now implements multiple recovery paths:

\`\`\`python
def recreate_mapping(self) -> bool:
    # 1. Try to read the backup mapping file
    # 2. If no backup exists, reconstruct from memory directories
    # 3. Generate consistent numeric IDs for all memories
    # 4. As last resort, generate sequential mappings
\`\`\`

### Automatic Repair in Core Initialization

The `SynthiansMemoryCore` initialization process now includes automatic repair:

\`\`\`python
async def _initialize(self):
    # ... existing initialization ...
    
    # Check vector index integrity
    is_consistent, diagnostics = self.vector_index.verify_index_integrity()
    
    if not is_consistent:
        # Handle critical inconsistencies
        # Initiate automatic repair
\`\`\`

## Practical Example

Example scenario of auto-repair with orphaned vectors:

\`\`\`
2025-03-30 17:39:58,654 - WARNING - Vector index inconsistency detected! FAISS count: 56, Mapping count: 0
2025-03-30 17:39:58,660 - INFO - Using sequential extraction for index with no ID mappings
2025-03-30 17:39:58,665 - INFO - Extracted 56 vectors using sequential extraction
2025-03-30 17:39:58,671 - INFO - Successfully migrated 56 vectors to IndexIDMap
\`\`\`

## Future Enhancements

Future improvements to the repair system may include:

1. Periodic automated integrity checks during system operation
2. More sophisticated fallback methods if primary repair strategies fail
3. Telemetry for repair operations to track long-term system health
4. Integration with emotional gating system to preserve memory emotional context during repairs

## Best Practices

1. Run preventative index checks during system idle periods
2. Maintain regular backups of the ID mapping file
3. When adding vector embeddings, always ensure ID mappings are properly maintained
4. Verify index integrity after bulk operations or migrations

```

# docs\archive\mac_variant_implementation.md

```md
# MAC Variant Implementation Guide

## Overview

The Memory-Attended Content (MAC) variant is a specialized architecture in the Lucidia Cognitive System that enhances retrieved memory embeddings using attention mechanisms over historical context. This document details the implementation, integration, and usage of the MAC variant within the refactored Context Cascade Engine.

## Architecture

The MAC variant follows this processing flow:

1. Retrieve raw embedding from Neural Memory → Get `y_t` (raw retrieval)
2. `q_t`, `y_t` + Historical context (K_hist, Y_hist) → Attend(q_t, K_hist, Y_hist) → `attended_y_t`
3. Return `attended_y_t` as enhanced memory representation

![MAC Architecture](../assets/diagrams/mac_architecture.png)

## Implementation Details

### Core Components

1. **TitansVariantBase**
   - Provides common infrastructure for all variants
   - Handles API client initialization and neural memory URL configuration
   - Manages sequence context and historical context tracking
   - Implements lazy loading for TensorFlow to prevent NumPy version conflicts

2. **MACVariant Class**
   - Implements the Memory-Attended Content logic
   - Initializes attention modules for output enhancement
   - Processes query embeddings and retrieved outputs through attention mechanisms
   - Applies attention over historical keys and values to enhance retrieved memory

3. **ContextCascadeEngine**
   - Orchestrates the variant selection and initialization
   - Routes memory operations through the appropriate variant
   - Invokes MAC processing *after* Neural Memory retrieval
   - Updates sequence history with the enhanced output

### Key Methods

#### MACVariant

\`\`\`python
async def process_output(self, q_t: np.ndarray, y_t: np.ndarray) -> Dict[str, Any]:
    """Process output through MAC variant logic to enhance retrieved memory.
    
    Args:
        q_t: Query projection from Neural Memory
        y_t: Raw retrieved embedding from Neural Memory
    
    Returns:
        Dict containing attended output and metrics
    """
    try:
        # Get historical keys and values for attention calculation
        k_hist = self.sequence_context.get_recent_keys()
        y_hist = self.sequence_context.get_recent_outputs()
        
        if not k_hist or len(k_hist) == 0 or not y_hist or len(y_hist) == 0:
            logger.warning("No historical context available for MAC attention")
            return {"status": "error", "error": "No historical context available"}
        
        # Apply attention between query and historical keys
        attention_output = self.compute_attention(
            query=q_t,
            keys=k_hist,
            values=y_hist
        )
        
        # Combine retrieved embedding with attention output
        attended_y_t = self.combine_outputs(y_t, attention_output)
        
        return {
            "status": "success",
            "attended_y_t": attended_y_t,
            "metrics": {
                "attention_magnitude": float(np.linalg.norm(attention_output)),
                "combination_ratio": self.combination_ratio
            }
        }
    except Exception as e:
        logger.error(f"Error in MAC variant processing: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

#### Integration with ContextCascadeEngine

The refactored ContextCascadeEngine handles the MAC variant by applying its processing *after* Neural Memory retrieval, enhancing the retrieved content before returning it:

\`\`\`python
async def _apply_variant_post_retrieval(self, step_context):
    """Apply variant-specific post-retrieval processing for MAC variant.
    
    Args:
        step_context: The current processing context
        
    Returns:
        Dict containing variant processing results
    """
    try:
        if self.active_variant_type == TitansVariantType.MAC:
            # Process MAC variant: Enhance retrieved embedding with attention
            mac_result = await self.variant_processor.process_output(
                step_context["q_t"], step_context["y_t"]
            )
            
            if "attended_y_t" in mac_result:
                # Replace retrieved embedding with attention-enhanced version
                step_context["y_t"] = mac_result["attended_y_t"]
                step_context["y_t_list"] = self._to_list(mac_result["attended_y_t"])
                logger.info("MAC variant produced attended output")
            else:
                logger.warning(f"MAC variant processing failed: {mac_result.get('error')}")
                
            return mac_result
            
        return {"status": "not_applicable"}
    except Exception as e:
        logger.error(f"Error in _apply_variant_post_retrieval: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

### Attention Mechanism

The MAC variant uses a multi-head attention mechanism to determine the relevance of historical memory embeddings to the current query:

\`\`\`python
def compute_attention(self, query, keys, values):
    """Compute attention between query and historical keys/values.
    
    Args:
        query: Current query embedding (q_t)
        keys: Historical key embeddings (k_hist)
        values: Historical value or output embeddings (y_hist)
    
    Returns:
        Attention-weighted combination of values
    """
    tf = _get_tf()  # Lazy load TensorFlow
    
    # Ensure inputs are properly shaped for attention
    query = tf.expand_dims(tf.convert_to_tensor(query, dtype=tf.float32), axis=0)  # [1, dim]
    keys = tf.convert_to_tensor(keys, dtype=tf.float32)  # [seq_len, dim]
    keys = tf.expand_dims(keys, axis=0)  # [1, seq_len, dim]
    values = tf.convert_to_tensor(values, dtype=tf.float32)  # [seq_len, dim]
    values = tf.expand_dims(values, axis=0)  # [1, seq_len, dim]
    
    # Apply attention
    attention_output = self.attention_layer(
        query=query,  # [1, 1, dim]
        key=keys,     # [1, seq_len, dim]
        value=values  # [1, seq_len, dim]
    )
    
    # Remove batch dimension [1, 1, dim] -> [dim]
    return tf.squeeze(attention_output).numpy()
\`\`\`

### Embedding Handling

The MAC variant includes robust handling for embedding dimension mismatches and malformed embeddings:

1. **Dimension Alignment**: Uses the `_align_vectors_for_comparison` method to handle mismatches between 384D and 768D embeddings
2. **Validation**: Validates embeddings to detect and handle NaN/Inf values
3. **Safe Conversion**: Properly handles different tensor types when converting between TensorFlow and NumPy

\`\`\`python
def _align_vectors(self, vector_a, vector_b):
    """Align vectors to the same dimension for processing.
    
    Handles dimension mismatches by padding smaller vectors with zeros
    or truncating larger vectors.
    
    Args:
        vector_a: First vector
        vector_b: Second vector to align with
        
    Returns:
        Tuple of aligned vectors (a_aligned, b_aligned)
    """
    a_dim = vector_a.shape[-1]
    b_dim = vector_b.shape[-1]
    
    if a_dim == b_dim:
        return vector_a, vector_b
    
    if a_dim < b_dim:
        # Pad vector_a to match vector_b
        padding = np.zeros(b_dim - a_dim)
        a_aligned = np.concatenate([vector_a, padding])
        return a_aligned, vector_b
    else:
        # Truncate vector_a to match vector_b
        return vector_a[:b_dim], vector_b
\`\`\`

## Testing the MAC Variant

To test the MAC variant, you can use the `lucidia_think_trace` tool with the appropriate environment variable:

\`\`\`bash
# Run in Docker container
docker exec -e TITANS_VARIANT=MAC trainer-server python -m synthians_memory_core.tools.lucidia_think_trace --query "Testing MAC variant" --memcore-url "http://host.docker.internal:5010"
\`\`\`

The output should show:

1. Successful Neural Memory retrieval
2. Proper enhancement of retrieved embedding via attention
3. Modified retrieved embedding in the response

## Activation

To activate the MAC variant, set the `TITANS_VARIANT` environment variable:

\`\`\`bash
export TITANS_VARIANT=MAC  # For Linux/macOS
set TITANS_VARIANT=MAC      # For Windows CMD
\`\`\`

In the Docker setup, you can specify this when starting the container:

\`\`\`bash
docker run -e TITANS_VARIANT=MAC ...
\`\`\`

## Common Issues and Troubleshooting

### Insufficient Historical Context

The MAC variant requires historical keys and values to calculate attention. If there isn't enough historical context, you might see warnings like:

\`\`\`
No historical context available for MAC attention
\`\`\`

Solution: Ensure that multiple inputs have been processed through the system before expecting MAC to enhance memory retrieval.

### TensorFlow Import Errors

If you encounter errors related to TensorFlow imports or NumPy version conflicts, verify that:

1. The lazy loading mechanism is correctly implemented
2. The fix_numpy.py script has run before any TensorFlow imports

## Conclusion

The MAC variant implementation enhances memory retrieval by using attention mechanisms to incorporate relevant historical context into retrieved embeddings. This approach provides several benefits:

1. Improved contextual relevance of retrieved memories
2. Enhanced continuity across sequential memory operations
3. Reduced retrieval errors by incorporating complementary information from past retrievals

By applying attention *after* the Neural Memory update and retrieval, MAC focuses on enhancing the usefulness of retrieved content rather than modifying how memories are stored.

```

# docs\archive\mag_variant_implementation.md

```md
# MAG Variant Implementation Guide

## Overview

The Memory-Attended Gates (MAG) variant is a specialized architecture in the Lucidia Cognitive System that modifies the gate values used in the Neural Memory update process through attention mechanisms. This document details the implementation, integration, and usage of the MAG variant within the refactored Context Cascade Engine.

## Architecture

The MAG variant follows this processing flow:

1. `q_t` → Attend(q_t, K_hist, K_hist) → `attention_output`
2. Call Neural Memory's `/calculate_gates` endpoint with attention output
3. Update memory with calculated gates

![MAG Architecture](../assets/diagrams/mag_architecture.png)

## Implementation Details

### Core Components

1. **TitansVariantBase**
   - Provides common infrastructure for all variants
   - Handles API client initialization and neural memory URL configuration
   - Manages sequence context and historical context tracking
   - Implements lazy loading for TensorFlow to prevent NumPy version conflicts

2. **MAGVariant Class**
   - Implements the Memory-Attended Gates logic
   - Initializes attention modules for gate calculation
   - Processes input embeddings and queries through attention mechanisms
   - Calculates attention-based gate values to influence Neural Memory updates

3. **NeuralMemoryModule**
   - Provides gate calculation capabilities via dedicated projection layers
   - Processes attention outputs to compute optimal gate values
   - Applies external gate values during memory updates
   - Returns loss and gradient norm metrics for QuickRecal boosting

4. **ContextCascadeEngine**
   - Orchestrates the variant selection and initialization
   - Routes memory operations through the appropriate variant
   - Manages the flow of data between components
   - Ensures correct sequencing of operations to maximize variant effectiveness

### Key Methods

#### MAGVariant

\`\`\`python
async def process_input(self, q_t: np.ndarray):
    """Process input through MAG variant logic to generate gate values.
    
    Args:
        q_t: Query projection from Neural Memory
    
    Returns:
        Dict containing gate values and metrics
    """
    try:
        # Get historical keys for attention calculation
        k_hist = self.sequence_context.get_recent_keys()
        
        if not k_hist or len(k_hist) == 0:
            logger.warning("No historical keys available for MAG attention")
            return {"status": "error", "error": "No historical context available"}
        
        # Use attention to determine gate values
        attention_output = self.compute_attention(q_t, k_hist)
        
        # Call Neural Memory's /calculate_gates endpoint
        response = await self.api_client.calculate_gates(
            attention_output=self._to_list(attention_output)
        )
        
        # Extract the calculated gates
        gates = response.get("gates", {})
        
        return {
            "status": "success",
            "gates": gates,
            "metrics": {
                "attention_magnitude": float(np.linalg.norm(attention_output))
            }
        }
    except Exception as e:
        logger.error(f"Error in MAG variant processing: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

#### Integration with ContextCascadeEngine

The refactored ContextCascadeEngine handles the MAG variant by applying its processing *before* the Neural Memory update, ensuring gates can properly influence the memory update process:

\`\`\`python
async def _apply_variant_pre_update(self, step_context):
    """Apply variant-specific pre-update processing for MAG/MAL variants.
    
    For MAG: Calculates attention-based gates
    For MAL: Calculates modified value projection
    
    Args:
        step_context: The current processing context
        
    Returns:
        Dict containing variant processing results
    """
    try:
        if self.active_variant_type == TitansVariantType.MAG:
            # Process MAG variant
            mag_result = await self.variant_processor.process_input(step_context["q_t"])
            
            if mag_result.get("status") == "success":
                # Store gates for use in Neural Memory update
                step_context["gates"] = mag_result.get("gates", {})
                logger.info(f"MAG variant calculated gates: {step_context['gates']}")
            else:
                logger.warning(f"MAG variant processing failed: {mag_result.get('error')}")
            
            return mag_result
            
        elif self.active_variant_type == TitansVariantType.MAL:
            # Process MAL variant
            # ...
            
    except Exception as e:
        logger.error(f"Error in _apply_variant_pre_update: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

### Neural Memory Update

The Neural Memory update process now accepts and applies the gates calculated by the MAG variant:

\`\`\`python
async def _update_neural_memory(self, step_context):
    """Update Neural Memory with appropriate modifications based on active variant.
    
    Args:
        step_context: The current processing context
        
    Returns:
        Dict containing update response
    """
    try:
        # Prepare update parameters
        update_params = {"input_embedding": self._to_list(step_context["x_t"])}
        
        # Add MAG gates if available
        if "gates" in step_context and step_context["gates"]:
            update_params.update({
                "alpha_t": step_context["gates"].get("alpha_t"),
                "theta_t": step_context["gates"].get("theta_t"),
                "eta_t": step_context["gates"].get("eta_t")
            })
            
        # Add MAL modified value if available
        if "v_prime" in step_context and step_context["v_prime"] is not None:
            update_params.update({
                "key_projection": self._to_list(step_context["k_t"]),
                "value_projection": self._to_list(step_context["v_prime"])
            })
            
        # Call Neural Memory update endpoint
        update_resp = await self.neural_memory_client.update_memory(**update_params)
        
        # Update step context with response data
        step_context["loss"] = update_resp.get("loss")
        step_context["grad_norm"] = update_resp.get("grad_norm")
        
        return update_resp
        
    except Exception as e:
        logger.error(f"Error updating Neural Memory: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

## Testing the MAG Variant

To test the MAG variant, you can use the `lucidia_think_trace` tool with the appropriate environment variable:

\`\`\`bash
# Run in Docker container
docker exec -e TITANS_VARIANT=MAG trainer-server python -m synthians_memory_core.tools.lucidia_think_trace --query "Testing MAG variant" --memcore-url "http://host.docker.internal:5010"
\`\`\`

The output should show:

1. Successful calculation of attention-based gates
2. Proper application of gates during Neural Memory update
3. Expected loss and gradient norm metrics

## Activation

To activate the MAG variant, set the `TITANS_VARIANT` environment variable:

\`\`\`bash
export TITANS_VARIANT=MAG  # For Linux/macOS
set TITANS_VARIANT=MAG      # For Windows CMD
\`\`\`

In the Docker setup, you can specify this when starting the container:

\`\`\`bash
docker run -e TITANS_VARIANT=MAG ...
\`\`\`

## Common Issues and Troubleshooting

### Insufficient Historical Context

The MAG variant requires historical keys to calculate attention-based gates. If there isn't enough historical context, you might see warnings like:

\`\`\`
No historical keys available for MAG attention
\`\`\`

Solution: Ensure that multiple inputs have been processed through the system before expecting MAG to influence the memory update process.

### TensorFlow Import Errors

If you encounter errors related to TensorFlow imports or NumPy version conflicts, verify that:

1. The lazy loading mechanism is correctly implemented
2. The fix_numpy.py script has run before any TensorFlow imports

## Conclusion

The refactored MAG variant implementation enables more effective memory-based cognitive processing by:

1. Using attention mechanisms to dynamically adjust Neural Memory update parameters
2. Properly sequencing operations to ensure gates are calculated before the memory update
3. Maintaining a clean and modular architecture with appropriate separation of concerns

This implementation follows the general Lucidia principle: "Memory shapes how we think, and thinking shapes how we remember." By allowing attention over past experiences to modulate how new experiences are stored, the MAG variant enhances the cognitive system's ability to prioritize and integrate information.

```

# docs\archive\mal_variant_implementation.md

```md
# MAL Variant Implementation Guide

## Overview

The Memory-Attended Learning (MAL) variant is a specialized architecture in the Lucidia Cognitive System that modifies the value projections used in Neural Memory updates through attention mechanisms over historical context. This document details the implementation, integration, and usage of the MAL variant within the refactored Context Cascade Engine.

## Architecture

The MAL variant follows this processing flow:

1. Get projections from Neural Memory (k_t, v_t, q_t) without updating
2. `q_t`, `v_t` + Historical context (K_hist, V_hist) u2192 Attend(q_t, K_hist, V_hist) u2192 Modified value `v_prime`
3. Update Neural Memory using modified value projection `v_prime`

![MAL Architecture](../assets/diagrams/mal_architecture.png)

## Implementation Details

### Core Components

1. **TitansVariantBase**
   - Provides common infrastructure for all variants
   - Handles API client initialization and neural memory URL configuration
   - Manages sequence context and historical context tracking
   - Implements lazy loading for TensorFlow to prevent NumPy version conflicts

2. **MALVariant Class**
   - Implements the Memory-Attended Learning logic
   - Initializes attention modules for value projection modification
   - Processes query and value projections through attention mechanisms
   - Creates enhanced value representations for memory storage

3. **NeuralMemoryModule**
   - Processes input embeddings to calculate key, value, and query projections
   - Supports updates with externally provided value projections
   - Performs memory updates with the modified value projection

4. **ContextCascadeEngine**
   - Orchestrates the variant selection and initialization
   - Routes memory operations through the appropriate variant
   - Invokes MAL processing *before* Neural Memory update
   - Passes the modified value projection to the Neural Memory update

### Key Methods

#### MALVariant

\`\`\`python
async def calculate_v_prime(self, q_t: np.ndarray, v_t: np.ndarray) -> Dict[str, Any]:
    """Calculate modified value projection using attention over historical values.
    
    Args:
        q_t: Query projection from Neural Memory
        v_t: Original value projection from Neural Memory
    
    Returns:
        Dict containing modified value projection and metrics
    """
    try:
        # Get historical keys and values for attention calculation
        k_hist, v_hist = self.sequence_context.get_recent_kv_pairs()
        
        if not k_hist or len(k_hist) == 0 or not v_hist or len(v_hist) == 0:
            logger.warning("No historical context available for MAL attention")
            return {"status": "error", "error": "No historical context available"}
        
        # Validate inputs and handle dimension mismatches
        q_t = self._validate_embedding(q_t)
        v_t = self._validate_embedding(v_t)
        
        # Apply attention between query and historical keys/values
        tf = _get_tf()  # Lazy load TensorFlow
        
        # Ensure inputs are properly shaped for attention
        query = tf.expand_dims(tf.convert_to_tensor(q_t, dtype=tf.float32), axis=0)  # [1, dim]
        keys = tf.convert_to_tensor(k_hist, dtype=tf.float32)  # [seq_len, dim]
        keys = tf.expand_dims(keys, axis=0)  # [1, seq_len, dim]
        values = tf.convert_to_tensor(v_hist, dtype=tf.float32)  # [seq_len, dim]
        values = tf.expand_dims(values, axis=0)  # [1, seq_len, dim]
        
        # Apply attention to generate attended values
        attended_v = self.attention_module(
            query=query,  # [1, 1, dim]
            key=keys,     # [1, seq_len, dim]
            value=values  # [1, seq_len, dim]
        )
        
        # Remove batch dimension [1, 1, dim] -> [dim]
        attended_v = tf.squeeze(attended_v).numpy()
        
        # Combine original and attended values to create v_prime
        v_prime = self.combine_values(v_t, attended_v)
        
        return {
            "status": "success",
            "v_prime": v_prime,
            "metrics": {
                "attention_magnitude": float(np.linalg.norm(attended_v)),
                "combination_ratio": self.combination_ratio
            }
        }
    except Exception as e:
        logger.error(f"Error in MAL variant processing: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

#### Integration with ContextCascadeEngine

The refactored ContextCascadeEngine handles the MAL variant by applying its processing *before* the Neural Memory update, modifying how memories are stored:

\`\`\`python
async def _apply_variant_pre_update(self, step_context):
    """Apply variant-specific pre-update processing for MAG/MAL variants.
    
    Args:
        step_context: The current processing context
        
    Returns:
        Dict containing variant processing results
    """
    try:
        # ... [MAG variant handling code] ...
        
        elif self.active_variant_type == TitansVariantType.MAL:
            # Process MAL variant: Calculate modified value projection
            mal_result = await self.variant_processor.calculate_v_prime(
                step_context["q_t"], step_context["v_t"]
            )
            
            if "v_prime" in mal_result:
                # Store modified value projection for use in Neural Memory update
                step_context["v_prime"] = mal_result["v_prime"]
                logger.info("MAL variant calculated modified value projection")
            else:
                logger.warning(f"MAL variant processing failed: {mal_result.get('error')}")
            
            return mal_result
            
        return {"status": "not_applicable"}
    except Exception as e:
        logger.error(f"Error in _apply_variant_pre_update: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

### Neural Memory Update

The Neural Memory update process accepts and applies the modified value projection calculated by the MAL variant:

\`\`\`python
async def _update_neural_memory(self, step_context):
    """Update Neural Memory with appropriate modifications based on active variant.
    
    Args:
        step_context: The current processing context
        
    Returns:
        Dict containing update response
    """
    try:
        # Prepare update parameters
        update_params = {"input_embedding": self._to_list(step_context["x_t"])}
        
        # ... [MAG variant handling code] ...
        
        # Add MAL variant modified value if available
        if "v_prime" in step_context and step_context["v_prime"] is not None:
            update_params.update({
                "key_projection": self._to_list(step_context["k_t"]),
                "value_projection": self._to_list(step_context["v_prime"])
            })
            logger.info("Adding MAL modified value projection to Neural Memory update")
        
        # Call Neural Memory update endpoint
        update_resp = await self.neural_memory_client.update_memory(**update_params)
        
        # Update step context with response data
        step_context["loss"] = update_resp.get("loss")
        step_context["grad_norm"] = update_resp.get("grad_norm")
        
        return update_resp
        
    except Exception as e:
        logger.error(f"Error updating Neural Memory: {str(e)}")
        return {"status": "error", "error": str(e)}
\`\`\`

### Value Combination

The MAL variant combines the original value projection with the attention-based value to create the enhanced `v_prime`:

\`\`\`python
def combine_values(self, v_t, attended_v):
    """Combine original value projection with attention-based value.
    
    Args:
        v_t: Original value projection
        attended_v: Attention-based value from historical context
    
    Returns:
        Combined value projection (v_prime)
    """
    # Ensure dimensions match
    v_t, attended_v = self._align_vectors(v_t, attended_v)
    
    # Combine using configured ratio
    v_prime = (1 - self.combination_ratio) * v_t + self.combination_ratio * attended_v
    
    return v_prime
\`\`\`

### Embedding Handling

The MAL variant includes robust handling for embedding dimension mismatches and malformed embeddings:

1. **Dimension Alignment**: Uses the `_align_vectors` method to handle mismatches between 384D and 768D embeddings
2. **Validation**: Uses the `_validate_embedding` method to detect and handle NaN/Inf values
3. **Safe Conversion**: Uses proper tensor conversion with error handling

\`\`\`python
def _validate_embedding(self, embedding):
    """Validate embedding and replace invalid values with zeros.
    
    Args:
        embedding: Input embedding to validate
    
    Returns:
        Validated embedding with NaN/Inf replaced by zeros
    """
    try:
        # Convert to numpy if needed
        if not isinstance(embedding, np.ndarray):
            embedding = np.array(embedding, dtype=np.float32)
        
        # Check for NaN or Inf values
        if np.isnan(embedding).any() or np.isinf(embedding).any():
            logger.warning(f"Found NaN/Inf in embedding, replacing with zeros")
            # Replace NaN/Inf with zeros
            embedding = np.where(np.isnan(embedding) | np.isinf(embedding), 0.0, embedding)
        
        return embedding
    except Exception as e:
        logger.error(f"Error validating embedding: {str(e)}")
        # Return zero vector as fallback
        return np.zeros(768, dtype=np.float32)
\`\`\`

## Testing the MAL Variant

To test the MAL variant, you can use the `lucidia_think_trace` tool with the appropriate environment variable:

\`\`\`bash
# Run in Docker container
docker exec -e TITANS_VARIANT=MAL trainer-server python -m synthians_memory_core.tools.lucidia_think_trace --query "Testing MAL variant" --memcore-url "http://host.docker.internal:5010"
\`\`\`

The output should show:

1. Successful calculation of modified value projection
2. Proper application of modified value during Neural Memory update
3. Expected loss and gradient norm metrics

## Activation

To activate the MAL variant, set the `TITANS_VARIANT` environment variable:

\`\`\`bash
export TITANS_VARIANT=MAL  # For Linux/macOS
set TITANS_VARIANT=MAL      # For Windows CMD
\`\`\`

In the Docker setup, you can specify this when starting the container:

\`\`\`bash
docker run -e TITANS_VARIANT=MAL ...
\`\`\`

## Common Issues and Troubleshooting

### Insufficient Historical Context

The MAL variant requires historical keys and values to calculate the modified value projection. If there isn't enough historical context, you might see warnings like:

\`\`\`
No historical context available for MAL attention
\`\`\`

Solution: Ensure that multiple inputs have been processed through the system before expecting MAL to influence the memory update process.

### TensorFlow Import Errors

If you encounter errors related to TensorFlow imports or NumPy version conflicts, verify that:

1. The lazy loading mechanism is correctly implemented
2. The fix_numpy.py script has run before any TensorFlow imports

### Dimension Mismatch Errors

If you encounter dimension mismatch errors, verify that:

1. The `_align_vectors` method is properly handling dimension differences
2. All inputs are properly validated before processing
3. TensorFlow operations are properly handling tensor shapes

## Conclusion

The MAL variant implementation enhances memory storage by modifying how value projections are calculated before Neural Memory updates. This approach provides several benefits:

1. Improved contextual coherence in stored memories
2. Enhanced learning by incorporating relevant historical values
3. More efficient memory representation through context-aware value projections

By applying attention to modify the value projection *before* the Neural Memory update, MAL influences how memories are stored rather than how they're retrieved, complementing the approaches of the MAC and MAG variants.

```

# docs\archive\memory_system_remaster.md

```md
# Synthians Memory System Remaster

_Documentation for the comprehensive memory system enhancements_

**Date**: March 27, 2025  
**Branch**: Synthience_memory_remaster

## 🧠 Overview

The Synthians Memory Core is a sophisticated system that integrates vector search, embedding processing, and emotional analysis to create a cohesive memory retrieval mechanism. This document outlines recent critical enhancements to the system, focusing on persistence, reliability, and observability.

## 🔍 Problem Statement

The memory system was experiencing several key issues:

1. **Vector Index Persistence**: Memories were being added to the FAISS vector index but the index itself wasn't being saved to disk during the persistence process, causing all lookups to fail after system restart.

2. **Observability Gaps**: The system lacked proper diagnostics and stats for monitoring the vector index state and memory operations.

3. **Embedding Dimension Mismatches**: The system struggled with handling different embedding dimensions (primarily between 384 and 768), causing comparison errors.

4. **Retrieval Thresholds**: The default threshold was too high (0.5), causing many relevant memories to be filtered out.

## 🛠️ Solutions Implemented

### 1. Fixed Vector Index Persistence

\`\`\`python
# Added code to _persist_all_managed_memories to save the vector index
if self.vector_index.count() > 0:
    vector_index_saved = self.vector_index.save()
    logger.info("SynthiansMemoryCore", f"Vector index saved: {vector_index_saved} with {self.vector_index.count()} vectors and {len(self.vector_index.id_to_index)} id mappings")
\`\`\`

This critical fix ensures that the FAISS index and ID-to-index mappings are properly saved to disk during the persistence cycle, enabling consistent memory retrieval even after system restarts.

### 2. Enhanced API Observability

\`\`\`python
# Extended the /stats endpoint with vector index information
vector_index_stats = {
    "count": app.state.memory_core.vector_index.count(),
    "id_mappings": len(app.state.memory_core.vector_index.id_to_index),
    "index_type": app.state.memory_core.vector_index.config.get('index_type', 'Unknown')
}
\`\`\`

Improved the `/stats` endpoint to provide comprehensive vector index information, enabling better monitoring and debugging of the memory system.

### 3. Embedding Dimension Handling

\`\`\`python
# Added vector alignment utilities
def _align_vectors_for_comparison(self, vec1, vec2):
    """Safely align two vectors to the same dimension for comparison operations."""
    if vec1.shape[0] != vec2.shape[0]:
        # Either pad with zeros or truncate to match dimensions
        target_dim = min(vec1.shape[0], vec2.shape[0])
        if vec1.shape[0] > target_dim:
            vec1 = vec1[:target_dim]
        if vec2.shape[0] > target_dim:
            vec2 = vec2[:target_dim]
    return vec1, vec2
\`\`\`

Implemented robust dimension handling to ensure vector operations work correctly regardless of the embedding dimensions used.

### 4. Retrieval Threshold Adjustments

\`\`\`python
# Lowered threshold for better recall sensitivity
if threshold is None:
    threshold = 0.2  # Lowered from 0.5 to 0.2 for better recall
\`\`\`

Adjusted the pre-filter threshold from 0.5 to 0.2 to improve recall sensitivity while maintaining precision.

## 📊 Testing and Validation

We created comprehensive testing tools to validate the memory system:

1. **direct_test.py**: Validates the full memory lifecycle through the API:
   - Memory creation
   - Proper persistence
   - Retrieval with similarity scores

2. **tests/test_memory_retrieval_api.py**: API-based test suite for Docker:
   - Health checks
   - Memory creation and retrieval tests
   - GPU detection and validation

## 🔄 Additional System Improvements

### Metadata Enrichment

\`\`\`python
# Add memory ID to metadata for easier access
memory.metadata["uuid"] = memory.id
\`\`\`

Enhanced memory metadata with additional context (UUID, content length) to improve traceability.

### Redundant Computation Prevention

\`\`\`python
# Analyze Emotion only if not already provided
emotional_context = metadata.get("emotional_context")
if not emotional_context:
    emotional_context = await self.emotional_analyzer.analyze(content)
    metadata["emotional_context"] = emotional_context
else:
    logger.debug("Using precomputed emotional context from metadata")
\`\`\`

Optimized processing by avoiding redundant emotion analysis when data is already available.

## 🚀 Deployment and Usage

### Docker Integration

The system fully supports GPU acceleration through FAISS when deployed with Docker:

\`\`\`bash
# Start the service with GPU support
docker-compose up -d

# Run tests inside the container
docker exec -it synthians_core python /workspace/project/direct_test.py
\`\`\`

### API Endpoints

- `/process_memory`: Create new memories with optional embeddings
- `/retrieve_memories`: Retrieve memories using semantic similarity
- `/stats`: Get comprehensive system statistics

## 🧪 Validation Process

To verify the system is working correctly:

1. Create a memory via the API
2. Check that it's properly saved to disk
3. Restart the container
4. Verify the memory can be retrieved using a semantically similar query

## 📝 Conclusion

The Synthians Memory System has been significantly enhanced with better persistence, observability, and reliability. These improvements ensure consistent memory retrieval, better debugging capabilities, and more robust embedding handling.

```

# docs\archive\metadata_handling.md

```md
# Metadata Handling Improvements in SynthiansMemoryCore

**Date:** March 29, 2025

## Overview

This document describes the enhanced metadata handling capabilities implemented in the `SynthiansMemoryCore` class, focusing on the improved deep dictionary merging strategy used during memory updates.

## Problem Statement

Prior to the March 2025 improvements, the `update_memory` method in `SynthiansMemoryCore` suffered from inadequate handling of nested metadata dictionaries. The implementation used a shallow merging strategy that replaced entire nested dictionaries rather than performing a proper deep merge. This led to data loss in several scenarios:

1. When updating a nested dictionary field, the entire nested structure was replaced rather than merged
2. When updating metadata while preserving timestamp information (e.g., `quickrecal_updated_at`), the timestamps were being overwritten
3. When attempting to persist memories after updates, important metadata fields were being lost

## Implementation Details

### Deep Dictionary Merge

The core improvement involves the enhanced `_deep_update_dict` method which now properly handles nested dictionary structures:

\`\`\`python
def _deep_update_dict(self, d: Dict, u: Dict) -> Dict:
    """
    Recursively update a dictionary with another dictionary
    This handles nested dictionaries properly
    """
    for k, v in u.items():
        if isinstance(v, dict) and k in d and isinstance(d[k], dict):
            # Only recursively merge if both the source and update have dict values
            d[k] = self._deep_update_dict(d[k], v)
        else:
            d[k] = v
    return d
\`\`\`

Key changes in this implementation:
- Only attempts recursive merging when both the source (`d[k]`) and update (`v`) values are dictionaries
- Ensures the key exists in the source dictionary before attempting to merge
- Preserves the existing structure when merging nested dictionaries

### Improved Metadata Update Flow

The `update_memory` method now processes metadata updates in a more controlled manner:

1. Metadata updates are collected separately during the main attribute update loop
2. Direct attributes (like `quickrecal_score`) are processed first
3. Metadata updates are applied after all direct attributes have been processed
4. Deep merging is used to preserve existing metadata while adding/updating specific fields

This ensures that important metadata like timestamps and source information are preserved across updates.

### Vector Index Update

The method now also properly handles the vector index update by:
1. Using the `update_entry` method when available
2. Falling back to a remove/add pattern when `update_entry` isn't available
3. Adding robust error handling for vector index operations

## Benefits

These improvements provide several important benefits:

1. **Data Preservation:** Existing metadata is preserved when updating specific fields or nested structures
2. **Increased Robustness:** The system now properly handles complex nested metadata structures
3. **Improved Test Stability:** Tests that rely on metadata persistence now work consistently
4. **Better Vector Index Management:** More robust handling of embedding updates in the vector index

## Usage Examples

When updating memory metadata with nested structures:

\`\`\`python
# Original metadata
# memory.metadata = {
#    "source": "user_input",
#    "nested": {"key1": "value1", "key2": "value2"},
#    "timestamp": "2025-03-29T10:00:00Z"
# }

# Update with nested structure
await memory_core.update_memory(memory_id, {
    "metadata": {
        "nested": {"key1": "updated_value", "key3": "new_value"}
    }
})

# Result (with proper deep merging):
# memory.metadata = {
#    "source": "user_input",
#    "nested": {"key1": "updated_value", "key2": "value2", "key3": "new_value"},
#    "timestamp": "2025-03-29T10:00:00Z"
# }
\`\`\`

## Related Components

This improvement affects several key components:
- `SynthiansMemoryCore` class
- `MemoryPersistence` class
- `TrainerIntegrationManager` (which relies on metadata persistence)
- All test suites involving memory updates and persistence

## Future Considerations

Future enhancements could include:
1. Adding explicit schema validation for metadata structures
2. Implementing metadata normalization functions to ensure consistent formats
3. Adding metadata pruning to prevent unbounded growth of nested structures

```

# docs\archive\numpy_tensorflow_compatibility.md

```md
# NumPy-TensorFlow Compatibility Solution

## Overview

This document describes the solution implemented to resolve NumPy version incompatibility issues in the Lucidia cognitive system, particularly focusing on the TensorFlow integration in the Titans architecture variants.

## Problem Statement

The system experienced a binary incompatibility error related to NumPy versions:

\`\`\`
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject
\`\`\`

This occurred because:

1. The `fix_numpy.py` script downgraded NumPy to version 1.26.4
2. TensorFlow was being imported during module initialization
3. TensorFlow's import chain loaded NumPy before the downgrade could take effect
4. This created conflicts between the original NumPy version and the downgraded version

## Solution: Lazy Loading Pattern

We implemented a lazy loading pattern for TensorFlow that delays its import until actually needed at runtime, allowing the NumPy downgrade to complete first.

### Implementation Details

#### 1. Lazy Loading Mechanism in `titans_variants.py`

\`\`\`python
# Global variable to hold the TensorFlow module
_tf = None

def _get_tf():
    """Lazy-load TensorFlow only when needed to avoid early NumPy conflicts"""
    global _tf
    if _tf is None:
        import tensorflow as tf
        _tf = tf
    return _tf
\`\`\`

#### 2. Replacing Direct TensorFlow References

Before:
\`\`\`python
import tensorflow as tf

def process_input(self, attention_output: tf.Tensor) -> Dict[str, Any]:
    # Function implementation
\`\`\`

After:
\`\`\`python
def process_input(self, attention_output) -> Dict[str, Any]:
    tf = _get_tf()  # Only imported when function is called
    # Function implementation
\`\`\`

#### 3. Type Annotation Modifications

Before:
\`\`\`python
def calculate_gates_from_attention(self, attention_output: tf.Tensor) -> Tuple[float, float, float]:
\`\`\`

After:
\`\`\`python
def calculate_gates_from_attention(self, attention_output) -> Tuple[float, float, float]:
\`\`\`

## Key Files Modified

1. `titans_variants.py` - Implemented lazy loading for TensorFlow and updated all TensorFlow references
2. `context_cascade_engine.py` - Updated imports to avoid direct TensorFlow loading

## Benefits

1. **Proper Initialization Sequence**: Ensures NumPy is downgraded before TensorFlow tries to use it
2. **Reduced Import Coupling**: Components only import TensorFlow when actually needed
3. **Improved Startup Performance**: Modules can be imported without loading the entire TensorFlow stack

## Usage Guidelines

When working with TensorFlow in the Lucidia system:

1. Always use the `_get_tf()` function instead of directly importing TensorFlow
2. Avoid type annotations that directly reference TensorFlow types
3. Use string literals for type annotations when needed: `def func(x: 'tf.Tensor') -> None:`

## Testing

After implementing the lazy loading pattern, all Titans variants (MAC, MAG, MAL) can be initialized and used without triggering NumPy compatibility errors. The system now starts up cleanly and operates as expected.

## Docker Networking Configuration

When testing the Titans architecture variants in a Docker environment, proper service name resolution is critical. The following solution was implemented to ensure communication between the trainer-server and memory-core containers:

1. **Service Discovery Issue**: Direct communication using service names (e.g., `memory-core:5010`) may not work due to Docker networking configuration.

2. **Solution**: Use the special DNS name `host.docker.internal` which allows containers to access services on the host machine:
   \`\`\`
   --memcore-url http://host.docker.internal:5010
   \`\`\`

3. **Execution Example**: Run Titans variants with the correct memory core URL:
   \`\`\`bash
   docker exec -e TITANS_VARIANT=MAC trainer-server python -m synthians_memory_core.tools.lucidia_think_trace --query "This is a test" --memcore-url "http://host.docker.internal:5010"
   \`\`\`

4. **Results**: All three Titans variants (MAC, MAG, MAL) successfully connect to the Memory Core service and complete processing with proper neural memory integration.

```

# docs\archive\phase_4_implementation.md

```md
# Phase 4 Implementation: Titans Architecture Variants

**Author:** Lucidia (MEGA)
**Date:** 2025-03-28 15:45:00 UTC
**Status:** Complete

## Overview

This document details the implementation of the Titans Architecture Variants (MAC, MAG, MAL) as outlined in Section 4 of the Titans paper. Phase 4 extends Lucidia's cognitive architecture by integrating attention mechanisms with the Neural Memory module, enhancing its adaptive capabilities and contextual awareness.

> *"The blueprint remembers, but attention shapes what is recalled."*

## Implementation Components

The implementation consists of five key components:

1. **MultiHeadAttentionModule**: A robust attention mechanism implemented in `synthians_trainer_server/attention.py`
2. **SequenceContextManager**: A deque-based context buffer in `orchestrator/history.py`
3. **Neural Memory API Extensions**: Enhanced API endpoints in `synthians_trainer_server/http_server.py`
4. **Titans Variant Implementations**: Base class and specific variant implementations in `orchestrator/titans_variants.py`
5. **ContextCascadeEngine Integration**: Connection of variants to the orchestration layer in `orchestrator/context_cascade_engine.py`

## Detailed Implementation

### 1. MultiHeadAttentionModule

Implemented in `synthians_trainer_server/attention.py`, this module provides a configurable multi-head attention mechanism with:

- Dimension validation and standardization (handles the 384D vs 768D embedding mismatch issues)
- Optional residual connections and layer normalization
- Metrics tracking for attention scores, entropy, and sparsity
- Robust error handling for malformed embeddings and NaN/Inf values

\`\`\`python
class MultiHeadAttentionModule(tf.keras.layers.Layer):
    """Multi-head attention module with dimension validation and metrics tracking."""
    # Implementation details in attention.py
\`\`\`

### 2. SequenceContextManager

Implemented in `orchestrator/history.py`, this module manages a history of context tuples:

- Stores `(timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)` tuples
- Provides methods for retrieving recent keys, values, and outputs
- Uses a deque with configurable max length to control memory usage

\`\`\`python
class SequenceContextManager:
    """Manages a sequence of context tuples for attention-based processing."""
    # Implementation details in history.py
\`\`\`

### 3. Neural Memory API Extensions

Enhanced in `synthians_trainer_server/http_server.py` to expose internal projections:

- Extended `UpdateMemoryResponse` to include `key_projection` and `value_projection`
- Extended `RetrieveResponse` to include `query_projection`
- Modified handlers to calculate projections and include them in responses

\`\`\`python
class UpdateMemoryResponse(BaseModel):
    status: str
    loss: Optional[float] = None
    grad_norm: Optional[float] = None
    key_projection: Optional[List[float]] = None
    value_projection: Optional[List[float]] = None
\`\`\`

### 4. Titans Variant Implementations

Implemented in `orchestrator/titans_variants.py`, providing three attention-based variants:

#### 4.1 Base Variant Class

\`\`\`python
class TitansVariantBase:
    """Base class for all Titans architecture variants."""
    # Common functionality and interfaces for all variants
\`\`\`

#### 4.2 Memory-Attended Computation (MAC)

\`\`\`python
class MACVariant(TitansVariantBase):
    """Memory-Attended Computation (MAC) variant.
    
    Enhances memory retrieval by attending over historical memory outputs.
    Flow: q_t -> M -> y_t -> Attend(q_t, K_hist, Y_hist) -> attended_y_t
    """
    # Implementation in titans_variants.py
\`\`\`

MAC enhances output by applying attention over historical memory outputs, providing a more contextually relevant retrieval.

#### 4.3 Memory-Attended Gates (MAG)

\`\`\`python
class MAGVariant(TitansVariantBase):
    """Memory-Attended Gates (MAG) variant.
    
    Modifies gate values (alpha, theta, eta) for the neural memory update
    by attending over historical key projections.
    """
    # Implementation in titans_variants.py
\`\`\`

MAG dynamically adjusts memory decay rates based on contextual relevance, allowing for adaptive forgetting.

#### 4.4 Memory-Augmented Learning (MAL)

\`\`\`python
class MALVariant(TitansVariantBase):
    """Memory-Augmented Learning (MAL) variant.
    
    Modifies value projection for neural memory update by attending over
    historical value projections.
    """
    # Implementation in titans_variants.py
\`\`\`

MAL enhances learning by augmenting value projections with historically relevant values, facilitating associative connections.

### 5. ContextCascadeEngine Integration

Extended in `orchestrator/context_cascade_engine.py` to activate and utilize the appropriate variant:

- Reads `TITANS_VARIANT` environment variable to determine active variant
- Initializes variant processor with appropriate configuration
- Extracts projections from API responses and populates the context manager
- Processes inputs through the active variant and handles variant-specific outputs

## Configuration

Titans variants can be configured via environment variables and configuration objects:

\`\`\`python
# Select variant via environment variable
os.environ["TITANS_VARIANT"] = "MAC"  # Options: NONE, MAC, MAG, MAL

# Configure attention parameters
attention_config = {
    'num_heads': 4,
    'key_dim': 32,  # Per head dimension
    'dropout': 0.0,
    'use_layer_norm': True,
    'use_residual': True,
}
\`\`\`

## Using the Variants

### MAC Variant

The MAC variant enhances memory retrieval by attending over historical memory outputs. It's particularly useful for tasks requiring coherent sequential recall, such as conversation modeling or narrative generation.

### MAG Variant

The MAG variant dynamically adjusts the memory decay rates (alpha, theta, eta) based on contextual relevance. This is beneficial for systems that need to selectively preserve or forget information based on changing contexts.

### MAL Variant

The MAL variant augments the learning process by modifying value projections with historically relevant values. This facilitates richer associations and connections between memories, enhancing conceptual learning.

## Current Limitations & Future Work

1. **MAG and MAL Timing**: The current implementation processes MAG and MAL variants after the `/update_memory` call, whereas ideally they should influence the call itself. Future work will refactor the processing order.

2. **Neural Memory Configuration**: Currently using hardcoded attention parameters. Future implementation could fetch these from a Neural Memory config endpoint.

3. **Integration Testing**: Comprehensive integration tests for each variant in different scenarios are needed.

4. **Documentation**: API reference and usage examples for each variant should be expanded.

## Conclusion

The Phase 4 implementation of Titans Architecture Variants significantly enhances Lucidia's cognitive architecture by introducing contextual attention mechanisms. These variants enable more adaptive, context-aware memory operations, aligning with the core principles of the cognitive architecture:

- "Memory is weighted, not just chronological" (QuickRecal)
- "Emotion shapes recall" (Emotional Gating)
- "Surprise signals significance" (Neural Memory Loss/Grad → QuickRecal Boost)
- "Ideas cluster and connect" (Attention-based context)
- "Presence emerges from adaptive memory" (Variant-specific adaptive mechanisms)

---

**Next Steps:**

1. Refactor processing flow for MAG and MAL to influence the `/update_memory` call
2. Implement integration tests for each variant
3. Enhance configuration options with dynamic parameter loading
4. Expand metrics tracking for variant-specific performance analysis

```

# docs\archive\phase_4_plan.md

```md
Okay, Phase 3 is complete, and the core bi-hemispheric loop is functional! Now, let's plan for Phase 4: **Implementing Titans Architecture Variants (MAC, MAG, MAL)**.

This phase involves integrating attention mechanisms with the Neural Memory module, as described in Section 4 of the Titans paper, to enhance its capabilities.

**Phase 4 Goal:** To implement, integrate, and provide configuration options for the Memory-Attended Computation (MAC), Memory-Attended Gates (MAG), and Memory-Augmented Learning (MAL) variants.

**Prerequisites:**

1.  **Stable Phase 3:** Ensure the current codebase (post-Phase 3 fixes) is stable, committed, and tests are passing. The core loop (MemCore Store -> NeuralMem Update -> QuickRecal Boost -> NeuralMem Retrieve) must be reliable.
2.  **Confirm Configuration:** Verify the `NeuralMemoryConfig` (in `neural_memory.py` defaults and `http_server.py` startup) has `key_dim` and `query_dim` set correctly and *identically* (e.g., both 128).
3.  **Confirm QuickRecal Fix:** Double-check Memory Core logs to ensure the `update_quickrecal_score` endpoint is working correctly after the `get_memory_by_id`/`update_memory` fixes.
4.  **Understand Attention:** Familiarity with standard multi-head self-attention and cross-attention mechanisms (as implemented in TensorFlow/Keras or described in "Attention Is All You Need").
5.  **Review Titans Paper (Sec 4):** Re-read Section 4 and study the diagrams for MAC, MAG, and MAL to understand the data flow and where attention interacts.

**Architectural Decisions:**

1.  **Attention Module Location:** A new, reusable attention module (`attention.py`?) should be created within `synthians_trainer_server`.
2.  **Orchestration Location:** The `ContextCascadeEngine` (CCE) remains the central orchestrator. It will be responsible for:
    *   Maintaining necessary context/history for attention (e.g., recent keys, values, memory outputs).
    *   Calling the appropriate attention module based on the active variant.
    *   Modifying the data flow and calls to the `NeuralMemoryServer` according to the variant's logic.
3.  **Parameter Location:**
    *   Core attention parameters (projection matrices within the attention module) will be part of the attention module itself.
    *   Any *new* trainable parameters needed specifically for MAG (projecting attention output to gates) or MAL (gating/combining values) should ideally reside within the `NeuralMemoryModule` (as *outer* parameters) to keep related components together, but the CCE might need to trigger their calculation via new API endpoints or modified existing ones.
4.  **Configuration:** Introduce a new configuration setting (e.g., environment variable `TITANS_VARIANT` or a config file entry) read by the CCE to determine which variant (`NONE`, `MAC`, `MAG`, `MAL`) is active.

## Phase 4 Implementation Plan

**Step 1: Setup & Attention Core Module**

1.  **Branching:** Create a new feature branch (e.g., `feature/phase4-attention-variants`).
2.  **Configuration:**
    *   Define how the active variant (`NONE`, `MAC`, `MAG`, `MAL`) will be configured (e.g., add `TITANS_VARIANT` environment variable).
    *   Modify `ContextCascadeEngine.__init__` to read this configuration and store the active variant mode.
3.  **Create Attention Module (`synthians_trainer_server/attention.py`):**
    *   Implement a `MultiHeadAttentionModule` class using `tf.keras.layers.MultiHeadAttention`.
    *   Make it configurable (num_heads, key_dim, value_dim, dropout).
    *   Ensure it handles mask inputs if necessary (though likely not needed for these variants initially).
    *   Add basic unit tests for this module.
4.  **Context History in CCE:**
    *   Modify the `ContextCascadeEngine.sequence_context` list. Instead of just storing embeddings and IDs, ensure it stores the necessary tuples for attention based on potential future needs: `(timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)` where `x_t` is the input embedding, `k/v/q_t` are projections, and `y_t` is the output from `NeuralMemoryModule.call`.
    *   This requires adding `/get_projections` calls *during* the CCE's `process_new_input` flow (likely after getting `actual_embedding` from MemCore) *before* calling `/update_memory` and `/retrieve`, and storing these projections. Modify the `/update_memory` and `/retrieve` request/response cycle if needed to avoid redundant calculations. **Alternative:** Modify `/update_memory` and `/retrieve` responses to *return* the `k_t, v_t, q_t` they calculated internally. The latter is probably more efficient.
        *   **Decision:** Let's modify `/update_memory` and `/retrieve` to return the projections they compute.
        *   **Action:** Update `UpdateMemoryResponse` and `RetrieveResponse` models (and handlers in `http_server.py`) to include optional `key_projection`, `value_projection`, `query_projection` fields. Modify `NeuralMemoryModule.update_step` and `call` to potentially return these. Update CCE to store these in `sequence_context`.

**Step 2: Implement MAC (Memory-Attended Computation) Variant**

1.  **Modify CCE (`process_new_input`):**
    *   Add logic branch: `if self.active_variant == 'MAC':`.
    *   Inside this branch, *after* the call to `NeuralMemoryServer:/retrieve` which returns the raw memory output `y_t = M(q_t)` (and also `q_t` itself, based on Step 1 refinement):
        *   Retrieve recent history pairs `(k_i, y_i)` from `self.sequence_context`. Let `Y_hist = [y_i]` and `K_hist = [k_i]`.
        *   Instantiate or get the `MultiHeadAttentionModule`.
        *   Calculate attended output: `attended_y_t = AttentionModule(query=q_t, keys=K_hist, values=Y_hist)`.
        *   **Crucially:** Replace the raw `retrieved_embedding` in the `response` dictionary and potentially `self.last_retrieved_embedding` with this `attended_y_t`. This attended value is what downstream components will use.
2.  **Testing:**
    *   Add integration tests (e.g., modifying `lucidia_think_trace.py` or creating new tests) that activate MAC mode.
    *   Verify that the final `retrieved_embedding` differs from the raw output of `/retrieve` when history is present.
    *   Check logs for attention calculations.

**Step 3: Implement MAG (Memory-Attended Gates) Variant**

1.  **Modify `NeuralMemoryModule` (`neural_memory.py`):**
    *   Add new trainable layers (e.g., `Dense` layers) responsible for projecting the attention output to scalar gate logits. These layers belong to the *outer* parameters.
        \`\`\`python
        # In __init__
        self.attention_to_alpha = tf.keras.layers.Dense(1, name="att_alpha_proj", kernel_initializer=initializer_outer)
        self.attention_to_theta = tf.keras.layers.Dense(1, name="att_theta_proj", kernel_initializer=initializer_outer)
        self.attention_to_eta = tf.keras.layers.Dense(1, name="att_eta_proj", kernel_initializer=initializer_outer)
        # Add these layers' variables to outer_trainable_variables property
        \`\`\`
    *   Add a new method like `calculate_gates_from_attention(self, attention_output: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]`:
        \`\`\`python
        def calculate_gates_from_attention(self, attention_output):
            alpha_logit = self.attention_to_alpha(attention_output)
            theta_logit = self.attention_to_theta(attention_output)
            eta_logit = self.attention_to_eta(attention_output)
            # Return scalar tensors (remove batch dim if present)
            return tf.squeeze(tf.sigmoid(alpha_logit)), tf.squeeze(tf.sigmoid(theta_logit)), tf.squeeze(tf.sigmoid(eta_logit))
        \`\`\`
    *   Modify `update_step`: Add optional arguments `alpha_t_ext=None, theta_t_ext=None, eta_t_ext=None`. If these arguments are provided (not None), use them instead of calculating gates from the internal `alpha_logit`, etc.
        \`\`\`python
        # Inside update_step
        alpha_t = tf.sigmoid(self.alpha_logit) if alpha_t_ext is None else alpha_t_ext
        theta_t = tf.sigmoid(self.theta_logit) if theta_t_ext is None else theta_t_ext
        eta_t = tf.sigmoid(self.eta_init) if eta_t_ext is None else eta_t_ext # Corrected: Use eta_logit
        # eta_t = tf.sigmoid(self.eta_logit) if eta_t_ext is None else eta_t_ext # <-- Corrected Line
        \`\`\`
2.  **Modify Neural Memory Server API (`http_server.py`):**
    *   Add a new endpoint `/calculate_gates` (POST) that takes an `attention_output` vector and returns the calculated `alpha_t, theta_t, eta_t` by calling `nm.calculate_gates_from_attention`.
    *   Modify `UpdateMemoryRequest` to include optional `alpha_t`, `theta_t`, `eta_t` fields.
    *   Modify the `/update_memory` handler to pass these external gates to `nm.update_step` if they are present in the request.
3.  **Modify CCE (`process_new_input`):**
    *   Add logic branch: `if self.active_variant == 'MAG':`.
    *   Inside this branch, *before* calling `/update_memory`:
        *   Get `q_t` (either from the `/process_memory` response via Memory Core call if we modify that, or by calling `/get_projections` on NeuralMem). Let's assume we get it along with `k_t` from the initial processing step.
        *   Retrieve recent history keys `K_hist = [k_i]` from `self.sequence_context`.
        *   Instantiate or get the `MultiHeadAttentionModule`.
        *   Calculate attention output: `attention_output = AttentionModule(query=q_t, keys=K_hist, values=K_hist)` (Attending query to past keys).
        *   Call the *new* `NeuralMemoryServer:/calculate_gates` endpoint with `attention_output`.
        *   Receive `alpha_t, theta_t, eta_t` from the response.
        *   Modify the payload for the *subsequent* `/update_memory` call to include these calculated gates (`alpha_t`, `theta_t`, `eta_t`).
4.  **Outer Loop Training (`NeuralMemoryModule.train_step`):** Ensure the gradients flow back through the new gate projection layers (`attention_to_alpha`, etc.) when calculating `outer_grads`.
5.  **Testing:** Add integration tests for MAG mode. Verify that gate values passed externally influence the update step. Check gradients for the new layers.

**Step 4: Implement MAL (Memory-Augmented Learning) Variant**

1.  **Modify `NeuralMemoryModule` (`neural_memory.py`):**
    *   Modify `update_step`: Instead of calculating `k_t, v_t` from `x_t` internally, change the method signature to accept `k_t` and `v_prime_t` directly: `update_step(self, k_t: tf.Tensor, v_prime_t: tf.Tensor)`. Update the loss calculation to use `v_prime_t`: `loss = 0.5 * tf.reduce_sum(tf.square(predicted_v_t - v_prime_t))`. Remove the `get_projections` call from within `update_step`.
2.  **Modify Neural Memory Server API (`http_server.py`):**
    *   Modify `UpdateMemoryRequest`: Change `input_embedding` to `key_projection: List[float]` and `value_projection: List[float]` (representing `k_t` and `v'_t`).
    *   Modify the `/update_memory` handler:
        *   Validate `key_projection` against `key_dim` and `value_projection` against `value_dim`.
        *   Convert them to tensors.
        *   Call `nm.update_step(k_tensor, v_prime_tensor)`.
3.  **Modify CCE (`process_new_input`):**
    *   Add logic branch: `if self.active_variant == 'MAL':`.
    *   Inside this branch, *before* calling `/update_memory`:
        *   Get `k_t, v_t, q_t` for the current input `x_t` (e.g., via `/get_projections` or from refined response).
        *   Retrieve recent history pairs `(k_i, v_i)` from `self.sequence_context`. Let `K_hist = [k_i]` and `V_hist = [v_i]`.
        *   Instantiate or get the `MultiHeadAttentionModule`.
        *   Calculate attention output: `attended_v_t = AttentionModule(query=q_t, keys=K_hist, values=V_hist)`.
        *   Combine `attended_v_t` with the current `v_t` to get `v_prime_t`. (Start with simple addition: `v_prime_t = v_t + attended_v_t`. Later, this could be a learned gating mechanism requiring new outer parameters).
        *   Modify the payload for the `/update_memory` call to send `key_projection=k_t` and `value_projection=v_prime_t`.
4.  **Testing:** Add integration tests for MAL mode. Verify that the `v_prime_t` calculated in CCE is correctly used in the Neural Memory's loss calculation.

**Step 5: Refinement, Integration Testing & Benchmarking**

1.  **Code Review & Refactoring:** Clean up the CCE logic, ensure efficient history management, and refine error handling.
2.  **Configuration Testing:** Test switching between `NONE`, `MAC`, `MAG`, `MAL` modes using the configuration mechanism.
3.  **Comprehensive Integration Tests:** Create tests simulating longer sequences and verifying the distinct behaviors of each variant. Use `lucidia_think_trace.py` extensively.
4.  **(Optional/Future) Benchmarking:** If specific tasks (like those in the Titans paper) are defined, implement the necessary outer loop training (`/train_outer`) adjustments for each variant and benchmark performance on evaluation datasets. This is a significant undertaking beyond the core implementation.

**Step 6: Documentation**

1.  **Update `README.md` / `NEWEST-DOCUMENTATION.md`:** Reflect the completion of Phase 4 and the availability of the variants.
2.  **Update `architecture_overview.md` / `bihemispheric_architecture.md`:** Add descriptions and potentially diagrams illustrating the data flow for MAC, MAG, MAL.
3.  **Update `api_reference.md`:** Document any changes to the Neural Memory Server endpoints (e.g., `/calculate_gates`, modified `/update_memory` payload).
4.  **Create `attention.md`:** Document the `MultiHeadAttentionModule`.
5.  **Update `implementation_guide.md`:** Explain how to configure and use the different Titans variants.

This plan provides a structured approach to implementing the attention-based variants, focusing on modifying the CCE and the Neural Memory API/Module iteratively for each variant. Remember to test thoroughly at each step.
```

# docs\archive\phase1_retrieval_enhancements.md

```md
# Phase 1: Memory Retrieval Pipeline Enhancements

## Overview

The Phase 1 enhancements focused on improving the robustness and reliability of the memory retrieval pipeline in the `SynthiansMemoryCore`. The primary objectives were to:

1. Fix the "0 memories" issue where queries would fail to return results
2. Ensure proper handling of FAISS candidates
3. Implement robust validation for embeddings
4. Add detailed logging throughout the pipeline
5. Enable reliable filtering based on similarity and thresholds

## Key Enhancements

### 1. Embedding Validation and Alignment

- Added explicit validation of query embeddings to detect and handle NaN/Inf values
- Implemented proper alignment of embeddings with different dimensions (384D vs 768D)
- Added safeguards to prevent division by zero during vector normalization

\`\`\`python
# Example of validation and alignment
query_embedding = self._validate_vector(query_embedding)
if query_embedding is None:
    logger.warning("Invalid query embedding detected. Using zero vector.")
    query_embedding = np.zeros(self.config['embedding_dim'])

# Memory embedding alignment and validation
memory_embedding_np = self._validate_vector(memory_embedding)
if memory_embedding_np is None:
    logger.warning(f"Invalid memory embedding for {mem_id}. Using zero vector.")
    memory_embedding_np = np.zeros(self.config['embedding_dim'])

# Explicit alignment before similarity calculation
aligned_query, aligned_memory = self._align_vectors(query_embedding, memory_embedding_np)
if aligned_query is None or aligned_memory is None:
    logger.warning(f"Alignment failed for {mem_id}. Skipping.")
    continue
\`\`\`

### 2. Comprehensive Logging

- Added categorized logging with clear prefixes for easier debugging (e.g., `[FAISS Results]`, `[Threshold Filtering]`)
- Logged critical information at each stage of the pipeline:
  - Raw candidates retrieved from FAISS
  - Vector dimensions before and after alignment
  - Similarity scores
  - Threshold filtering decisions
  - Emotional gating results
  - Metadata filtering results
  - Final memory IDs and scores

\`\`\`python
# Example of enhanced logging
logger.info(f"[FAISS Results] Retrieved {len(raw_candidates)} raw candidates from vector search")
logger.info(f"[Threshold Filtering] Using threshold: {current_threshold:.4f}")
logger.info(f"[Threshold Filtering] Kept {len(candidates_passing_threshold)} candidates, filtered out {len(candidates_filtered_out)}")
\`\`\`

### 3. Vector Index Integrity Verification

- Added the `verify_index_integrity()` method to `MemoryVectorIndex` to ensure consistency between the FAISS index and the ID-to-index mapping
- Implemented periodic index checks with configurable intervals
- Added detailed diagnostics for inconsistent states

\`\`\`python
def verify_index_integrity(self) -> Tuple[bool, Dict[str, Any]]:
    """Verify the integrity of the vector index."""
    faiss_count = self.count()
    mapping_count = len(self.id_to_index)
    is_consistent = faiss_count == mapping_count
    
    diagnostics = {
        "faiss_count": faiss_count,
        "mapping_count": mapping_count,
        "is_consistent": is_consistent
    }
    
    return is_consistent, diagnostics
\`\`\`

### 4. Threshold Configuration

- Made the default threshold configurable via `initial_retrieval_threshold` in the config
- Added support for dynamic threshold calibration based on user feedback
- Implemented logging of threshold decisions

### 5. Metadata Filtering

- Enhanced the `_filter_by_metadata` method to handle nested paths and complex filtering criteria
- Added the `metadata_filter` parameter to the `SynthiansClient.retrieve_memories()` method
- Improved logging of metadata filtering results

\`\`\`python
def _filter_by_metadata(self, candidates, metadata_filter):
    """Filter candidates based on metadata criteria."""
    if not metadata_filter:
        return candidates
        
    filtered_results = []
    for candidate in candidates:
        metadata = candidate.get("metadata", {})
        if not metadata:
            continue
            
        matches_all = True
        for key, value in metadata_filter.items():
            # Support for nested paths with dots
            if '.' in key:
                path_parts = key.split('.')
                current_obj = metadata
                # Navigate through the nested structure
                for part in path_parts[:-1]:
                    if part not in current_obj:
                        matches_all = False
                        break
                    current_obj = current_obj[part]
                
                if matches_all and (path_parts[-1] not in current_obj or current_obj[path_parts[-1]] != value):
                    matches_all = False
            elif key not in metadata or metadata[key] != value:
                matches_all = False
                break
                
        if matches_all:
            filtered_results.append(candidate)
            
    return filtered_results
\`\`\`

## Fixes for Specific Issues

### Fixed "0 Memories" Issue

The core issue preventing memory retrieval was identified as an `AttributeError` caused by calling the missing `verify_index_integrity()` method on the `MemoryVectorIndex` object. This was fixed by implementing the method with appropriate diagnostics.

**Error:**
\`\`\`
SynthiansMemory - ERROR - [SynthiansMemoryCore] Error in retrieve_memories: 'MemoryVectorIndex' object has no attribute 'verify_index_integrity'
SynthiansMemory - ERROR - Traceback (most recent call last):
  File "/workspace/project/synthians_memory_core/synthians_memory_core.py", line 441, in retrieve_memories
    is_consistent, diagnostics = self.vector_index.verify_index_integrity()
AttributeError: 'MemoryVectorIndex' object has no attribute 'verify_index_integrity'
\`\`\`

**Fix:**
Implemented the missing method in the `MemoryVectorIndex` class to check consistency between the FAISS index and the ID-to-index mapping.

### Fixed Client-Side Metadata Filtering

The `SynthiansClient` class was missing support for the `metadata_filter` parameter in its `retrieve_memories` method. This was fixed by adding the parameter and including it in the payload sent to the server.

\`\`\`python
async def retrieve_memories(self, query: str, top_k: int = 5, 
                           user_emotion: Optional[Dict[str, Any]] = None,
                           cognitive_load: float = 0.5,
                           threshold: Optional[float] = None,
                           metadata_filter: Optional[Dict[str, Any]] = None):
    # Add metadata_filter to payload
    if metadata_filter is not None:
        payload["metadata_filter"] = metadata_filter
\`\`\`

## Testing and Verification

A comprehensive diagnostic test was created to trace the memory lifecycle from creation to retrieval, revealing the root cause of the "0 memories" issue. After implementing the fixes, the test confirmed that:

1. Memories are successfully created and indexed
2. The index integrity check runs without errors
3. Memories are successfully retrieved with appropriate similarity scores
4. Target memories are found in results with high similarity scores

## Configuration Options

### New Options

- `check_index_on_retrieval` (bool): Controls whether to run index integrity checks on every retrieval
- `index_check_interval` (int): Time in seconds between periodic index integrity checks

## Future Considerations

### For Phase 2 (Metadata Integration & Filtering)

- Implement server-side metadata filtering logic to use the `metadata_filter` parameter in `retrieve_memories`
- Review and refine the emotional gating logic in `EmotionalGatingService`

### For Phase 3 (FAISS Index Management)

- Refactor `vector_index.py` to use FAISS's `IndexIDMap` for more reliable ID management
- Improve the persistence mechanism to ensure index consistency

```

# docs\archive\refactor-plan.md

```md
## **Unified Memory System: Technical Overview & Roadmap (Synthians Core)**

**Goal:** Consolidate the complex memory codebase into a single, efficient, unified system (`synthians_memory_core`) running locally (e.g., on an RTX 4090 via Docker), focusing on core memory operations, HPC-QuickRecal scoring, emotional context, and memory assemblies for an MVP by the end of the week.

---

### 1. **Technical Overview of the Unified `synthians_memory_core`**

This unified system centralizes memory functionality, integrating the most valuable and innovative concepts identified previously, while simplifying the architecture for clarity and maintainability.

**Core Components (Target Architecture):**

1.  **`SynthiansMemoryCore` (`synthians_memory_core.py`):**
    *   **Role:** The central orchestrator and main API endpoint.
    *   **Responsibilities:** Initializes and manages all other core components. Handles incoming requests for storing (`process_new_memory`) and retrieving (`retrieve_memories`) memories. Manages the in-memory cache/working set (`self.memories`), memory assemblies (`self.assemblies`), and coordinates background tasks. Delegates specialized tasks (scoring, geometry, persistence, emotion) to dedicated managers. Provides LLM tool interfaces (`get_tools`, `handle_tool_call`).
2.  **`UnifiedQuickRecallCalculator` (`hpc_quickrecal.py`):**
    *   **Role:** The single source of truth for calculating memory importance (`quickrecal_score`).
    *   **Responsibilities:** Implements various scoring modes (Standard, HPC-QR, Minimal, etc.) using configurable factor weights. Calculates factors like Recency, Emotion, Relevance, Importance, Personal, and potentially simplified versions of HPC-QR factors (Geometry, Novelty, Self-Org, Overlap) using the `GeometryManager`.
3.  **`GeometryManager` (`geometry_manager.py`):**
    *   **Role:** Central authority for all embedding geometry operations.
    *   **Responsibilities:** Validates embeddings (NaN/Inf checks). Normalizes vectors. Aligns vectors of different dimensions (e.g., 384 vs 768). Performs geometric transformations (e.g., Euclidean to Hyperbolic via `_to_hyperbolic`). Calculates distances and similarities based on the configured geometry (Euclidean, Hyperbolic, Spherical, Mixed).
4.  **`EmotionalAnalyzer` & `EmotionalGatingService` (`emotional_intelligence.py`):**
    *   **Role:** Handle emotional context.
    *   **Responsibilities:** `EmotionalAnalyzer` (simplified/placeholder for now) provides emotional analysis of text. `EmotionalGatingService` uses this analysis and user state to filter/re-rank retrieved memories, implementing cognitive defense and resonance scoring.
5.  **`MemoryPersistence` (`memory_persistence.py`):**
    *   **Role:** Sole handler for all disk-based memory operations.
    *   **Responsibilities:** Asynchronously saves (`save_memory`), loads (`load_memory`), and deletes (`delete_memory`) `MemoryEntry` objects using atomic writes (temp files + rename) and JSON format. Manages a memory index file (`memory_index.json`) and handles backups.
6.  **`MemoryEntry` & `MemoryAssembly` (`memory_structures.py`):**
    *   **Role:** Standard data structures.
    *   **Responsibilities:** `MemoryEntry` defines a single memory unit with content, embedding (standard and optional hyperbolic), QuickRecal score, and metadata. `MemoryAssembly` groups related `MemoryEntry` IDs, maintains a composite embedding (using `GeometryManager`), tracks activation, and handles emotional profiles/keywords for the group.
7.  **`ThresholdCalibrator` (`adaptive_components.py`):**
    *   **Role:** Enables adaptive retrieval relevance.
    *   **Responsibilities:** Dynamically adjusts the similarity threshold used in `retrieve_memories` based on feedback (`provide_feedback`) about whether retrieved memories were actually relevant.
8.  **`custom_logger.py`:**
    *   **Role:** Provides a consistent logging interface used by all components.

**Key Workflows in Unified System:**

*   **Memory Storage:**
    1.  `SynthiansMemoryCore.process_new_memory` receives content/embedding/metadata.
    2.  It calls `GeometryManager` to validate, align, and normalize the embedding.
    3.  It calls `UnifiedQuickRecallCalculator.calculate` to get the `quickrecal_score`.
    4.  It calls `EmotionalAnalyzer.analyze` to get emotional context for metadata.
    5.  If geometry is hyperbolic, it calls `GeometryManager._to_hyperbolic`.
    6.  It creates a `MemoryEntry`.
    7.  If score > threshold, it stores the `MemoryEntry` in `self.memories`.
    8.  It asynchronously calls `MemoryPersistence.save_memory`.
    9.  It calls `_update_assemblies` to potentially add the memory to relevant `MemoryAssembly` objects.
*   **Memory Retrieval:**
    1.  `SynthiansMemoryCore.retrieve_memories` receives query/embedding/context.
    2.  It calls `GeometryManager` to validate/align/normalize the query embedding.
    3.  It calls `_get_candidate_memories` which:
        *   Activates relevant `MemoryAssembly` objects based on similarity (using `GeometryManager.calculate_similarity`).
        *   Performs a quick direct similarity search against `self.memories` (using `GeometryManager.calculate_similarity`).
        *   Returns a combined list of candidate `MemoryEntry` objects.
    4.  It calculates relevance scores for candidates (using `GeometryManager.calculate_similarity`).
    5.  It calls `EmotionalGatingService.gate_memories` to filter/re-rank based on user emotion.
    6.  If `ThresholdCalibrator` is enabled, it filters results based on the current dynamic threshold.
    7.  Returns the top K results as dictionaries.

**Simplifications for MVP:**

*   **No Distributed Architecture:** Assumes a single process/container. `MemoryBroker` and `MemoryClientProxy` are removed.
*   **No Full Self/World Models:** The complex `SelfModel` and `WorldModel` classes are excluded. Basic context can be simulated or derived directly from memory/KG if needed later.
*   **No Advanced Dreaming/Narrative:** The `DreamProcessor`, `DreamManager`, `ReflectionEngine`, and `NarrativeIdentity` system are deferred. Dream insights could be stored as simple `MemoryEntry` objects if needed.
*   **Simplified Knowledge Graph:** The full modular KG is deferred. Core storage uses the `MemoryPersistence` layer. If basic graph features are needed *immediately*, use the `CoreGraphManager` directly, but avoid the full modular complexity for the MVP.
*   **Single Server:** Combines API endpoints into one server (`synthians_server.py`) using FastAPI. No separate Tensor/HPC servers needed locally; embedding/scoring happens within the `SynthiansMemoryCore` process.
*   **Simplified HPC-QR Factors:** For the MVP, `UnifiedQuickRecallCalculator` can initially focus on Recency, Relevance (Similarity), Emotion, Importance, Personal, Overlap. Geometric, Causal, and SOM factors can be added iteratively post-MVP.

---

### 2. **Identified Redundant Files/Components (To Be Removed for MVP)**

Based on the unification into `synthians_memory_core`:

1.  **High-Level Interfaces/Orchestrators:**
    *   `memory_manager.py`: Replaced by direct use of `SynthiansMemoryCore`.
    *   `memory_client.py` / `enhanced_memory_client.py`: Functionality absorbed into `SynthiansMemoryCore` or unnecessary.
    *   `advanced_memory_system.py`: Logic integrated into `SynthiansMemoryCore`.
    *   `memory_integration.py`: Replaced by `SynthiansMemoryCore`.
    *   `memory_router.py`: Routing logic is simplified within `SynthiansMemoryCore._get_candidate_memories`.
    *   `lucidia_memory.py` (`LucidiaMemorySystemMixin`): Not needed as components are directly integrated.
2.  **Persistence Layers:**
    *   `base.py` (`BaseMemoryClient`): Persistence logic replaced by `MemoryPersistence`.
    *   `long_term_memory.py`: Replaced by `SynthiansMemoryCore` + `MemoryPersistence`.
    *   `memory_system.py`: Replaced by `SynthiansMemoryCore` + `MemoryPersistence`.
    *   `unified_memory_storage.py`: Replaced by `MemoryPersistence` and `MemoryEntry`.
    *   `storage/memory_persistence_handler.py`: *This logic should be adapted/merged into `synthians_memory_core/memory_persistence.py`*. The file itself can then be removed.
3.  **Significance/QuickRecall Calculation:**
    *   `hpc_quickrecal.py` (Original `HPCQuickRecal` class): Logic merged into `UnifiedQuickRecallCalculator`.
    *   `hpc_qr_flow_manager.py`: Batching/workflow management integrated into `SynthiansMemoryCore` or handled by external callers if needed.
    *   `qr_calculator.py` (Original): Replaced by the version in `synthians_memory_core/hpc_quickrecal.py`.
4.  **HPC/Tensor Servers & Clients:**
    *   `hpc_server.py`: Not needed for local MVP; calculations happen within `SynthiansMemoryCore`.
    *   `updated_hpc_client.py`: Not needed.
    *   `tensor_server.py`: Not needed; embedding generation assumed external or handled differently.
5.  **Knowledge Graph:**
    *   `knowledge_graph.py` (Monolithic): Replaced by modular concept (deferred for MVP).
    *   `lucidia_memory_system/knowledge_graph/` (Entire modular directory): Deferred for post-MVP. Core storage uses `MemoryPersistence`.
6.  **Emotion Components:**
    *   `emotion.py` (`EmotionMixin`): Logic integrated into `SynthiansMemoryCore` using `EmotionalAnalyzer`.
    *   `emotional_intelligence.py` (within `Self`): Replaced by `synthians_memory_core/emotional_intelligence.py`.
    *   `emotion_graph_enhancer.py`: Deferred along with the full KG.
7.  **Adapters & Bridges:**
    *   `memory_adapter.py`: Not needed after unification.
    *   `memory_bridge.py`: Not needed after unification.
    *   `synthience_hpc_connector.py`: Logic for combining scores integrated into `SynthiansMemoryCore.retrieve_memories`. The external `SynthienceMemory` concept is removed for MVP.
8.  **Other:**
    *   `connectivity.py`: WebSocket logic removed as servers are removed.
    *   `tools.py`: Tool definitions moved to `SynthiansMemoryCore.get_tools`.
    *   `personal_details.py`: Basic pattern matching can be integrated directly into `SynthiansMemoryCore.process_new_memory` or a small utility function if needed.
    *   `rag_context.py`: Context generation handled by `SynthiansMemoryCore`.
    *   `memory_types.py` (Original): Replaced by `memory_structures.py`.
    *   `memory_client_example.py`: Update or remove.
    *   `test_advanced_memory.py`: Update or remove.
    *   All files under `lucidia_memory_system/core/Self/` and `lucidia_memory_system/core/World/`: Deferred for post-MVP.
    *   All files under `lucidia_memory_system/narrative_identity/`: Deferred for post-MVP.
    *   `system_events.py`: Event handling simplified or deferred.
    *   `memory_index.py`: Indexing logic might be integrated into `MemoryPersistence` or simplified.

**Files to Keep/Adapt for the MVP:**

*   All files within the new `synthians_memory_core/` directory (`__init__.py`, `synthians_memory_core.py`, `adaptive_components.py`, `custom_logger.py`, `emotional_intelligence.py`, `geometry_manager.py`, `hpc_quickrecal.py`, `memory_persistence.py`, `memory_structures.py`).
*   A *new* FastAPI server file (e.g., `synthians_server.py`) to expose `SynthiansMemoryCore`.
*   A *new* client file (e.g., `synthians_client.py`) to test the new server.
*   Relevant utility files (`logging_config.py`, `performance_tracker.py`, `cache_manager.py`) if their functionality is still desired and adapted.

---

### 3. **Development Roadmap for MVP (End of Week Target)**

**Goal:** A single Docker container running the unified `SynthiansMemoryCore` with basic storage, retrieval, HPC-QR scoring, emotional gating, assemblies, and adaptive thresholds.

**Assumptions:**
*   Focus is on the *memory system core*. Full Self/World model integration, Dreaming, Narrative, and complex KG are post-MVP.
*   Embedding generation is handled externally or via a placeholder within `SynthiansMemoryCore`.
*   You have a working Docker environment and Python 3.8+.

**Phase 1: Setup & Core Unification (Days 1-2)**

1.  **Directory Structure:**
    *   Create the new `synthians_memory_core` directory.
    *   Copy the proposed target files (`__init__.py`, `synthians_memory_core.py`, `hpc_quickrecal.py`, `geometry_manager.py`, `emotional_intelligence.py`, `memory_structures.py`, `memory_persistence.py`, `adaptive_components.py`, `custom_logger.py`) into it.
2.  **Dependencies:** Ensure all necessary libraries (`numpy`, `torch`, `aiofiles`) are installed (add to `requirements.txt`).
3.  **Integrate `UnifiedQuickRecallCalculator`:**
    *   Focus on `STANDARD` or `MINIMAL` mode initially for simplicity.
    *   Ensure it correctly uses `GeometryManager` for any distance/similarity calls.
    *   Implement basic versions of required factors (Recency, Relevance, Emotion, Importance, Overlap). Defer complex HPC-QR factors (Geometry, Causal, SOM) if necessary for speed, using defaults.
4.  **Integrate `GeometryManager`:**
    *   Ensure `SynthiansMemoryCore` uses it for all normalization, alignment, and similarity/distance calculations.
    *   Configure the desired default geometry (e.g., 'hyperbolic').
5.  **Integrate `MemoryPersistence`:**
    *   Ensure `SynthiansMemoryCore` uses this class *exclusively* for saving/loading memories via its async methods. Remove persistence logic from other classes.
6.  **Test Core Flow:** Write basic unit tests for `SynthiansMemoryCore.process_new_memory` and `SynthiansMemoryCore.retrieve_memories` using mock embeddings to verify the main data flow through the calculator, geometry manager, and persistence. Ensure GPU is utilized if configured and available (`torch.device`).

**Phase 2: Integrate Key Features (Days 3-4)**

1.  **Emotional Intelligence:**
    *   Wire `EmotionalAnalyzer` (even the simplified version) into `SynthiansMemoryCore`.
    *   Integrate `EmotionalGatingService` into the `retrieve_memories` flow.
    *   Test retrieval with different `user_emotion` contexts.
2.  **Memory Assemblies:**
    *   Implement the assembly creation (`_update_assemblies` triggered by `process_new_memory`) and retrieval (`_get_candidate_memories` using `_activate_assemblies`) logic within `SynthiansMemoryCore`.
    *   Assemblies should use `GeometryManager` for similarity.
    *   Test creating assemblies and retrieving memories via assembly activation.
3.  **Adaptive Thresholds:**
    *   Connect `ThresholdCalibrator` to the `retrieve_memories` results.
    *   Implement the `provide_feedback` method/endpoint to update the calibrator.
    *   Test retrieval results changing as feedback is provided.
4.  **Background Tasks:** Ensure the persistence and decay/pruning loops in `SynthiansMemoryCore` are functioning correctly using `asyncio`. Test shutdown.

**Phase 3: API Exposure & Cleanup (Day 5)**

1.  **Create FastAPI Server (`synthians_server.py`):**
    *   Create a new FastAPI app.
    *   In `startup`, initialize `SynthiansMemoryCore` (and call `initialize()`).
    *   In `shutdown`, call `SynthiansMemoryCore.shutdown()`.
    *   Expose endpoints mirroring the essential functions of `SynthiansMemoryCore`:
        *   `/process_memory` (POST)
        *   `/retrieve_memories` (POST)
        *   `/provide_feedback` (POST)
        *   `/detect_contradictions` (POST)
        *   `/health` (GET)
        *   `/stats` (GET)
        *   (Optional) Endpoints for assembly management.
2.  **Create Test Client (`synthians_client.py`):**
    *   Adapt `memory_client_example.py` to call the new FastAPI endpoints.
    *   Perform end-to-end tests: store, retrieve, feedback, check stats.
3.  **Dockerize:** Create a `Dockerfile` for the unified service. Include `requirements.txt`. Ensure the storage path is correctly mapped as a volume. Configure for GPU usage (e.g., using `nvidia-docker`).
4.  **Code Cleanup:** **Delete** all the identified redundant files from the project to avoid confusion.
5.  **Documentation:** Update the main `README.md` and the `synthians_memory_core/README.md` to reflect the new unified architecture.

**Post-MVP:**

*   Re-integrate advanced HPC-QR factors (Geometry, Causal Novelty, SOM) into `UnifiedQuickRecallCalculator`.
*   Re-introduce more sophisticated Dreaming, Narrative Identity, and Self/World Model components, ensuring they use the unified `SynthiansMemoryCore` API.
*   Re-implement the full modular Knowledge Graph system, potentially using `MemoryPersistence` for its storage backend.
*   Refine error handling and performance monitoring.
*   Consider re-introducing the `MemoryBroker` if a distributed architecture is needed later.

This roadmap focuses on creating a functional, unified core system quickly by leveraging the best existing components, centralizing logic, removing redundancy, and deferring the most complex cognitive features.
```

# docs\archive\synthience-trainer-compliment.md

```md
Okay, let's break down how the refactored `synthians_trainer_server` (based on the original "Titan" code) fits with your existing `synthians_memory_core` system.

Think of them as two specialized but complementary brain components:

1.  **`synthians_memory_core` (The Library / Database):**
    *   **Primary Role:** Stores, organizes, enriches, and retrieves *individual memories* (`MemoryEntry`).
    *   **Focus:** Content, metadata (emotion, importance, timestamps, etc.), relationships (assemblies), long-term persistence, fast similarity search (FAISS), adaptive relevance.
    *   **Analogy:** A highly organized, searchable, and cross-referenced library or knowledge base. You add individual books/articles (memories), tag them, link related ones, and can search for specific information or related topics. It knows *what* happened and *details* about it.

2.  **`synthians_trainer_server` (The Sequence Predictor):**
    *   **Primary Role:** Learns *temporal patterns and predicts sequences*. It operates on *sequences of embeddings*, not the raw memory content itself.
    *   **Focus:** Understanding the *flow* or *dynamics* between memory states (represented by embeddings). Given a current state (embedding + its internal memory `trainer_memory_vec`), it predicts the *next likely state* (embedding). It calculates "surprise" based on how well its prediction matches reality.
    *   **Analogy:** A system that learns the *plot* or *typical sequence of events* from reading sequences of stories (sequences of memory embeddings). It doesn't store the full stories themselves, but learns "if this kind of event happens, that kind of event often follows." It excels at prediction and understanding flow.

**How They Complement Each Other (The Workflow):**

An overarching AI system would likely use both in a loop:

1.  **Ingestion:** New information (text, audio transcript, interaction) comes in.
    *   **Memory Core:** Processes the information, generates an embedding, analyzes emotion, calculates QuickRecal, synthesizes metadata, and stores it as a `MemoryEntry`.
2.  **Sequence Generation:** Periodically, or based on context (e.g., retrieving memories related to a specific topic or time frame).
    *   **Memory Core:** Retrieves a *sequence* of related memories (likely represented by their embeddings, perhaps ordered by timestamp). This could be memories within an `MemoryAssembly` or memories retrieved based on a specific query over time.
3.  **Trainer Learning:** The sequence of embeddings retrieved from the *Memory Core* is fed into the...
    *   **Trainer Server:** Uses `train_sequence` or `train_step` to update its internal weights and `trainer_memory_vec`, learning the typical transitions between these memory states (embeddings).
4.  **Prediction & Understanding:** When the AI needs to anticipate, plan, or understand the current situation based on recent history:
    *   It takes the embedding of the *current* memory (or a recent sequence) from the *Memory Core*.
    *   **Trainer Server:** Uses `forward_pass` with the current embedding and its internal state (`trainer_memory_vec`) to predict the *next likely embedding* and calculate the `surprise`.
5.  **Feedback Loop (Optional but Powerful):**
    *   The predicted embedding from the *Trainer* could be used to *prime* or *guide* the next retrieval query in the *Memory Core*.
    *   The `surprise` value calculated by the *Trainer* could be added as metadata to new `MemoryEntry` objects being stored in the *Memory Core*, indicating how novel or unexpected that particular state transition was according to the learned sequence model. This could influence the `quickrecal_score`.

**Key Distinctions:**

*   **Data Unit:** Core handles `MemoryEntry` (content + embedding + metadata); Trainer handles sequences of *embeddings*.
*   **Goal:** Core is about *storage and recall*; Trainer is about *prediction and dynamics*.
*   **State:** Core maintains the state of individual memories; Trainer maintains an internal state (`trainer_memory_vec`) representing the *context of the current sequence*.
*   **Output:** Core retrieves existing memories; Trainer predicts *future* states (embeddings).

**In Summary:**

The `synthians_trainer_server` (formerly Titan) **doesn't store memories** like the `synthians_memory_core`. Instead, it **learns the relationships and transitions *between* the memories** (specifically, their embeddings) that are stored and retrieved by the `synthians_memory_core`. They work together: the Core provides the sequential data, and the Trainer learns the underlying patterns within that data, potentially feeding insights (like surprise) back to the Core.



```

# docs\CHANGELOG.md

```md
# Changelog

All notable changes to the Synthians Cognitive Architecture will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Comprehensive documentation structure in the `docs/` directory
- Placeholders for component deep dives to be filled in future updates

### Changed
- Reorganized documentation into logical sections (core, api, orchestrator, trainer, guides, testing)
- Updated API_REFERENCE.md to include metadata_filter parameter for memory retrieval

### Fixed
- Documentation now accurately reflects the latest codebase state
- Links and references updated to match the new structure

## [1.0.0] - 2025-03-30

### Added
- Functional surprise feedback loop from Neural Memory to Memory Core's QuickRecal score
- Comprehensive configuration via environment variables and config dictionaries
- Robust handling of embedding dimension mismatches (384D vs 768D)
- Enhanced emotional gating for memory retrieval

### Changed
- Refactored Vector Index to use FAISS IndexIDMap for more robust ID handling
- Improved retrieval pipeline with lower pre-filter threshold (0.3) for better recall sensitivity
- Centralized embedding geometry operations in GeometryManager

### Fixed
- Embedding validation to check for NaN/Inf values
- Metadata enrichment in process_new_memory workflow
- Redundant emotion analysis by respecting API-passed emotion data

## [0.9.0] - 2025-03-15

### Added
- Initial implementation of the Context Cascade Engine for orchestrating the cognitive cycle
- Implementation of the three Titans variants (MAC, MAG, MAL)
- Initial API for Neural Memory Server
- Test-time learning capability for Neural Memory Module

### Changed
- Enhanced FAISS integration with GPU support
- Improved Memory Core persistence mechanism

### Fixed
- TensorFlow and NumPy compatibility issues via lazy loading
- Background task cancellation during application shutdown

## [0.8.0] - 2025-02-28

### Added
- UnifiedQuickRecallCalculator with HPC-QR factors
- Emotional analysis and gating service
- MetadataSynthesizer for enriching memory entries
- Basic API server and client

### Changed
- Improved memory persistence with async operations
- Enhanced embedding generation with model configuration

### Fixed
- Memory retrieval performance issues
- Vector index persistence reliability

```

# docs\COMPONENT_GUIDE.md

```md
Okay, here are the component-specific documentation guides based on the current state of the codebase, emphasizing integration points.

---

## Component Guide: Synthians Memory Core

**Version:** 1.0
**Date:** March 29, 2025
**Primary Files:** `synthians_memory_core/`, `api/server.py`, `api/client/client.py`

### 1. Overview

The Synthians Memory Core serves as the primary, persistent storage and retrieval system for the Synthians cognitive architecture. It is responsible for managing individual memory entries (`MemoryEntry`) and related groups (`MemoryAssembly`), calculating memory relevance (`quickrecal_score`), handling emotional context, and providing fast, indexed access to memories. It is analogous to a highly organized, searchable knowledge base or library.

### 2. Core Responsibilities

*   **Memory Storage:** Persists `MemoryEntry` objects, including content, embeddings (Euclidean and optionally Hyperbolic), and rich metadata.
*   **Memory Retrieval:** Retrieves memories based on semantic similarity (via vector search), QuickRecal scores, emotional resonance, and optional metadata filters.
*   **Relevance Scoring:** Calculates `quickrecal_score` using the `UnifiedQuickRecallCalculator` based on factors like recency, emotion, importance, surprise feedback (intended), etc.
*   **Metadata Synthesis:** Enriches memories with temporal, emotional, cognitive, and embedding-based metadata using `MetadataSynthesizer`.
*   **Vector Indexing:** Provides fast similarity search using `MemoryVectorIndex` (FAISS), supporting CPU/GPU and persistence.
*   **Emotional Processing:** Analyzes emotional content (`EmotionAnalyzer`) and applies emotional gating/filtering during retrieval (`EmotionalGatingService`).
*   **Memory Assemblies:** Manages groups of related memories, maintaining composite embeddings and activation levels.
*   **Persistence:** Handles asynchronous saving/loading of memories and assemblies to disk (`MemoryPersistence`).
*   **Adaptive Thresholding:** Optionally adjusts retrieval similarity thresholds based on user feedback (`ThresholdCalibrator`).
*   **Geometry Management:** Uses `GeometryManager` for consistent handling of embedding dimensions, normalization, and geometric calculations.
*   **API Exposure:** Provides a comprehensive FastAPI interface for external interaction.
*   **Trainer Integration:** Provides endpoints (`/api/memories/get_sequence_embeddings`, `/api/memories/update_quickrecal_score`) for interaction with the sequence trainer/orchestrator.

### 3. Key Classes/Modules

*   `synthians_memory_core.SynthiansMemoryCore`: Main orchestrating class.
*   `synthians_memory_core.memory_structures`: Defines `MemoryEntry`, `MemoryAssembly`.
*   `synthians_memory_core.hpc_quickrecal`: `UnifiedQuickRecallCalculator`.
*   `synthians_memory_core.geometry_manager`: `GeometryManager`.
*   `synthians_memory_core.emotional_intelligence`: `EmotionalAnalyzer`, `EmotionalGatingService`.
*   `synthians_memory_core.memory_persistence`: `MemoryPersistence`.
*   `synthians_memory_core.vector_index`: `MemoryVectorIndex`.
*   `synthians_memory_core.metadata_synthesizer`: `MetadataSynthesizer`.
*   `synthians_memory_core.adaptive_components`: `ThresholdCalibrator`.
*   `synthians_memory_core.api.server`: FastAPI application exposing the core.
*   `synthians_memory_core.memory_core.trainer_integration`: `TrainerIntegrationManager`.

### 4. Configuration

*   Primary configuration is passed as a dictionary to the `SynthiansMemoryCore` constructor.
*   Key parameters include `embedding_dim`, `geometry` type, `storage_path`, `vector_index_type`, `persistence_interval`, etc.
*   Environment variables (`HOST`, `PORT`, `LOG_LEVEL`, `EMBEDDING_MODEL`) control the API server runtime.
*   `gpu_setup.py` attempts to configure FAISS for GPU usage during startup.

### 5. API Endpoints (Purpose)

The API (`api/server.py`) exposes core functionalities, including:
*   Memory CRUD-like operations (Process/Store, Retrieve).
*   Supporting functions (Generate Embedding, Analyze Emotion, Calculate QuickRecal).
*   Feedback mechanisms (Provide Relevance Feedback).
*   Advanced features (Detect Contradictions, Process Transcriptions, Assembly Management).
*   Integration endpoints for the trainer/orchestrator.

*(See `API_REFERENCE.md` for detailed endpoint definitions)*

### 6. Internal Workflow Example (Memory Storage)

1.  `/process_memory` endpoint receives data.
2.  Validates/aligns/normalizes incoming embedding (or generates one).
3.  Calls `SynthiansMemoryCore.process_new_memory`.
4.  `process_new_memory` orchestrates:
    *   Calculate QuickRecal score (`UnifiedQuickRecallCalculator`).
    *   Analyze emotion (`EmotionAnalyzer`).
    *   Calculate hyperbolic embedding if needed (`GeometryManager`).
    *   Synthesize metadata (`MetadataSynthesizer`).
    *   Create `MemoryEntry`.
    *   Store entry in `self._memories`.
    *   Save to disk (`MemoryPersistence.save_memory`).
    *   Update relevant `MemoryAssembly` objects.
    *   Add embedding to `MemoryVectorIndex`.
5.  Returns details of the processed memory.

### 7. Integration Points

*   **Receives From Context Cascade Engine (CCE):**
    *   New memory data via `POST /process_memory`.
    *   Requests for sequence embeddings via `POST /api/memories/get_sequence_embeddings`.
    *   Requests to update QuickRecal scores via `POST /api/memories/update_quickrecal_score` (Receives `memory_id`, `boost` value).
*   **Sends To Context Cascade Engine (CCE):**
    *   Response from `/process_memory` (includes `memory_id`, `embedding`, `quickrecal_score`, `metadata`).
    *   Response from `/retrieve_memories` (list of memory dictionaries).
    *   Response from `/api/memories/get_sequence_embeddings` (list of sequence embeddings).
    *   Response from `/api/memories/update_quickrecal_score` (update status).
*   **Internal Dependencies:** Relies heavily on its internal components (`GeometryManager`, `MemoryPersistence`, `MemoryVectorIndex`, `UnifiedQuickRecallCalculator`, etc.).

### 8. Current Status & Known Gaps

*   **Status:** Core storage, retrieval, indexing, emotion, metadata, and persistence functionalities are implemented and stable. The vector index uses `faiss.IndexIDMap` robustly. The API provides broad coverage, and the surprise feedback loop integration via `/api/memories/update_quickrecal_score` is functional.
*   **Minor Gaps:** Contradiction detection is basic; assembly removal logic is simplified. Embedding generation could be further centralized.

---

## Component Guide: Neural Memory Server

**Version:** 1.0
**Date:** March 29, 2025
**Primary Files:** `synthians_trainer_server/`

### 1. Overview

The Neural Memory Server implements an adaptive, associative memory based on the principles outlined in the Titans paper. Its core component is the `NeuralMemoryModule`, a TensorFlow/Keras model capable of **test-time learning**. It learns associations between Key and Value projections derived from input embeddings and can retrieve associated Values based on Query projections. It runs as a separate service, providing low-level associative memory operations.

### 2. Core Responsibilities

*   **Key/Value/Query Projections:** Calculates distinct vector projections (K, V, Q) from input embeddings using learned weight matrices (outer parameters).
*   **Associative Retrieval:** Given a Query projection (`q_t`), predicts/retrieves the associated Value embedding (`y_t`) using its internal `MemoryMLP` (`M`).
*   **Test-Time Update:** Updates the weights of its internal `MemoryMLP` (`M`) based on the association between the current `k_t` and `v_t` (or `v'_t` for MAL). Calculates `loss` and `grad_norm` as surprise metrics during this update.
*   **Dynamic Gate Calculation (for MAG):** Calculates adaptive gate values (`alpha_t`, `theta_t`, `eta_t`) based on attention outputs provided by the CCE.
*   **State Management:** Manages internal memory weights (`M`) and momentum state.
*   **Persistence:** Provides mechanisms to save and load its complete state (config, weights, momentum).
*   **Diagnostics:** Collects metrics (`MetricsStore`) and exposes diagnostic endpoints.
*   **API Exposure:** Provides a FastAPI interface for the CCE and potentially other tools.

### 3. Key Classes/Modules

*   `neural_memory.NeuralMemoryModule`: The core TensorFlow/Keras model implementing the memory logic.
*   `neural_memory.MemoryMLP`: The internal MLP representing the associative memory `M`.
*   `neural_memory.NeuralMemoryConfig`: Configuration class.
*   `http_server.py`: FastAPI application exposing the Neural Memory API.
*   `metrics_store.py`: `MetricsStore` for collecting operational metrics.
*   *(Surprise Calculation):* Core surprise metrics (`loss`, `grad_norm`) are calculated and returned directly by the `POST /update_memory` endpoint as part of the test-time update process. The `/analyze_surprise` endpoint (using `SurpriseDetector`) provides a way to calculate surprise post-hoc between two embeddings.

### 4. Configuration

*   Primary configuration via `NeuralMemoryConfig` (passed during initialization or loaded from state).
*   Key parameters: `input_dim`, `key_dim`, `value_dim`, `query_dim`, `memory_hidden_dims`, gate initial values, `outer_learning_rate`.
*   Initialized automatically on startup via `http_server.py`'s `startup_event`, but can be re-initialized via `POST /init`.
*   State persistence paths specified in `/save` and `/load` requests.

### 5. API Endpoints (Purpose)

The API (`http_server.py`) provides low-level operations for the CCE:
*   `POST /init`: Initialize/re-initialize the module.
*   `POST /get_projections`: Get K, V, Q projections without updating memory.
*   `POST /update_memory`: Perform the test-time learning update step, optionally using external gates (MAG) or projections (MAL). Returns surprise metrics.
*   `POST /retrieve`: Retrieve associated value embedding (`y_t`) given an input embedding.
*   `POST /calculate_gates`: Calculate dynamic gates based on attention output (for MAG).
*   `GET /config`, `POST /config`: Get/Set configuration details and capabilities.
*   `POST /save`, `POST /load`: Persist or load the module's state.
*   `GET /health`, `GET /status`: Check service health and initialization status.
*   `POST /analyze_surprise`: Analyze surprise between two embeddings.
*   `GET /diagnose_emoloop`: Get diagnostic metrics.

*(See `API_REFERENCE.md` for detailed endpoint definitions)*

### 6. Internal Workflow Example (Update Step)

1.  `/update_memory` receives data (input `x_t`, optional external K/V/Gates).
2.  `NeuralMemoryModule.update_step` is called.
3.  If K/V not provided externally, calculates `k_t, v_t` from `x_t` using `WK_layer`, `WV_layer`.
4.  Determines gate values (`alpha_t`, `theta_t`, `eta_t`) using external values (if MAG) or internal logits.
5.  Calculates predicted value `predicted_v = M(k_t)` using `MemoryMLP`.
6.  Calculates `loss = ||predicted_v - v_t||^2` (using original `v_t` or `v'_t` from MAL).
7.  Calculates gradients of `loss` w.r.t. `MemoryMLP` weights (`M`).
8.  Calculates `grad_norm`.
9.  Updates internal `momentum_state` using gradients and `theta_t`, `eta_t`.
10. Updates `MemoryMLP` weights (`M`) using `momentum_state` and `alpha_t`.
11. Returns `loss` and `grad_norm`.

### 7. Integration Points

*   **Receives From Context Cascade Engine (CCE):**
    *   Initialization requests via `POST /init`.
    *   Input embeddings (`x_t`) for projection via `POST /get_projections`.
    *   Input embeddings (`x_t`), optional external projections (`k_t`, `v'_t`), and optional external gates (`alpha_t`, `theta_t`, `eta_t`) for memory update via `POST /update_memory`.
    *   Input embeddings (`x_t`) for retrieval via `POST /retrieve`.
    *   Attention outputs for gate calculation via `POST /calculate_gates`.
    *   Requests for configuration/capabilities via `GET /config`.
    *   Requests for diagnostics via `GET /diagnose_emoloop`.
    *   Requests to save/load state via `POST /save`, `POST /load`.
*   **Sends To Context Cascade Engine (CCE):**
    *   Projections (`key_projection`, `value_projection`, `query_projection`) from `/get_projections`.
    *   Loss and gradient norm from `/update_memory`, along with projections/gates used.
    *   Retrieved embeddings (`retrieved_embedding`) and query projection from `/retrieve`.
    *   Calculated gate values (`alpha`, `theta`, `eta`) from `/calculate_gates`.
    *   Configuration details from `/config`.
    *   Diagnostic metrics from `/diagnose_emoloop`.
    *   Status/health information.
*   **Internal Dependencies:** TensorFlow, NumPy, `MetricsStore`, `SurpriseDetector`.

### 8. Current Status & Known Gaps

*   **Status:** Implemented and functional, including support for external projections (MAL) and gates (MAG) via the API. Test-time learning mechanism works. Persistence and diagnostics are integrated. Auto-initialization on startup implemented.
*   **Gaps:**
    *   **Performance:** Single-threaded `update_step` is slow; parallelization needed for high throughput.
    *   **Outer Loop:** `/train_outer` exists, but requires significant effort to use effectively for meta-learning optimal projection/gate parameters.
    *   **Complexity:** The internal dynamics are complex and require robust monitoring via the `MetricsStore` and diagnostic endpoints.

---

## Component Guide: Context Cascade Engine (CCE)

**Version:** 1.0
**Date:** March 29, 2025
**Primary Files:** `orchestrator/`

### 1. Overview

The Context Cascade Engine (CCE) serves as the central orchestrator for the Synthians cognitive architecture. It manages the bi-directional flow of information between the persistent Memory Core and the adaptive Neural Memory Server. The CCE implements the core cognitive cycle and dynamically adapts its processing based on the configured Titans Architecture Variant (NONE, MAC, MAG, MAL), integrating attention mechanisms where appropriate.

### 2. Core Responsibilities

*   **Orchestration:** Manages the step-by-step execution of the cognitive cycle for processing new inputs.
*   **Service Integration:** Communicates with the Memory Core and Neural Memory Server APIs.
*   **Variant Management:** Selects and executes the logic for the active Titans variant (MAC, MAG, MAL, or NONE).
*   **History Management:** Maintains a sequential history of embeddings and projections (`SequenceContextManager`) needed for attention calculations in variants.
*   **Surprise Feedback:** Receives surprise metrics (loss, grad_norm) from the Neural Memory Server and initiates QuickRecal score updates in the Memory Core via `POST /api/memories/update_quickrecal_score`.
*   **Context Propagation:** Ensures relevant information (embeddings, projections, metadata) is passed between stages.
*   **Error Handling:** Manages communication errors with downstream services.

### 3. Key Classes/Modules

*   `context_cascade_engine.ContextCascadeEngine`: The main orchestrating class.
*   `history.SequenceContextManager`: Manages the deque of historical context tuples.
*   `titans_variants.py`: Defines `TitansVariantType` enum, `TitansVariantBase`, `MACVariant`, `MAGVariant`, `MALVariant` classes, and the `create_titans_variant` factory.
*   `server.py`: Basic FastAPI application exposing the CCE (primarily `/process_memory`).

### 4. Configuration

*   Reads Memory Core URL (`MEMORY_CORE_URL`) and Neural Memory Server URL (`NEURAL_MEMORY_URL`) from environment variables or defaults.
*   Reads the active Titans variant (`TITANS_VARIANT`) from environment variables (defaults to `NONE`).
*   Configures `SequenceContextManager` length.
*   Retrieves dynamic configuration (e.g., attention parameters) from the Neural Memory Server via `/config` on initialization.

### 5. API Endpoints (Purpose)

The API (`orchestrator/server.py`) primarily exposes:
*   `POST /process_memory`: The main entry point that triggers the entire orchestrated cognitive cycle for a given input.
*   Potentially other passthrough endpoints (like `/get_sequence_embeddings`, `/analyze_surprise`).

*(See `API_REFERENCE.md` for detailed endpoint definitions)*

### 6. Internal Workflow (Refactored Cognitive Cycle)

The CCE's `process_new_input` method executes the following orchestrated steps:
1.  Call Memory Core (`/process_memory`) to store input and get `x_t`, `memory_id`.
2.  Call Neural Memory (`/get_projections`) to get `k_t`, `v_t`, `q_t`.
3.  If MAG/MAL active, execute variant pre-update logic (calculating gates or `v'_t`).
4.  Call Neural Memory (`/update_memory`) with `x_t` and any variant modifications (gates/projections). Receive `loss`, `grad_norm`.
5.  Call Memory Core (`/api/memories/update_quickrecal_score`) with surprise metrics to boost QuickRecal.
6.  Call Neural Memory (`/retrieve`) with `x_t` to get raw associated embedding `y_t_raw` and the `q_t` used.
7.  If MAC active, execute variant post-retrieval logic to get final `y_t_final`. Otherwise `y_t_final = y_t_raw`.
8.  Store the full context `(timestamp, memory_id, x_t, k_t, v_t, q_t, y_t_final)` in `SequenceContextManager`.
9.  Return a consolidated response.

*(See Architecture Diagram in main ARCHITECTURE.md)*

### 7. Integration Points

*   **Calls Synthians Memory Core API:**
    *   `POST /process_memory` (Input: content, embedding, metadata; Output: memory_id, embedding, score, metadata)
    *   `POST /api/memories/update_quickrecal_score` (Input: memory_id, delta, reason; Output: status)
    *   `POST /api/memories/get_sequence_embeddings` (Passthrough - Input: filters; Output: sequence)
*   **Calls Neural Memory Server API:**
    *   `POST /get_projections` (Input: input_embedding; Output: k, v, q projections)
    *   `POST /update_memory` (Input: input_embedding, optional external k/v/gates; Output: loss, grad_norm, projections used, gates applied)
    *   `POST /retrieve` (Input: input_embedding; Output: retrieved_embedding, query_projection)
    *   `POST /calculate_gates` (MAG only - Input: attention_output; Output: alpha, theta, eta)
    *   `GET /config` (Input: None; Output: Configuration details)
    *   `POST /analyze_surprise` (Passthrough - Input: pred_emb, actual_emb; Output: surprise metrics)
*   **Internal Dependencies:** `SequenceContextManager`, `TitansVariantBase` subclasses, shared `GeometryManager`, `MetricsStore`.

### 8. Current Status & Known Gaps

*   **Status:** Implements the refactored cognitive flow correctly, enabling proper timing for MAC, MAG, and MAL variants. Integrates with `SequenceContextManager` for history. Dynamically configures itself. Uses lazy loading for TensorFlow.
*   **Gaps:**
    *   **Error Handling:** While basic error handling exists, more sophisticated strategies for handling failures in Memory Core or Neural Memory calls (e.g., retries, fallback logic) could be added.
    *   **State Management:** If the CCE were to become stateful (beyond the sequence history), careful management would be needed. Currently designed as mostly stateless per request cycle.

---

## Inter-Component Integration Summary

*   **New Input:** User/System -> **CCE (`/process_memory`)** -> **Memory Core (`/process_memory`)** -> Returns `x_t`, `mem_id` to CCE.
*   **Association Learning:** CCE -> **Neural Memory (`/get_projections`)** -> Returns `k_t, v_t, q_t` -> CCE -> **Variant Pre-Update (MAG/MAL)** -> CCE -> **Neural Memory (`/update_memory`)** -> Returns `loss`, `grad_norm`.
*   **Surprise Feedback:** CCE -> **Memory Core (`/update_quickrecal_score`)** with `loss`/`grad_norm` -> Memory Core updates score.
*   **Associative Retrieval:** CCE -> **Neural Memory (`/retrieve`)** -> Returns `y_t_raw`, `q_t` -> CCE -> **Variant Post-Update (MAC)** -> Generates `y_t_final`.
*   **History:** CCE updates `SequenceContextManager` with `(ts, mem_id, x_t, k_t, v_t, q_t, y_t_final)`.
*   **Configuration:** CCE -> **Neural Memory (`/config`)** -> Returns NM/Attention config.

This flow highlights the central role of the CCE in mediating all interactions and implementing the core logic of the bi-hemispheric model and its variants.
```

# docs\core\Embedding_Dimension_Handling_Strategy.md

```md
Okay, here is the specific document detailing the embedding dimension handling strategy currently implemented in the Synthians codebase.

\`\`\`markdown
# Synthians Cognitive Architecture: Embedding Dimension Handling Strategy

**Version:** 1.0
**Date:** March 29, 2025

## 1. Overview

This document outlines the strategy employed across the Synthians cognitive architecture (Memory Core, Neural Memory Server, Orchestrator) to handle potentially different embedding dimensions (e.g., 384D vs. 768D) and ensure robust processing of vector data.

The core goals of this strategy are:

1.  **Consistency:** Ensure vector operations (similarity, distance) work reliably even with mixed-dimension inputs.
2.  **Configurability:** Allow definition of a primary `embedding_dim` and an `alignment_strategy`.
3.  **Robustness:** Validate embeddings for correctness (e.g., check for NaN/Inf values) and handle invalid data gracefully.
4.  **Compatibility:** Ensure components requiring specific dimensions (like FAISS index, Neural Memory projections) receive correctly dimensioned data.

## 2. Core Strategy: Multi-Layered Validation and Alignment

The system uses a multi-layered approach, primarily centered around the `GeometryManager`, but with validation and alignment steps occurring at different component boundaries:

1.  **Central Authority (`GeometryManager`):**
    *   Defines the system's target `embedding_dim` via its configuration.
    *   Defines the `alignment_strategy` (`'truncate'` or `'pad'`) to use when dimensions mismatch the target.
    *   Provides core methods for validation (`_validate_vector`), alignment (`align_vectors`), and normalization (`normalize_embedding`).

2.  **API Layer Validation (Memory Core API):**
    *   The main API server (`api/server.py`) performs initial validation and alignment of embeddings received in requests (e.g., in `/process_memory`) *before* passing them to the `SynthiansMemoryCore` logic. This acts as a first line of defense.

3.  **Memory Core Internal Processing:**
    *   The `SynthiansMemoryCore` class relies heavily on the `GeometryManager` instance for all internal embedding operations: validating inputs, aligning vectors for comparison (`calculate_similarity`), and normalizing vectors.

4.  **Vector Index Internal Alignment (`MemoryVectorIndex`):**
    *   The `MemoryVectorIndex` (FAISS wrapper) performs its *own* validation and alignment (`_validate_embedding`, `_align_embedding_dimension`) when adding vectors (`add`) or receiving query vectors (`search`).
    *   **Crucially:** It ensures that all vectors *stored within the FAISS index itself* strictly match the index's configured `embedding_dim`. This is achieved by padding or truncating vectors *before* they are added to the FAISS C-level index.

5.  **Neural Memory Server Expectations:**
    *   The Neural Memory module (`neural_memory.py`) expects input tensors matching the dimensions defined in its `NeuralMemoryConfig` (`input_dim`, `key_dim`, etc.).
    *   Validation for the Neural Memory API (`http_server.py`) checks incoming vectors against these expected dimensions using `_validate_vector`.

6.  **Orchestrator (`ContextCascadeEngine`):**
    *   Acts primarily as a conduit, converting numpy arrays to lists for API calls.
    *   Relies on the shared `GeometryManager` for any internal validation or processing needs.

## 3. Key Components and Implementation Details

### 3.1. `GeometryManager`

*   **Configuration:**
    *   `embedding_dim`: Target dimension (e.g., 768).
    *   `alignment_strategy`: `'truncate'` (shorten longer vectors) or `'pad'` (zero-pad shorter vectors). Default appears to be a hybrid based on relative size if not specified, but explicit config is preferred.
    *   `normalization_enabled`: Controls L2 normalization.
*   **`_validate_vector`:** Checks for `None`, converts to `np.float32` array, checks for `NaN`/`Inf` (replaces with zeros and warns).
*   **`align_vectors`:** Takes two vectors, aligns *both* to the configured `embedding_dim` based on the `alignment_strategy`. Logs warnings on dimension mismatch (limited number of warnings).
*   **`normalize_embedding`:** Performs L2 normalization if enabled. Handles zero vectors.
*   **Backward Compatibility:** Includes `_align_vectors` and `_normalize` methods that simply forward calls to the non-underscored versions, ensuring components using older naming still work.

\`\`\`python
# synthians_memory_core/geometry_manager.py

def align_vectors(self, vec_a: np.ndarray, vec_b: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    # ... validation ...
    target_dim = self.config['embedding_dim']
    strategy = self.config['alignment_strategy']
    # ... logic to pad/truncate vec_a and vec_b to target_dim ...
    if dim_a != target_dim:
        # Apply strategy to align vec_a to target_dim
        aligned_a = self._apply_alignment(vec_a, target_dim, strategy)
    if dim_b != target_dim:
        # Apply strategy to align vec_b to target_dim
        aligned_b = self._apply_alignment(vec_b, target_dim, strategy)
    return aligned_a, aligned_b

def _validate_vector(...):
    # ... checks for None, type, NaN/Inf ...
    if np.isnan(vector).any() or np.isinf(vector).any():
        # ... log warning ...
        return np.zeros_like(vector) # Replace invalid vector with zeros
    return vector
\`\`\`

### 3.2. `MemoryVectorIndex` (FAISS Wrapper)

*   **Configuration:** Takes `embedding_dim` on initialization, which *must* match the dimension of the internal FAISS index.
*   **`_validate_embedding`:** Internal validation similar to `GeometryManager`, but *also* performs alignment (padding/truncation) to match `self.embedding_dim`. This is crucial because FAISS requires all vectors within an index to have the same dimension.
*   **`add`:** Calls `_validate_embedding` on the input vector. The validated (and potentially aligned) vector is added to the FAISS index.
*   **`search`:** Calls `_validate_embedding` on the query vector to ensure it matches the index dimension before performing the FAISS search.

\`\`\`python
# synthians_memory_core/vector_index.py

def _validate_embedding(self, embedding: Union[np.ndarray, list, tuple]) -> Optional[np.ndarray]:
    # ... checks for None, type, 1D shape, NaN/Inf ...

    # Check dimension and align to self.embedding_dim
    if len(embedding) != self.embedding_dim:
        logger.warning(f"Embedding dimension mismatch: expected {self.embedding_dim}, got {len(embedding)}")
        if len(embedding) < self.embedding_dim:
            # Pad with zeros
            padding = np.zeros(self.embedding_dim - len(embedding), dtype=np.float32)
            embedding = np.concatenate([embedding, padding])
        else:
            # Truncate
            embedding = embedding[:self.embedding_dim]
    # ... ensure float32 ...
    return embedding

def add(self, memory_id: str, embedding: np.ndarray) -> bool:
    validated_embedding = self._validate_embedding(embedding)
    if validated_embedding is None: return False
    # FAISS expects shape [n, dim]
    self.index.add(np.array([validated_embedding], dtype=np.float32))
    # ... update mapping ...

def search(self, query_embedding: np.ndarray, k: int = 5, threshold: float = 0.0) -> List[Tuple[str, float]]:
    validated_query = self._validate_embedding(query_embedding)
    if validated_query is None: return []
    # FAISS expects shape [n, dim]
    distances, indices = self.index.search(np.array([validated_query], dtype=np.float32), k)
    # ... process results ...
\`\`\`

### 3.3. Memory Core API Server (`api/server.py`)

*   **`process_memory` Endpoint:** Explicitly validates the incoming `embedding` for NaN/Inf and dimension mismatches *before* calling `memory_core.process_new_memory`. It aligns the embedding to the expected dimension (`memory_core.config['embedding_dim']`).
*   **Other Endpoints:** Generally pass embeddings as lists within JSON payloads. Downstream components are responsible for validation and alignment.

\`\`\`python
# synthians_memory_core/api/server.py - Inside process_memory endpoint

if embedding is not None:
    # ... Check for NaN/Inf ...
    # Ensure correct dimensionality
    expected_dim = app.state.memory_core.config.get('embedding_dim', 768)
    actual_dim = len(embedding)
    if actual_dim != expected_dim:
        logger.warning(...)
        if actual_dim < expected_dim:
            embedding = embedding + [0.0] * (expected_dim - actual_dim) # Pad
        else:
            embedding = embedding[:expected_dim] # Truncate

# Call core processing with potentially aligned embedding
result = await app.state.memory_core.process_new_memory(...)
\`\`\`

### 3.4. `SynthiansMemoryCore` Class

*   **`process_new_memory`:** Receives embedding (potentially pre-aligned by the API layer), validates again using `geometry_manager._validate_vector`, aligns using `geometry_manager._align_vectors` (often redundant if API pre-aligned, but safe), and normalizes using `geometry_manager._normalize`.
*   **`retrieve_memories` / `_get_candidate_memories`:** Uses `geometry_manager.calculate_similarity` for comparisons *after* retrieving candidates. Candidate retrieval relies on `vector_index.search`, where alignment happens internally.

### 3.5. Neural Memory Server (`synthians_trainer_server/http_server.py`)

*   **`_validate_vector` Helper:** Validates incoming vectors in API requests against the specific dimensions required by the endpoint (e.g., `input_dim` for `/update_memory`, `query_dim` for `/retrieve` queries *after projection*). It raises HTTPExceptions on mismatch. **It does not perform alignment.**
*   **Expectation:** Assumes the caller (CCE) provides correctly dimensioned vectors based on the Neural Memory's configuration.

### 3.6. Orchestrator (`orchestrator/context_cascade_engine.py`)

*   Relies on the shared `GeometryManager` for validation (`_validate_embedding`).
*   Uses helper (`_to_list`) to convert numpy arrays to lists before sending them via API calls to the Memory Core or Neural Memory Server.

## 4. Validation Details

*   **NaN/Inf Handling:** Vectors containing `NaN` or `Inf` are detected by `_validate_vector` (in `GeometryManager` and `MemoryVectorIndex`). These invalid vectors are typically replaced with **zero vectors** of the appropriate dimension, accompanied by a warning log.
*   **Shape:** Validation generally ensures vectors are 1-dimensional.
*   **Type:** Vectors are consistently converted to `np.float32` before being used in FAISS or TensorFlow operations.

## 5. Normalization

*   L2 normalization is typically applied to embeddings before storage, similarity calculation, or use in geometric operations.
*   This is controlled by the `normalization_enabled` flag in `GeometryManager` and implemented in `normalize_embedding`.

## 6. Configuration

*   **`embedding_dim`:** Set consistently across `GeometryManager`, `MemoryVectorIndex`, Memory Core API server (`SynthiansMemoryCore` config), and relevant dimensions in `NeuralMemoryConfig`.
*   **`alignment_strategy`:** Configured in `GeometryManager` (`'truncate'` or `'pad'`).

## 7. Potential Issues & Areas for Improvement

*   **Redundancy:** `MetadataSynthesizer` contains its own `_validate_embedding` and `_align_vectors_for_comparison` methods. These should ideally be removed, and it should use the shared `GeometryManager` instance for consistency.
*   **Consistency Checks:** Add startup checks to verify that `embedding_dim` configurations match across key components (GeometryManager, VectorIndex, NeuralMemory input/output dims where applicable).
*   **Alignment Strategy Default:** The default behavior in `GeometryManager`'s `align_vectors` if `alignment_strategy` isn't explicitly 'pad' or 'truncate' seems to be a mix (truncate if larger, pad if smaller). This should be clarified or made stricter based on the config value.

## 8. Conclusion

The Synthians system employs a robust, multi-layered strategy for handling embedding dimensions and validation. `GeometryManager` serves as the central configuration point, while `MemoryVectorIndex` ensures internal consistency for FAISS. Validation and alignment occur at API boundaries and within core components, aiming for both flexibility and operational reliability. Key features include NaN/Inf replacement, configurable alignment (padding/truncation), and consistent use of L2 normalization.
\`\`\`
```

# docs\core\embedding_handling.md

```md
# Embedding Handling in Synthians Memory Core

## Overview

The Synthians Memory Core implements robust handling for embeddings throughout the system, addressing several critical challenges:

1. **Dimension Mismatches**: Safely handling vectors of different dimensions (e.g., 384 vs. 768)
2. **Malformed Embeddings**: Detecting and handling NaN/Inf values in embedding vectors
3. **Efficient Retrieval**: Using FAISS for fast similarity search with automatic GPU acceleration
4. **Component Compatibility**: Ensuring consistent behavior across different components through backward compatibility

## System Architecture for Embedding Processing

The embedding handling system is integrated throughout the Memory Core with several key components working together:

1. **Entry Points:**
   * `process_new_memory`: Initial ingestion of embeddings from the API
   * `retrieve_memories`: Handling query embeddings for retrieval
   * `update_memory`: Updates to memory vectors

2. **Core Components:**
   * `GeometryManager`: Provides the mathematical operations (see `geometry.md`)
   * `MemoryVectorIndex`: Manages storage and retrieval of embeddings with FAISS (see `vector_index.md`)
   * `MetadataSynthesizer`: Enriches metadata with embedding-related statistics
   * `EmotionalGatingService`: Uses embeddings for emotional gating

3. **Processing Pipeline:**
   * Validation → Enrichment → Storage → Indexing → Retrieval

## Validation and Fallback System

The Memory Core implements a comprehensive validation system for embeddings:

\`\`\`python
def _validate_embedding(embedding, allow_zero=True):
    """Validate that an embedding vector contains only valid values.
    
    Args:
        embedding: The embedding vector to validate
        allow_zero: Whether to allow zero vectors
        
    Returns:
        bool: True if the embedding is valid, False otherwise
    """
    if embedding is None:
        return False
        
    # Convert to numpy array if needed
    if not isinstance(embedding, np.ndarray):
        embedding = np.array(embedding, dtype=np.float32)
        
    # Check for NaN or Inf values
    if np.isnan(embedding).any() or np.isinf(embedding).any():
        return False
        
    # Optionally check for zero vectors
    if not allow_zero and np.all(embedding == 0):
        return False
        
    return True
\`\`\`

When invalid embeddings are detected, the system provides fallbacks:

1. **Zero Vector Substitution**: Invalid embeddings are replaced with zero vectors
2. **Default Embedding Generation**: For text content, a default embedding can be generated
3. **Error Logging**: Comprehensive logging of embedding issues for diagnostics
4. **Safe Comparison**: Ensures no operations fail due to invalid inputs

## Backward Compatibility Layer

To ensure consistent behavior across all components, backward compatibility methods bridge naming conventions and handle legacy code patterns:

\`\`\`python
def _align_vectors(self, v1: np.ndarray, v2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Backward compatibility method that forwards to align_vectors."""
    return self.align_vectors(v1, v2)

def _normalize(self, vector: np.ndarray) -> np.ndarray:
    """Backward compatibility method that forwards to normalize_embedding."""
    # Ensure vector is numpy array before calling
    validated_vector = self._validate_vector(vector, "Vector for _normalize")
    if validated_vector is None:
        # Return zero vector if validation fails
        return np.zeros(self.config.get('embedding_dim', 768), dtype=np.float32)
    return self.normalize_embedding(validated_vector)
\`\`\`

## Integration with Vector Index

The embedding handling system integrates with the FAISS vector index:

\`\`\`python
def search(self, query_embedding: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
    """Search for similar embeddings in the index.
    
    Args:
        query_embedding: The embedding to search for
        k: Number of results to return
        
    Returns:
        List of (memory_id, similarity_score) tuples
    """
    # Validate and normalize the query embedding
    if not self._validate_embedding(query_embedding):
        logger.warning("Invalid query embedding provided to vector index search")
        # Return empty results rather than crashing
        return []
    
    # Normalize for cosine similarity
    query_embedding = self._normalize_embedding(query_embedding)
    
    # Perform the search
    D, I = self.index.search(query_embedding.reshape(1, -1), k)
    
    # Map FAISS IDs back to memory_ids and return with similarity scores
    results = []
    for i, (distance, idx) in enumerate(zip(D[0], I[0])):
        if idx != -1:  # -1 indicates no match found
            memory_id = self.id_map.get(int(idx))
            if memory_id:
                # Convert distance to similarity score
                similarity = 1.0 - min(1.0, float(distance) / 2.0)
                results.append((memory_id, similarity))
    
    return results
\`\`\`

## Cross-Component Embedding Dimension Handling

The Memory Core handles embedding dimensions consistently across components:

1. **Configuration Inheritance**:
   * The main `SynthiansMemoryCore` config sets the primary `embedding_dim` (default: 768)
   * This is passed down to `GeometryManager`, `MemoryVectorIndex`, and other components

2. **Runtime Dimension Handling**:
   * Components can handle input embeddings of different dimensions
   * The configurable `alignment_strategy` in `GeometryManager` determines how these mismatches are handled
   * By default, the system uses `'truncate'` strategy (truncating larger vectors to match smaller ones)

3. **Service Integration**:
   * Neural Memory Server may use a different embedding dimension
   * Alignment happens automatically when integrating with external services

## QuickRecal and Embedding Properties

The embedding system interacts with QuickRecal calculation:

1. **Geometric Properties**:
   * The UnifiedQuickRecallCalculator uses embedding properties for novelty calculation
   * Geometric metrics like causal novelty are computed from embeddings

2. **Integration with Neural Memory**:
   * Embeddings are passed to the Neural Memory for learning and prediction
   * Surprise metrics from Neural Memory affect QuickRecal scores

## Recent System Improvements

Recent updates to the embedding handling system include:

1. **Robust Validation Pipeline**:
   * Enhanced validation throughout the system 
   * Consistent handling of edge cases (NaN, Inf, zero vectors)

2. **Dimension Mismatch Handling**:
   * Improved handling of 384 vs 768 dimension embeddings
   * Configurable alignment strategies with sensible defaults

3. **Service Integration**:
   * Better interoperability with Neural Memory Server
   * Enhanced error handling for external service failures

4. **Performance Optimizations**:
   * Reduced redundant embedding operations
   * More efficient vector storage and retrieval

```

# docs\core\emotion.md

```md
# Emotional Intelligence Components

The Synthians Memory Core incorporates emotional context into memory processing and retrieval through two key components within the `synthians_memory_core.emotional_intelligence` module.

## 1. `EmotionAnalyzer`

*   **Purpose:** Analyzes text content to determine its emotional profile.
*   **Functionality:**
    *   Typically utilizes an external library or model (like `transformers` with a sentiment/emotion classification model) to analyze input text.
    *   Outputs structured emotional data, often including:
        *   `dominant_emotion`: The most prominent emotion detected (e.g., joy, sadness, anger).
        *   `sentiment_label`: Positive, Negative, or Neutral.
        *   `sentiment_score`: A numerical value indicating sentiment polarity/intensity.
        *   Emotion scores: Confidence scores for various basic emotions.
    *   This information is added to the `metadata` of a `MemoryEntry` during processing.
*   **Configuration:** May require specifying the model name or path in the core configuration.

## 2. `EmotionalGatingService`

*   **Purpose:** Filters or re-ranks memory retrieval results based on emotional context.
*   **Functionality:**
    *   Takes the initial list of candidate memories retrieved (e.g., via vector search).
    *   Considers the user's current emotional state (if provided) and the emotional metadata stored within each candidate memory.
    *   Applies rules or scoring adjustments to:
        *   **Filter:** Remove memories that clash significantly with the user's current state or are deemed inappropriate given the context.
        *   **Re-rank:** Boost memories that resonate emotionally with the user's state or the query context.
    *   Aims to provide more contextually relevant and potentially more empathetic recall.
*   **Integration:** Used within the `SynthiansMemoryCore.retrieve_memories` method after initial candidate retrieval.

## Importance

Integrating emotional intelligence allows the memory system to:

*   Tag memories with their emotional context at the time of encoding.
*   Provide recall that is sensitive to the user's current emotional state.
*   Potentially prioritize memories associated with strong emotions, mimicking aspects of human memory.

## Recent Improvements

The emotion processing components have been enhanced to handle embedding dimension mismatches (384D vs 768D) through:

- Updates to the `_calculate_emotion` method to use vector alignment utilities
- Proper fallbacks when either the emotion service is unavailable or dimension mismatches occur
- Integration with the `MetadataSynthesizer` to ensure emotional metadata is consistently stored

## Configuration Options

*To be added: Documentation on configuration parameters for the emotion components*

```

# docs\core\geometry.md

```md
# Geometry Management

The `synthians_memory_core.geometry_manager.GeometryManager` class is responsible for handling the geometric aspects of embedding vectors within the Synthians Memory Core.

## Core Responsibilities

1.  **Dimension Handling & Alignment:**
    *   Ensures that vectors being compared or processed have compatible dimensions, even if the system ingests embeddings of different sizes (e.g., 384 vs. 768).
    *   Uses the configured `alignment_strategy` with a **default of `'truncate'`** (not `'pad'`). This means that by default, when aligning vectors of different dimensions, the larger vector will be truncated to match the smaller one's dimension.
    *   The other available strategies are `'pad'` (which pads the smaller vector with zeros) and `'project'` (reserved for future implementation of dimension reduction techniques).
    *   Implementation of the alignment logic via the `align_vectors` method:
      \`\`\`python
      if strategy == 'pad':
          # Pad the smaller vector with zeros
          if dim_a < target_dim:
              aligned_a = np.pad(vec_a, (0, target_dim - dim_a), 'constant')
          if dim_b < target_dim:
              aligned_b = np.pad(vec_b, (0, target_dim - dim_b), 'constant')
      elif strategy == 'truncate':
          # Truncate to smaller dimension
          if dim_a > target_dim:
              aligned_a = vec_a[:target_dim]
          if dim_b > target_dim:
              aligned_b = vec_b[:target_dim]
      \`\`\`

2.  **Normalization:**
    *   Provides methods for L2 normalization (`normalize_embedding`), ensuring vectors have unit length, which is crucial for accurate cosine similarity calculations.
    *   Handles edge cases like zero vectors and vectors with NaN/Inf values during normalization.

3.  **Distance & Similarity Calculation:**
    *   Offers functions to compute distances (e.g., Euclidean) and similarities (e.g., Cosine) between vectors.
    *   Abstracts the specific geometric calculations based on configuration.
    *   Supports different similarity metrics:
      \`\`\`python
      def calculate_similarity(self, vec_a, vec_b):
          """Calculate similarity between two vectors based on the configured geometry."""
          geometry_type = self.config.get('geometry_type', GeometryType.EUCLIDEAN)
          
          if geometry_type == GeometryType.EUCLIDEAN:
              return self.calculate_cosine_similarity(vec_a, vec_b)
          elif geometry_type == GeometryType.HYPERBOLIC:
              return self.calculate_hyperbolic_similarity(vec_a, vec_b)
          # ... other geometries
      \`\`\`

4.  **Geometric Space Management:**
    *   Supports different geometric spaces beyond Euclidean:
      * `EUCLIDEAN`: Standard Euclidean space with cosine similarity
      * `HYPERBOLIC`: Hyperbolic space with custom similarity calculation
      * `SPHERICAL`: Reserved for future implementation
      * `MIXED`: Reserved for future implementation
    *   The `curvature` parameter (default `-1.0`) controls the properties of non-Euclidean spaces.

5.  **Robust Vector Validation:**
    *   Provides the `_validate_vector` method to detect and handle problematic vectors:
      * Checks for NaN/Inf values and replaces them with zeros
      * Handles different input types (lists, numpy arrays, torch tensors)
      * Tracks warning counts to avoid log spamming

## Key Difference from `embedding_handling.md`

While there is some overlap, the key distinction is:

* **GeometryManager (This Document)**: Focuses on the mathematical/geometric operations on vectors - how they are compared, aligned, normalized, and what geometric space they live in. This is the core component that implements the operations.

* **Embedding Handling (embedding_handling.md)**: Focuses on the overall system approach to embedding processing, including the integration points, validation flow, backward compatibility mechanisms, and how the GeometryManager is utilized throughout the system.

## Recent Implementation Improvements

Recent updates to the dimension handling implementation include:

* Unified approach to vector alignment across the system using the central GeometryManager
* Enhanced handling of dimension mismatches in HPC-QR factor calculations
* Improved validation to handle NaN/Inf values consistently
* Added backward compatibility methods to ensure consistent naming conventions

## Configuration

The behavior of the `GeometryManager` is influenced by the main `SynthiansMemoryCore` configuration:

*   `embedding_dim`: The primary embedding dimension used internally (default: `768`).
*   `geometry_type`: Specifies the default geometric space (default: `'euclidean'`).
*   `alignment_strategy`: How to handle dimension mismatches (default: `'truncate'`).
*   `normalization_enabled`: Whether to normalize vectors during operations (default: `True`).
*   `curvature`: Parameter for non-Euclidean geometries (default: `-1.0`).

## Importance

Centralizing geometric operations in `GeometryManager` ensures:

*   **Consistency:** All parts of the system use the same methods for alignment, normalization, and distance calculation.
*   **Robustness:** Handles potential issues like dimension mismatches gracefully.
*   **Flexibility:** Allows easier adaptation to different embedding types or geometric calculations in the future.

```

# docs\core\metadata.md

```md
# Metadata Synthesis

The `synthians_memory_core.metadata_synthesizer.MetadataSynthesizer` class is responsible for automatically generating and enriching the metadata associated with each `MemoryEntry`.

## Purpose

Metadata provides crucial context about a memory beyond its raw content and embedding. Synthesized metadata helps in:

*   **Enhanced Retrieval:** Filtering or boosting memories based on time, emotion, complexity, etc.
*   **Analysis & Understanding:** Providing insights into the nature and origin of memories.
*   **Scoring:** Contributing factors to the `quickrecal_score` calculation.

## Key Component: `MetadataSynthesizer`

*   **Functionality:** Takes the raw input (content, timestamp, source information, embedding) and generates a dictionary of derived metadata fields.
*   **Integration:** Called by `SynthiansMemoryCore.process_new_memory` after initial processing but before final storage.

## Synthesized Metadata Fields (Examples)

The synthesizer aims to add fields like:

*   **Temporal:**
    *   `timestamp_iso`: Standardized ISO 8601 format.
    *   `time_of_day`: Morning, Afternoon, Evening, Night.
    *   `day_of_week`: Monday, Tuesday, etc.
    *   `month`, `year`.
*   **Emotional (if `EmotionAnalyzer` is used):**
    *   `dominant_emotion`, `sentiment_label`, `sentiment_score`.
*   **Cognitive/Complexity:**
    *   `word_count`, `char_count`.
    *   `complexity_estimate`: A simple measure (e.g., based on sentence length or vocabulary).
*   **Embedding Information:**
    *   `embedding_dim`: Dimension of the stored embedding.
    *   `embedding_norm`: Magnitude of the embedding vector (before/after normalization).
    *   `embedding_provider`: Source of the embedding (e.g., model name).
*   **Identifiers:**
    *   `memory_id`: The unique UUID assigned to the memory entry.
    *   `source`, `user_id`, `session_id`: Preserved if provided in the initial input metadata.

## Configuration

*   The specific metadata fields generated might be influenced by the availability of other components (like the `EmotionAnalyzer`) and potential configuration flags (though currently less configurable than other components).

## Importance

Automated metadata synthesis ensures that memories are consistently tagged with rich contextual information without requiring manual input for every field, significantly enhancing the utility and searchability of the memory core.

```

# docs\core\persistence.md

```md
# Memory Persistence

The `synthians_memory_core.memory_persistence.MemoryPersistence` class handles the saving and loading of memory structures (primarily `MemoryEntry` and `MemoryAssembly` objects) to and from the filesystem.

## Purpose

Persistence ensures that the state of the memory core (memories, assemblies, metadata) survives restarts and shutdowns.

## Key Component: `MemoryPersistence`

*   **Functionality:**
    *   Provides asynchronous methods (`save_memory`, `load_memory`, `delete_memory`, `save_assembly`, `load_assembly`, etc.) to interact with the filesystem.
    *   Typically saves individual `MemoryEntry` objects as separate JSON files within a structured directory (`storage_path/memories/`).
    *   Saves `MemoryAssembly` objects similarly (`storage_path/assemblies/`).
    *   Manages a central index file (`storage_path/memory_index.json`) which maps memory IDs to their file paths and potentially stores lightweight metadata for faster loading or indexing.
    *   Uses `aiofiles` for non-blocking file I/O, crucial for an asynchronous system.
*   **Integration:**
    *   Used by `SynthiansMemoryCore` to save new/updated memories and assemblies.
    *   Used during `SynthiansMemoryCore` initialization to load existing memories and assemblies from disk.
    *   Coordinates with `MemoryVectorIndex` to ensure consistency between saved memories and their vector representations.

## Storage Structure (Example)

\`\`\`
<storage_path>/
├── memory_index.json        # Maps memory_id -> filepath, metadata
├── memories/
│   ├── <memory_uuid_1>.json # Complete MemoryEntry object
│   ├── <memory_uuid_2>.json
│   └── ...
├── assemblies/
│   ├── <assembly_id_1>.json # Complete MemoryAssembly object
│   ├── <assembly_id_2>.json
│   └── ...
└── vector_index/           # Managed by MemoryVectorIndex
    ├── memory_vectors.faiss # FAISS binary index file
    └── mapping.json        # Backup of string_id -> faiss_id mapping
\`\`\`

## Memory Index Structure

The `memory_index.json` file maintains a master record of all memories and their metadata:

\`\`\`json
{
  "memories": {
    "550e8400-e29b-41d4-a716-446655440000": {
      "filepath": "memories/550e8400-e29b-41d4-a716-446655440000.json",
      "created_at": "2025-03-15T14:32:01.123456",
      "updated_at": "2025-03-15T14:45:22.654321",
      "quickrecal_score": 0.85,
      "content_hash": "sha256:a1b2c3..."
    },
    "550e8400-e29b-41d4-a716-446655440001": {
      "filepath": "memories/550e8400-e29b-41d4-a716-446655440001.json",
      "created_at": "2025-03-16T08:12:35.789012",
      "updated_at": "2025-03-16T08:12:35.789012",
      "quickrecal_score": 0.72,
      "content_hash": "sha256:d4e5f6..."
    },
    // Additional memories...
  },
  "assemblies": {
    "assembly_001": {
      "filepath": "assemblies/assembly_001.json",
      "created_at": "2025-03-17T10:24:56.135790",
      "updated_at": "2025-03-18T15:30:42.864209",
      "member_count": 5
    },
    // Additional assemblies...
  },
  "metadata": {
    "version": "2.3.0",
    "last_updated": "2025-03-18T15:30:42.864209",
    "memory_count": 237,
    "assembly_count": 42
  }
}
\`\`\`

## Implementation Details

### 1. Asynchronous Operations

All file operations are implemented asynchronously using `aiofiles` to prevent blocking the main API service:

\`\`\`python
async def save_memory(self, memory: MemoryEntry) -> None:
    """Save a memory to the filesystem asynchronously."""
    file_path = os.path.join(self.memories_path, f"{memory.memory_id}.json")
    memory_dict = memory.dict()
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    # Asynchronously write the memory to a file
    async with aiofiles.open(file_path, mode='w') as f:
        await f.write(json.dumps(memory_dict, indent=2))
    
    # Update the memory index
    await self._update_memory_index(memory)
\`\`\`

### 2. Batch Operations

The persistence layer supports batch operations for improved performance:

\`\`\`python
async def save_memories_batch(self, memories: List[MemoryEntry]) -> None:
    """Save multiple memories efficiently."""
    # Group operations to reduce disk I/O
    tasks = [self._save_memory_file(memory) for memory in memories]
    await asyncio.gather(*tasks)
    
    # Update the index in one operation
    await self._update_memory_index_batch(memories)
\`\`\`

### 3. Error Handling and Recovery

The system implements robust error handling to prevent data loss:

* **Transaction-like Approach**: For critical operations, files are first written to temporary locations, then atomically moved to their final destinations
* **Backup Creation**: Periodic backups of the memory index are maintained
* **Consistency Checks**: When loading memories, the system verifies consistency between the memory index and actual files
* **Auto-Recovery**: Can rebuild the memory index from individual memory files if the index becomes corrupted

\`\`\`python
async def verify_and_repair_consistency(self) -> Dict[str, Any]:
    """Verify consistency between memory index and files, repairing if needed."""
    # Implementation scans files, verifies against index, and repairs inconsistencies
    found_files = await self._scan_memory_files()
    index_entries = await self._load_memory_index()
    
    missing_from_index = [f for f in found_files if f not in index_entries]
    missing_files = [e for e in index_entries if e not in found_files]
    
    # Repair actions
    repair_results = await self._repair_inconsistencies(missing_from_index, missing_files)
    
    return repair_results
\`\`\`

## Integration with Vector Index

The persistence layer works in coordination with the `MemoryVectorIndex` to ensure consistency:

1. **Memory Creation Flow**:
   * Memory is saved to filesystem via `save_memory`
   * Memory embedding is added to vector index via `add_vector`
   * Memory index is updated with metadata

2. **Memory Deletion Flow**:
   * Memory is marked for deletion in the index
   * Memory is removed from vector index via `remove_vector`
   * Memory file is deleted from filesystem

3. **Startup Consistency**:
   * During initialization, the system verifies that memories in the filesystem have corresponding vectors in the FAISS index
   * Mismatches are resolved either by rebuilding missing vector entries or removing orphaned vectors

## Configuration

*   `storage_path`: The root directory for all persistent memory data (default: `./storage`)
*   `index_backup_count`: Number of backup copies to maintain for the memory index (default: `3`)
*   `auto_repair`: Whether to automatically repair inconsistencies during startup (default: `True`)
*   `backup_interval`: Interval in seconds between automatic backups (default: `3600` - 1 hour)
*   `flush_threshold`: Number of memory changes before forcing a flush to disk (default: `20`)

## Performance Considerations

* **Lazy Loading**: By default, the system loads only the memory index at startup, with individual memories loaded on-demand
* **LRU Cache**: Frequently accessed memories are cached in memory for faster access
* **Chunked Processing**: For large memory stores, batch operations are chunked to manage memory usage
* **Optimistic Locking**: Minimal file locking to maximize concurrency, with conflicts resolved through update timestamps

## Failure Handling

* **Disk Full**: If the disk is full, the system attempts to complete critical operations and logs severe warnings
* **Corrupted Files**: JSON parsing errors are handled gracefully, with attempts to recover partial data
* **Permission Issues**: Clear error messages indicate permission problems with helpful resolution steps
* **Storage Migration**: Built-in utilities for safely migrating memory storage to a new location

## Importance

Reliable persistence is fundamental. Without it, the memory core would be volatile, losing all information upon restart. The asynchronous nature ensures that saving/loading operations don't block the main application thread, while the robust error handling and recovery mechanisms protect against data loss.

```

# docs\core\quickrecal.md

```md
# QuickRecall Scoring

QuickRecall (`quickrecal_score`) is a dynamic score assigned to each `MemoryEntry` that estimates its relevance or importance at a given time. It moves beyond simple chronological or similarity-based retrieval.

## Purpose

The score helps prioritize memories during retrieval, ensuring that the most relevant, important, or timely memories surface first, even if they aren't the absolute closest match in embedding space.

## Key Component: `UnifiedQuickRecallCalculator`

*   **Location:** `synthians_memory_core.hpc_quickrecal.UnifiedQuickRecallCalculator` (The "HPC" prefix is historical).
*   **Functionality:** Calculates the `quickrecal_score` based on a combination of weighted factors.
*   **Integration:** Called by `SynthiansMemoryCore.process_new_memory` to assign an initial score and potentially by other processes (like the surprise feedback loop) to update the score.

## Scoring Factors (Examples)

The calculator combines multiple factors, often configurable via weights in the core settings. Common factors include:

*   **Recency:** How recently the memory was created or accessed.
*   **Importance (Explicit/Implicit):** Was the memory marked as important? Does its content suggest importance?
*   **Relevance (Similarity):** How similar is the memory to a current query or context (often incorporated during retrieval ranking rather than the stored score).
*   **Emotional Salience:** Strength or type of emotion associated with the memory.
*   **Surprise/Novelty:** How unexpected or informative the memory was when processed (Boosted via the Neural Memory feedback loop).
*   **Frequency/Access Count:** How often the memory has been retrieved.
*   **Connectivity/Coherence:** How well the memory fits within existing `MemoryAssembly` clusters.
*   **Decay:** A mechanism to gradually reduce the score over time if not accessed or reinforced.

## Surprise Feedback Integration

A key aspect is the integration with the Neural Memory Server:

1.  When the Neural Memory processes an embedding corresponding to a Memory Core entry, it calculates surprise (`loss`, `grad_norm`).
2.  The Context Cascade Engine sends a boost request (`/api/memories/update_quickrecal_score`) to the Memory Core.
3.  The Memory Core uses this signal to increase the `quickrecal_score` of the specific `MemoryEntry`, marking it as significant due to its surprising nature.

## Importance

QuickRecall scoring makes the memory system more dynamic and context-aware, better reflecting how human memory seems to prioritize information based on more than just similarity or time.

```

# docs\core\README.md

```md
# Memory Core Components Documentation

This directory provides detailed documentation on the internal components of the `synthians_memory_core` package.

## Contents

*   [Embedding Handling](./embedding_handling.md): Details on how embeddings are validated, normalized, aligned, and compared using the `GeometryManager`.
*   [Emotional Intelligence](./emotion.md): Describes the `EmotionAnalyzer` for sentiment/emotion detection and the `EmotionalGatingService` for filtering retrieval results.
*   [Geometry Management](./geometry.md): Covers the `GeometryManager`'s role in handling different vector dimensions and geometric spaces (Euclidean, Hyperbolic).
*   [Metadata Synthesis](./metadata.md): Explains how the `MetadataSynthesizer` enriches memories with derived information (temporal, cognitive, etc.).
*   [Persistence](./persistence.md): Details the `MemoryPersistence` class responsible for asynchronously saving/loading memory entries and assemblies.
*   [QuickRecall Scoring](./quickrecal.md): Describes the `UnifiedQuickRecallCalculator` and the factors contributing to a memory's relevance score, including the surprise feedback mechanism.
*   [Vector Index (FAISS)](./vector_index.md): Covers the `MemoryVectorIndex` implementation using `faiss.IndexIDMap` for efficient, ID-keyed vector search, including persistence and integrity checks.

Refer to the main [Architecture](../ARCHITECTURE.md) and [Component Guide](../COMPONENT_GUIDE.md) for a higher-level overview.

```

# docs\core\vector_index.md

```md
# Vector Index (FAISS)

The `synthians_memory_core.vector_index.MemoryVectorIndex` class manages the storage and efficient retrieval of high-dimensional embedding vectors using the FAISS library.

## Purpose

Vector indexing allows for fast approximate nearest neighbor (ANN) searches, enabling the Memory Core to quickly find memories semantically similar to a given query embedding.

## Key Component: `MemoryVectorIndex`

*   **Functionality:**
    *   Wraps a FAISS index object (e.g., `faiss.IndexFlatL2`, `faiss.IndexFlatIP`).
    *   Crucially, uses `faiss.IndexIDMap` to map the user-facing string `memory_id` (UUID) to the internal 64-bit integer IDs required by FAISS. This allows adding and retrieving vectors using the meaningful string IDs.
    *   Handles adding new vectors (`add_vector`), searching for similar vectors (`search`), removing vectors (`remove_vector`), and updating vectors (`update_vector`).
    *   Manages persistence of the FAISS index to disk (`save_index`, `load_index`).
    *   Provides utilities for verifying index integrity (`verify_index_integrity`) and migrating older index formats (`migrate_to_idmap`).
    *   Supports GPU acceleration if configured and available (`_initialize_gpu`).
*   **Integration:** Used extensively by `SynthiansMemoryCore` for storing embeddings associated with memories and performing similarity searches during retrieval.

## FAISS `IndexIDMap`

*   **Requirement:** Standard FAISS indices operate on sequential integer IDs (0, 1, 2...).
*   **Solution:** `IndexIDMap` acts as a layer on top of a base index (like `IndexFlatL2`). It maintains an internal mapping between arbitrary 64-bit integer IDs (which we derive from the string `memory_id`s) and the sequential IDs used by the base index.
*   **Benefit:** Allows using meaningful, potentially non-sequential IDs directly with `add_with_ids` and interpreting the IDs returned by `search`.
*   **GPU Limitation:** ⚠️ **Important:** When using `IndexIDMap`, the `add_with_ids` operation does not support GPU acceleration. The implementation falls back to CPU for these operations, even if the system is configured to use GPU. This is a limitation of the FAISS library itself, not the Synthians implementation. Search operations with `IndexIDMap` can still benefit from GPU acceleration.

## Persistence

*   The FAISS index itself (vectors and the ID map) is saved to a `.faiss` file (e.g., `storage_path/vector_index/memory_vectors.faiss`).
*   A separate `mapping.json` file is often kept as a backup, storing the `string_memory_id -> int64_faiss_id` mapping.

## Configuration

*   `vector_index_path`: Directory to store the index files.
*   `vector_index_type`: The base FAISS index type (e.g., `'IndexFlatL2'`, `'IndexFlatIP'`).
*   `use_gpu`: Boolean flag to enable GPU usage.
*   `embedding_dim`: Must match the dimension of the stored vectors.

## Importance

The vector index is the foundation of memory retrieval by semantic similarity, which is the core functionality of the Memory Core. An efficient, robust, and scalable vector index implementation is essential for overall system performance.

## Failure Handling

*   **Missing Index:** If the index file is not found on disk, a new one is automatically created.
*   **Index Corruption:** Methods like `verify_index_integrity` and `repair_index` can help diagnose and fix index issues.
*   **ID Mapping Loss:** If the mapping between string IDs and FAISS integer IDs is lost, it can potentially be recreated from the `memory_index.json` file using consistent hashing.
*   **GPU Fallback:** If GPU initialization fails, the system automatically falls back to CPU and logs a warning.

## Migration from Legacy Formats

Older versions might have used FAISS indices without `IndexIDMap`, relying on sequential IDs matching the position in some external memory list. The `migrate_to_idmap` method can convert these legacy indices to the more robust `IndexIDMap` format, ensuring each vector has a stable 64-bit ID derived from its string memory UUID.

```

# docs\faiss_gpu_integration.md

```md
# FAISS GPU Integration Guide

## Overview

This document explains how GPU support is integrated with FAISS in the Synthians Memory Core system. The integration enables significant performance improvements for vector similarity searches when GPU hardware is available.

## Implementation Approach

Our implementation follows a robust multi-layered approach to ensure FAISS with GPU acceleration is available whenever possible:

1. **Docker Pre-Installation**: FAISS is installed during container startup based on hardware detection
2. **Dynamic Code Installation**: Fallback auto-installation occurs if the import fails at runtime
3. **Graceful Degradation**: If GPU support isn't available, the system falls back to CPU mode

## Docker Integration

### Container Startup Process

The Docker Compose configuration detects GPU availability and installs the appropriate FAISS package during container initialization:

\`\`\`yaml
command: >
  /bin/bash -c '
  # Pre-install FAISS before Python importing it
  echo "[+] PRE-INSTALLING FAISS FOR MEMORY VECTOR INDEX" &&
  pip install --upgrade pip setuptools wheel &&
  # Install CPU version first as a fallback
  pip install --no-cache-dir faiss-cpu &&
  # If GPU available, replace with GPU version
  if command -v nvidia-smi > /dev/null 2>&1; then
    echo "[+] GPU DETECTED - Installing FAISS-GPU for better performance" &&
    pip uninstall -y faiss-cpu &&
    pip install --no-cache-dir faiss-gpu
  fi &&
  # Verify FAISS installation
  python -c "import faiss; print(f\'[+] FAISS {getattr(faiss, \\\'__version__\\\', \\\'unknown\\\')} pre-installed successfully\')" &&
  ...
\`\`\`

Key aspects of this approach:
- Installs CPU version first as a reliable fallback
- Only replaces with GPU version when hardware is confirmed available
- Verifies installation succeeded before proceeding

## Dynamic Import with Auto-Installation

The `vector_index.py` module implements dynamic FAISS import with automatic installation if the package is missing:

\`\`\`python
# Dynamic FAISS import with auto-installation fallback
try:
    import faiss
except ImportError:
    import sys
    import subprocess
    import logging
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("vector_index")
    
    logger.warning("FAISS not found. Attempting to install...")
    
    # Check for GPU availability
    try:
        gpu_available = False
        try:
            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
            gpu_available = result.returncode == 0
        except:
            pass
            
        # Install appropriate FAISS package
        if gpu_available:
            logger.info("GPU detected, installing FAISS with GPU support")
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', 'faiss-gpu'])
        else:
            logger.info("No GPU detected, installing CPU-only FAISS")
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', 'faiss-cpu'])
            
        # Try importing again
        import faiss
        logger.info(f"Successfully installed and imported FAISS {getattr(faiss, '__version__', 'unknown')}")
    except Exception as e:
        logger.error(f"Failed to install FAISS: {str(e)}")
        raise ImportError("Failed to install FAISS. Please install it manually.")
\`\`\`

This approach provides resilience against:
- Missing dependencies at runtime
- Container rebuilds that might lose installed packages
- Varying hardware configurations

## GPU Utilization in the Vector Index

The `MemoryVectorIndex` class handles runtime GPU utilization:

\`\`\`python
def __init__(self, config=None):
    # ...
    self.is_using_gpu = False
    
    # Move to GPU if available and requested
    if self.config['use_gpu']:
        self._move_to_gpu_if_available()

def _move_to_gpu_if_available(self):
    """Move the index to GPU if available."""
    try:
        # Check if FAISS was built with GPU support
        if hasattr(faiss, 'StandardGpuResources'):
            logger.info("Moving FAISS index to GPU...")
            self.gpu_res = faiss.StandardGpuResources()
            gpu_index = faiss.index_cpu_to_gpu(self.gpu_res, self.config['gpu_id'], self.index)
            self.index = gpu_index
            self.is_using_gpu = True
            logger.info(f"FAISS index successfully moved to GPU {self.config['gpu_id']}")
        else:
            logger.warning("FAISS was not built with GPU support. Using CPU index.")
    except Exception as e:
        logger.error(f"Failed to move index to GPU: {str(e)}. Using CPU index.")
\`\`\`

This implementation:
1. Attempts to move the index to GPU memory when initialized
2. Provides detailed logging about GPU utilization status
3. Falls back gracefully to CPU if GPU transfer fails

## Performance Considerations

### Expected Speedups

Typical performance improvements with GPU acceleration:

| Vector Count | Query Count | CPU Time | GPU Time | Speedup |
|--------------|-------------|----------|----------|--------|
| 10,000       | 100         | 0.087s   | 0.024s   | 3.6x   |
| 100,000      | 100         | 0.830s   | 0.064s   | 13.0x  |
| 1,000,000    | 100         | 8.214s   | 0.356s   | 23.1x  |

*Note: These are approximate values that will vary based on GPU model and vector dimensionality*

### Memory Management

For optimal GPU performance:

- The system sets `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512` to avoid memory fragmentation
- Consider adjusting this value for your specific GPU memory size
- For very large indices, you may need to implement index sharding

## Troubleshooting GPU Support

### Verifying GPU Usage

To verify if FAISS is using GPU acceleration:

\`\`\`python
from synthians_memory_core.vector_index import MemoryVectorIndex

index = MemoryVectorIndex()
print(f"Using GPU: {index.is_using_gpu}")
\`\`\`

### Common GPU Issues

1. **CUDA Version Mismatch**
   - FAISS-GPU requires a specific CUDA version
   - We added `PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu118` to ensure compatible versions

2. **Insufficient GPU Memory**
   - Large indices may exceed GPU memory
   - Solution: Implement index sharding or reduce batch sizes

3. **GPU Not Visible to Docker**
   - Ensure Docker has GPU access: `--runtime=nvidia` and proper device mapping
   - Verify NVIDIA Container Toolkit is properly installed

## Conclusion

This implementation ensures that the Synthians Memory Core system can leverage GPU acceleration for vector similarity searches whenever possible, while gracefully falling back to CPU processing when necessary. The multi-layered approach provides robust operation across different deployment environments.

```

# docs\guides\CONFIGURATION_GUIDE.md

```md
# Synthians Cognitive Architecture: Configuration Guide

**Version:** 1.2
**Date:** March 30, 2025

## 1. Overview

This guide details the configuration parameters for the Synthians Cognitive Architecture, focusing primarily on the Memory Core service which is the central component of the system.

**Core Services:**

1.  **Synthians Memory Core:** Manages persistent memory, retrieval, and scoring.
2.  **Neural Memory Server:** Handles adaptive associative memory and test-time learning. *(Documentation for this service is provided separately)*
3.  **Context Cascade Engine (CCE):** Orchestrates the flow between the Memory Core and Neural Memory. *(Documentation for this service is provided separately)*

## 3. Synthians Memory Core Configuration (`synthians_memory_core`)

These parameters configure the main memory storage and retrieval service, typically controlled via the `config` dictionary passed to the `SynthiansMemoryCore` class constructor and environment variables for the API server.

### 3.1. Core Parameters (`SynthiansMemoryCore` config dict)

| Parameter                       | Type              | Default                               | Description                                                                                                  | Passed To               |
| :------------------------------ | :---------------- | :------------------------------------ | :----------------------------------------------------------------------------------------------------------- | :---------------------- |
| `embedding_dim`                 | int               | 768                                   | **CRITICAL:** Dimension of embeddings used throughout the system. Must match embedding model output.         | All Components          |
| `geometry`                      | str               | "hyperbolic"                          | Geometric space for embedding operations: "euclidean", "hyperbolic", "spherical", or "mixed"               | `GeometryManager`       |
| `hyperbolic_curvature`          | float             | -1.0                                  | Curvature parameter for hyperbolic geometry (`< 0`). Lower magnitude = more curved.                         | `GeometryManager`       |
| `storage_path`                  | str               | "/app/memory/stored/synthians"        | Base path for persistent storage of memories, indices, and assemblies.                                     | `MemoryPersistence`     |
| `vector_index_type`             | str               | "Cosine"                              | Vector similarity metric: "L2" (Euclidean), "IP" (Inner Product), or "Cosine" (normalized inner product).   | `MemoryVectorIndex`     |
| `max_memory_entries`            | int               | 50000                                 | Maximum allowed memory entries before pruning is triggered.                                                | Core                    |
| `prune_threshold_percent`       | float             | 0.9                                   | Percentage of `max_memory_entries` at which pruning is triggered (0.0-1.0).                               | Core                    |
| `min_quickrecal_for_ltm`        | float             | 0.2                                   | Minimum QuickRecal score required to retain a memory after decay (0.0-1.0).                                | Core                    |
| `assembly_threshold`            | float             | 0.75                                  | Minimum similarity threshold for memories to form an assembly (0.0-1.0).                                   | Core                    |
| `max_assemblies_per_memory`     | int               | 3                                     | Maximum number of assemblies a single memory can belong to.                                                | Core                    |
| `adaptive_threshold_enabled`    | bool              | True                                  | Enable adaptive similarity threshold for retrieval based on feedback.                                       | `ThresholdCalibrator`   |
| `initial_retrieval_threshold`   | float             | 0.75                                  | Initial similarity threshold for memory retrieval (0.0-1.0).                                               | `ThresholdCalibrator`   |
| `persistence_interval`          | float             | 60.0                                  | Seconds between automated persistence operations.                                                          | Core                    |
| `decay_interval`                | float             | 3600.0                                | Seconds between automated QuickRecal decay checks.                                                         | Core                    |
| `prune_check_interval`          | float             | 600.0                                 | Seconds between automated memory pruning checks.                                                           | Core                    |
| `persistence_batch_size`        | int               | 100                                   | Number of memories to persist in a single batch.                                                           | Core                    |
| `check_index_on_retrieval`      | bool              | False                                 | Whether to check vector index integrity during retrieval operations.                                        | Core                    |
| `index_check_interval`          | float             | 3600                                  | Seconds between automated vector index verification checks.                                                | Core                    |
| `migrate_to_idmap`              | bool              | True                                  | Whether to migrate older FAISS indices to IndexIDMap format.                                                | `MemoryVectorIndex`     |

### 3.2. Component-Specific Parameters (Passed to Subcomponents)

#### 3.2.1 GeometryManager Parameters

The following parameters are extracted from the main config and passed to the `GeometryManager` constructor:

\`\`\`python
self.geometry_manager = GeometryManager({
    'embedding_dim': self.config['embedding_dim'],
    'geometry_type': self.config['geometry'],
    'curvature': self.config['hyperbolic_curvature']
})
\`\`\`

Additional `GeometryManager` parameters (with their own defaults if not specified):

| Parameter               | Type   | Default      | Description                                                               |
| :---------------------- | :----- | :----------- | :------------------------------------------------------------------------ |
| `alignment_strategy`    | str    | "truncate"   | Strategy for aligning embedding dimensions: "truncate", "pad", or "project" |
| `normalization_enabled` | bool   | True         | Whether to normalize vectors during operations                            |

#### 3.2.2 UnifiedQuickRecallCalculator Parameters

The following parameters are extracted from the main config and passed to the `UnifiedQuickRecallCalculator` constructor:

\`\`\`python
self.quick_recal = UnifiedQuickRecallCalculator({
    'embedding_dim': self.config['embedding_dim'],
    'mode': QuickRecallMode.HPC_QR,  # Default to HPC-QR mode
    'geometry_type': self.config['geometry'],
    'curvature': self.config['hyperbolic_curvature']
}, geometry_manager=self.geometry_manager)
\`\`\`

#### 3.2.3 MemoryVectorIndex Parameters

The following parameters are extracted from the main config and passed to the `MemoryVectorIndex` constructor:

\`\`\`python
self.vector_index = MemoryVectorIndex({
    'embedding_dim': self.config['embedding_dim'],
    'storage_path': os.path.join(self.config['storage_path'], 'index'),
    'index_type': self.config['vector_index_type'],
    'use_gpu': self.config.get('use_gpu_for_index', False)
})
\`\`\`

## 5. Recommended Configurations

### 5.1. Memory Core Production Configuration

\`\`\`python
memory_core_config = {
    'embedding_dim': 768,
    'geometry': 'hyperbolic',  # Using hyperbolic geometry for better representation
    'storage_path': '/persistent/data/memory_storage',
    'vector_index_type': 'Cosine',  # Cosine similarity (normalized inner product)
    'max_memory_entries': 100000,  # Larger memory capacity
    'prune_threshold_percent': 0.95,  # Trigger pruning at 95% capacity
    'min_quickrecal_for_ltm': 0.25,  # Higher bar for long-term retention
    'persistence_interval': 30.0,  # More frequent saves
    'adaptive_threshold_enabled': True,
    'initial_retrieval_threshold': 0.72,  # Slightly more permissive initial threshold
    'use_gpu_for_index': True  # Use GPU acceleration if available
}
\`\`\`

## Important Notes on Parameter Inheritance

1. **Embedding Dimension:** The `embedding_dim` parameter is particularly critical as it's passed to multiple components and must match the output dimension of your embedding model. If you're using a pre-trained model like `all-MiniLM-L6-v2` (384D) or `all-mpnet-base-v2` (768D), ensure this parameter matches exactly.

2. **Geometry Settings:** The `geometry` and `hyperbolic_curvature` parameters are passed to both the `GeometryManager` and `UnifiedQuickRecallCalculator`. If you want to override geometry settings for only one component, you'll need to initialize that component directly rather than relying on the SynthiansMemoryCore to do it for you.

3. **Storage Paths:** The base `storage_path` is used to derive component-specific paths:
   - Vector index: `{storage_path}/index/`
   - Memory files: `{storage_path}/memories/`
   - Memory index: `{storage_path}/memory_index.json`
   - Assemblies: `{storage_path}/assemblies/`

```

# docs\guides\implementation_guide.md

```md
# Bi-Hemispheric Cognitive Architecture: Implementation Guide

## Introduction

This technical guide explains how to implement and integrate the components of the Bi-Hemispheric Cognitive Architecture. It covers deployment, configuration, and development patterns to extend the system.

## System Requirements

- Docker and Docker Compose
- Python 3.9+
- CUDA-compatible GPU (optional, for accelerated embedding generation)
- 8GB+ RAM 

## Component Deployment

### Using Docker Compose

The easiest way to deploy the full architecture is using the included `docker-compose-bihemispheric.yml` file:

\`\`\`bash
docker-compose -f docker-compose-bihemispheric.yml up -d
\`\`\`

This launches all three components (Memory Core, Trainer Server, and Context Cascade Engine) with proper networking and configuration.

### Manual Deployment

To run components individually (useful for development):

1. **Memory Core**
   \`\`\`bash
   cd synthians_memory_core
   python -m server.main
   \`\`\`

2. **Trainer Server**
   \`\`\`bash
   cd synthians_memory_core/synthians_trainer_server
   python -m http_server
   \`\`\`

3. **Context Cascade Engine**
   \`\`\`bash
   cd synthians_memory_core/orchestrator
   python -m server
   \`\`\`

## Configuration

### Environment Variables

The architecture uses the following environment variables (can be set in Docker Compose or locally):

\`\`\`
# Memory Core
PORT=8000
VECTOR_DB_PATH=./vectordb
MEMORY_STORE_PATH=./memorystore
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIM=768
GEOMETRY_TYPE=euclidean
ALIGNMENT_STRATEGY=truncate
VECTOR_INDEX_TYPE=L2
RETRIEVAL_THRESHOLD=0.3

# Trainer Server
PORT=8001
MEMORY_CORE_URL=http://memory_core:8000
INPUT_DIM=768
HIDDEN_DIM=256
OUTPUT_DIM=768
MEMORY_DIM=128
LEARNING_RATE=0.001

# Context Cascade Engine
PORT=8002
MEMORY_CORE_URL=http://memory_core:8000
TRAINER_URL=http://trainer:8001
\`\`\`

## Component Integration

### GeometryManager

The `GeometryManager` is a central utility class shared across components to ensure consistent handling of embeddings:

\`\`\`python
from synthians_memory_core.geometry_manager import GeometryManager

# Create a shared instance with default configuration
geometry_manager = GeometryManager({
    'embedding_dim': 768,
    'geometry_type': 'euclidean',
    'alignment_strategy': 'truncate'
})

# Use for vector operations
normalized = geometry_manager.normalize_embedding(embedding)
similarity = geometry_manager.calculate_similarity(vec1, vec2)
aligned_a, aligned_b = geometry_manager.align_vectors(vec1, vec2)
\`\`\`

### Vector Index Management

The `MemoryVectorIndex` handles storage and retrieval of embedding vectors using FAISS:

\`\`\`python
from synthians_memory_core.vector_index import MemoryVectorIndex

# Initialize with configuration
index = MemoryVectorIndex({
    'embedding_dim': 768,
    'index_type': 'L2',
    'vector_index_path': './storage/vector_index',
    'use_gpu': False  # Set to True for GPU acceleration where available
})

# Add vectors
index.add_vector('memory_123', embedding)

# Search for similar vectors
results = index.search(query_embedding, k=10)

# Save and load
index.save_index()
index.load_index()
\`\`\`

### Metadata Enrichment

The `MetadataSynthesizer` enriches memory metadata with various properties:

\`\`\`python
from synthians_memory_core.metadata_synthesizer import MetadataSynthesizer

# Initialize the synthesizer
metadata_synthesizer = MetadataSynthesizer()

# Enrich a memory's metadata
enriched_metadata = metadata_synthesizer.synthesize_metadata(
    content="Sample memory content",
    embedding=embedding,
    existing_metadata={}
)

# The enriched metadata includes:
# - timestamp_iso, time_of_day, day_of_week
# - complexity_estimate, word_count
# - embedding_dim, embedding_norm
# - uuid (memory_id)
# - content_length
\`\`\`

## Robust Error Handling

### Embedding Validation

All embeddings are validated to detect and handle invalid values:

\`\`\`python
def _validate_embedding(embedding, allow_zero=True):
    """Validate that an embedding vector contains only valid values."""
    if embedding is None:
        return False
        
    # Convert to numpy array if needed
    if not isinstance(embedding, np.ndarray):
        embedding = np.array(embedding, dtype=np.float32)
        
    # Check for NaN or Inf values
    if np.isnan(embedding).any() or np.isinf(embedding).any():
        return False
        
    # Optionally check for zero vectors
    if not allow_zero and np.all(embedding == 0):
        return False
        
    return True
\`\`\`

### Dimension Mismatch Handling

The system automatically handles embeddings of different dimensions (e.g., 384 vs. 768):

\`\`\`python
# In Memory Core API handlers
async def retrieve_memories(request_data):
    # Extract query embedding
    query_embedding = request_data.get('query_embedding')
    
    # The system will handle dimension mismatches automatically
    # If the query is 384D but the system uses 768D, alignment happens transparently
    memories = await memory_core.retrieve_memories_by_vector(
        query_embedding=query_embedding,
        limit=request_data.get('limit', 10),
        threshold=request_data.get('threshold', 0.3)  # Explicit threshold parameter
    )
    
    return memories
\`\`\`

## Performance Optimization

### Memory Retrieval Enhancements

The system includes several optimizations for memory retrieval:

1. **Lower Default Threshold**: The default similarity threshold has been reduced from 0.5 to 0.3 for better recall sensitivity
2. **Client-Controlled Thresholds**: API endpoints accept an explicit `threshold` parameter for fine-tuning retrieval sensitivity
3. **Enhanced Logging**: The system provides detailed similarity score logging for debugging
4. **Two-Stage Retrieval**: First uses vector similarity search, then applies additional filters as needed

### Emotion Analysis Optimization

The system performs emotion analysis efficiently:

1. **Respects Provided Emotions**: If emotions are already provided in the input, no redundant analysis is performed
2. **On-Demand Processing**: Emotion analysis only runs when actually needed
3. **Caching**: Results are cached to avoid repeated analysis of the same content

## Deployment Example

Example Docker Compose configuration:

\`\`\`yaml
services:
  memory_core:
    build:
      context: ./synthians_memory_core
    ports:
      - "8000:8000"
    volumes:
      - ./storage:/app/storage
    environment:
      - PORT=8000
      - VECTOR_DB_PATH=/app/storage/vectordb
      - MEMORY_STORE_PATH=/app/storage/memorystore
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      - EMBEDDING_DIM=768
      - GEOMETRY_TYPE=euclidean
      - ALIGNMENT_STRATEGY=truncate
      - RETRIEVAL_THRESHOLD=0.3

  trainer:
    build:
      context: ./synthians_memory_core/synthians_trainer_server
    ports:
      - "8001:8001"
    environment:
      - PORT=8001
      - MEMORY_CORE_URL=http://memory_core:8000
      - INPUT_DIM=768
      - HIDDEN_DIM=256
      - OUTPUT_DIM=768
      - MEMORY_DIM=128

  orchestrator:
    build:
      context: ./synthians_memory_core/orchestrator
    ports:
      - "8002:8002"
    environment:
      - PORT=8002
      - MEMORY_CORE_URL=http://memory_core:8000
      - TRAINER_URL=http://trainer:8001
    depends_on:
      - memory_core
      - trainer
\`\`\`

## GPU Acceleration Notes

1. **FAISS GPU Support**: The Memory Core can utilize GPU acceleration for vector similarity search
   * Set `USE_GPU=true` in the environment variables
   * Note the limitation with `IndexIDMap` operations: adding vectors with custom IDs doesn't benefit from GPU acceleration, though search operations still do

2. **Embedding Generation**: If using a local embedding model, GPU acceleration can provide significant performance benefits
   * Requires a CUDA-compatible GPU
   * Set `USE_GPU=true` for the embedding service

```

# docs\guides\README.md

```md
# Guides & Configuration Documentation

This directory contains guides and configuration documentation for the Synthians cognitive system.

## Contents

* [Configuration Guide](./CONFIGURATION_GUIDE.md): Explains key configuration parameters for the Memory Core and Neural Memory servers, often managed via environment variables or configuration files.
* [Implementation Guide](./implementation_guide.md): Provides deeper insights into the system's setup, including dependencies, running the services (e.g., using Docker Compose), and potential extension points.
* [Tooling Guide](./tooling_guide.md): Describes available utilities and scripts for maintenance, diagnostics, and repair tasks, such as index verification or data migration.

## User Guides

This directory contains practical guides for interacting with and utilizing the Synthians Memory Core API.

## Available Guides

*   [Client Usage Guide](./client_usage.md): Detailed instructions on how to use the `SynthiansClient` library to interact with the Memory Core API, including initialization, core operations (processing memories, retrieval), asynchronous context management, and basic examples.
*   [Error Handling Guide](./error_handling.md): Best practices for handling potential errors when interacting with the API, covering common HTTP status codes, API error responses, and client-side exception handling.
*   [Adaptive Threshold Feedback Loop Guide](./feedback_loop.md): Explanation of how to provide feedback on the relevance of retrieved memories using the `provide_feedback` method to help the system adapt its retrieval threshold.

Refer to the main [API Reference](../API_REFERENCE.md) for detailed endpoint specifications.

## Technical Details

* **Environment Variables**: How environment variables control service behavior, including model selection, embedding dimensions, logging levels, and variant selection.
* **Configuration Dictionaries**: How the services can be configured programmatically via configuration dictionaries passed to their constructors.
* **Service Integration**: How to set up and integrate the three core services (Memory Core, Neural Memory Server, Context Cascade Engine).
* **Deployment Options**: Different deployment configurations (local development, production, GPU vs CPU).
* **Maintenance Procedures**: Guidelines for backing up memory data, monitoring system health, and troubleshooting common issues.

```

# docs\guides\tooling_guide.md

```md
# System Tooling Guide

The Synthians Memory Core system includes several utility scripts and potential API endpoints designed for maintenance, diagnostics, and repair.

## Purpose

These tools help ensure the integrity, consistency, and performance of the memory system, especially the persistent components like the vector index and memory storage.

## Available Tools & Utilities

*(Note: The exact implementation and availability might vary. This describes common utilities found in such systems.)*

### 1. Vector Index Verification (`MemoryVectorIndex.verify_index_integrity`)

*   **Location:** Method within `synthians_memory_core.vector_index.MemoryVectorIndex`.
*   **Functionality:**
    *   Checks consistency between the FAISS index (`.faiss` file) and the string ID-to-int64 ID mapping (often stored in `mapping.json` or derived from `memory_index.json`).
    *   Ensures that every vector in the FAISS index corresponds to a known `memory_id` and vice versa.
    *   Detects orphaned vectors (in FAISS but not mapped) or orphaned mappings (mapped but not in FAISS).
*   **Usage:** Typically called internally during index loading or can be exposed via a maintenance script or API endpoint (e.g., `/admin/verify_index`).

### 2. Index ID Mapping Reconstruction

*   **Location:** Potentially a standalone script (`scripts/rebuild_faiss_mapping.py`) or part of the verification process.
*   **Functionality:** If the `mapping.json` (string ID -> int64 ID) is lost or corrupted, this tool can attempt to rebuild it by:
    1.  Loading the main `memory_index.json` (which maps string ID -> memory file path).
    2.  Assuming a consistent hashing function (`_get_int64_id_from_string`) was used to generate the int64 IDs initially.
    3.  Re-generating the int64 ID for each string ID found in `memory_index.json`.
*   **Usage:** Used in recovery scenarios when the primary FAISS ID map is suspect.

### 3. Memory Index Reconstruction (`MemoryPersistence.reconstruct_index_from_files`)

*   **Location:** Method within `synthians_memory_core.memory_persistence.MemoryPersistence`.
*   **Functionality:** If the main `memory_index.json` is corrupted or lost, this tool scans the `storage_path/memories/` directory:
    1.  Loads each `<uuid>.json` file.
    2.  Extracts key metadata (like timestamp, `quickrecal_score`).
    3.  Rebuilds the `memory_index.json` file from the contents of the individual memory files.
*   **Usage:** Recovery scenario for the primary memory index.

### 4. FAISS Index Migration (`MemoryVectorIndex.migrate_to_idmap`)

*   **Location:** Method within `synthians_memory_core.vector_index.MemoryVectorIndex`.
*   **Functionality:** Handles the migration of older FAISS index formats (that might not have used `IndexIDMap`) to the current format using `IndexIDMap`. Ensures compatibility with systems using string-based memory IDs.
*   **Usage:** Run once during system upgrades if the index format changes.

### 5. Diagnostic API Endpoints

*   **Location:** Exposed via the FastAPI applications (Memory Core or Trainer).
*   **Functionality:**
    *   `/status`, `/health`: Basic health checks.
    *   `/metrics`: Operational metrics (see `docs/trainer/metrics_store.md`).
    *   `/config`: (Potentially) Shows the current runtime configuration.
    *   `/admin/...`: Administrative endpoints for triggering verification, backup, etc. (Ensure these are properly secured).
*   **Usage:** Monitoring, debugging, and remote administration.

### 6. Backup & Restore Scripts

*   **Location:** Standalone scripts (`scripts/backup.sh`, `scripts/restore.sh`) or integrated into deployment processes.
*   **Functionality:** Automates the process of creating consistent backups of the persistent storage (`storage_path`), including memory files, index files, and FAISS data. Provides a mechanism to restore from a backup.
*   **Usage:** Disaster recovery and data safety.

## Best Practices

*   Regularly run verification checks, especially after potentially disruptive events.
*   Implement automated backups of the persistent storage directory.
*   Secure administrative endpoints appropriately.

```

# docs\integration_fixes.md

```md
# Integration Fixes - Lucidia Memory System

*Last Updated: March 29, 2025*

## Overview

This document details critical integration fixes implemented to ensure seamless communication between the Memory Core, Neural Memory module, and Context Cascade Engine components of the Lucidia bi-hemispheric memory system.

## Latest Critical Fixes (March 29, 2025)

### 1. Deep Metadata Merging in Memory Updates

**Issues Fixed:**
- Nested metadata dictionaries were being overwritten rather than merged during updates
- Metadata fields like timestamps and source information were lost during updates
- Test failures occurred in `test_update_metadata`, `test_update_persistence`, and `test_quickrecal_updated_timestamp`

**Solution:**
- Enhanced the `_deep_update_dict` method with improved dictionary merging:
  \`\`\`python
  def _deep_update_dict(self, d: Dict, u: Dict) -> Dict:
      """
      Recursively update a dictionary with another dictionary
      This handles nested dictionaries properly
      """
      for k, v in u.items():
          if isinstance(v, dict) and k in d and isinstance(d[k], dict):
              # Only recursively merge if both the source and update have dict values
              d[k] = self._deep_update_dict(d[k], v)
          else:
              d[k] = v
      return d
  \`\`\`

- Restructured the `update_memory` method to handle metadata updates separately:
  \`\`\`python
  # Store metadata update separately to apply after all direct attributes
  metadata_to_update = None
  
  # Update the memory fields
  for key, value in updates.items():
      if key == "metadata" and isinstance(value, dict):
          # Store metadata updates to apply them after direct attribute updates
          metadata_to_update = value
          continue
      
      # Process other attributes...
  
  # Apply metadata updates after other fields have been processed
  if metadata_to_update:
      if memory.metadata is None:
          memory.metadata = {}
      # Use deep update to properly handle nested dictionaries
      self._deep_update_dict(memory.metadata, metadata_to_update)
  \`\`\`

- Fixed Vector Index update method:
  \`\`\`python
  try:
      self.vector_index.update_entry(memory_id, memory.embedding)
  except AttributeError:
      # Handle case where update_entry doesn't exist (use remove/add pattern)
      self.vector_index.add(memory_id, memory.embedding)
  \`\`\`

**Benefits:**
- Preserves existing metadata structures when updating nested dictionaries
- Ensures timestamp and source information persist across updates
- Improves robustness of the memory persistence system
- See the detailed [metadata_handling.md](./metadata_handling.md) document for more information

### 2. Memory ID Retrieval and Update

**Issues Fixed:**
- Missing `get_memory_by_id` method in SynthiansMemoryCore prevented updating quickrecal scores
- Missing `update_memory` method in SynthiansMemoryCore blocked surprise-based memory boosting

**Solution:**
- Implemented `get_memory_by_id` in SynthiansMemoryCore:
  \`\`\`python
  async def get_memory_by_id(self, memory_id: str) -> Optional[MemoryEntry]:
      async with self._lock:
          return self._memories.get(memory_id, None)
  \`\`\`

- Implemented `update_memory` in SynthiansMemoryCore:
  \`\`\`python
  async def update_memory(self, memory_id: str, updates: Dict[str, Any]) -> bool:
      async with self._lock:
          # Get the memory
          memory = self._memories.get(memory_id)
          if not memory:
              return False
              
          # Update memory fields
          for key, value in updates.items():
              if hasattr(memory, key):
                  setattr(memory, key, value)
              # Special handling for metadata
              elif key == "metadata" and isinstance(value, dict):
                  if memory.metadata is None:
                      memory.metadata = {}
                  memory.metadata.update(value)
          
          # Update quickrecal timestamp if score changed
          if "quickrecal_score" in updates:
              memory.quickrecal_updated = datetime.utcnow()
          
          # Update vector index if necessary
          if memory.embedding is not None and memory_id in self.vector_index.id_to_index:
              self.vector_index.update_entry(memory_id, memory.embedding)
          
          # Schedule persistence update
          await self.persistence.save_memory(memory)
          return True
  \`\`\`

### 3. Neural Memory Dimension Mismatch

**Issues Fixed:**
- Configuration error: `query_dim` (768) not matching `key_dim` (128) in Neural Memory module
- Memory retrieval failing with "Input dimension mismatch" errors

**Solution:**
- Enhanced dimension validation in Neural Memory `call` method:
  \`\`\`python
  # Config sanity check - key_dim and query_dim should match
  if self.config['query_dim'] != self.config['key_dim']:
      logger.error(f"CONFIG ERROR: query_dim ({self.config['query_dim']}) != key_dim ({self.config['key_dim']})")
      # Use key_dim as the source of truth for validation
      expected_dim = self.config['key_dim']
      logger.warning(f"Using key_dim={expected_dim} as expected dimension for memory_mlp input")
  else:
      expected_dim = self.config['key_dim']
  \`\`\`

- Implemented adaptive projection handling in the retrieval endpoint:
  \`\`\`python
  # Check for dimension mismatch in configuration
  if nm.config['query_dim'] != nm.config['key_dim']:
      logger.warning(f"Configuration error detected! Using key projection instead")
      # Use k_t which is already at key_dim (128) dimensionality
      input_tensor = k_t
  else:
      # Configuration is correct, use q_t as intended
      input_tensor = q_t
          
  # Use the properly dimensioned tensor for memory retrieval
  retrieved_tensor = nm(input_tensor, training=False)
  \`\`\`

### 4. Cognitive Cascade Integration

**Issues Fixed:**
- Context Cascade Engine wasn't properly passing raw embeddings to Neural Memory module
- Surprise feedback loop was broken, preventing quickrecal score boosts

**Solution:**
- Updated query generation in Context Cascade Engine to pass raw embedding:
  \`\`\`python
  # Use actual_embedding as the query for Neural Memory retrieval
  query_for_retrieve = actual_embedding
  \`\`\`

- Fixed surprise feedback path through TrainerIntegrationManager:
  \`\`\`python
  async def update_quickrecal_score(self, request: UpdateQuickrecalScoreRequest) -> UpdateQuickrecalScoreResponse:
      memory_id = request.memory_id
      surprise_value = request.surprise_value
      grad_norm = request.grad_norm
      
      # Retrieve the memory by ID
      memory = await self.memory_core.get_memory_by_id(memory_id)
      
      if not memory:
          logger.error(f"Memory {memory_id} not found for quickrecal update")
          return UpdateQuickrecalScoreResponse(status="error", message=f"Memory {memory_id} not found")
      
      # Calculate QuickRecal boost based on surprise metrics
      boost = self._calculate_boost(surprise_value, grad_norm)
      
      # Update the memory's quickrecal score
      new_quickrecal = min(1.0, memory.quickrecal_score + boost)
      
      # Apply the update to the memory
      update_success = await self.memory_core.update_memory(memory_id, {"quickrecal_score": new_quickrecal})
      
      if update_success:
          return UpdateQuickrecalScoreResponse(status="success", 
                                              old_score=memory.quickrecal_score,
                                              new_score=new_quickrecal,
                                              boost_applied=boost)
      else:
          return UpdateQuickrecalScoreResponse(status="error", message="Failed to update memory")
  \`\`\`

## Results

The full cognitive cycle is now operational, with:

1. Memory ingestion and embedding storage working correctly
2. Neural memory test-time learning capturing associations
3. Surprise detection feeding back into the memory system
4. QuickRecal scores being dynamically updated based on cognitive significance
5. Emotional and relevance-based memory retrieval functioning properly

These fixes have resulted in:
- Reduced processing time (from ~4900ms to ~650ms)
- Stable cognitive diagnostics
- Complete end-to-end memory processing and retrieval

## Previous Component Compatibility Fixes

### 1. GeometryManager Method Naming Consistency

**Issues Fixed:**
- Method naming inconsistencies between different components calling GeometryManager methods
- Some components used underscore-prefixed method names (`_align_vectors`, `_normalize`) while the GeometryManager implemented non-underscore versions (`align_vectors`, `normalize_embedding`)

**Solution:**
- Added backward compatibility methods in `geometry_manager.py`:
  \`\`\`python
  def _align_vectors(self, v1: np.ndarray, v2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
      """Backward compatibility method that forwards to align_vectors."""
      return self.align_vectors(v1, v2)

  def _normalize(self, vector: np.ndarray) -> np.ndarray:
      """Backward compatibility method that forwards to normalize_embedding."""
      # Ensure vector is numpy array before calling
      validated_vector = self._validate_vector(vector, "Vector for _normalize")
      if validated_vector is None:
          # Return zero vector if validation fails
          return np.zeros(self.config.get('embedding_dim', 768), dtype=np.float32)
      return self.normalize_embedding(validated_vector)
  \`\`\`

### 2. API Response Enhancements

**Issues Fixed:**
- The `ProcessMemoryResponse` model was missing an `embedding` field expected by the ContextCascadeEngine
- This caused errors when the CCE attempted to access the embedding after calling Memory Core

**Solution:**
- Updated the `ProcessMemoryResponse` model in `api/server.py`:
  \`\`\`python
  class ProcessMemoryResponse(BaseModel):
      success: bool
      memory_id: Optional[str] = None
      quickrecal_score: Optional[float] = None
      embedding: Optional[List[float]] = None  # Added this field
      metadata: Optional[Dict[str, Any]] = None
  \`\`\`
- Modified the response construction to include the embedding in the JSON response

### 3. Configuration Parameter Consistency

**Issues Fixed:**
- `TrainerIntegrationManager` was initializing `GeometryManager` with incorrect parameters
- Explicit parameters (`target_dim`, `max_warnings`) were used instead of the expected configuration dictionary

**Solution:**
- Modified the initialization in `trainer_integration.py`:
  \`\`\`python
  # Before:
  self.geometry_manager = GeometryManager(target_dim=768, max_warnings=10)
  
  # After:
  self.geometry_manager = GeometryManager({
      'embedding_dim': self.memory_core.config.get('embedding_dim', 768),
      'max_warnings': 10
  })
  \`\`\`

## Neural Memory Module Enhancements

### 1. Auto-Initialization

**Issues Fixed:**
- Neural Memory server required explicit initialization via `/init` endpoint
- Context Cascade Engine did not automatically initialize it

**Solution:**
- Added startup auto-initialization in `http_server.py`:
  \`\`\`python
  @app.on_event("startup")
  async def startup_event():
      global neural_memory, memory_core_url, surprise_detector, geometry_manager
      
      # Auto-initialization logic
      try:
          default_config_dict = {
              'input_dim': 768,
              'query_dim': 768,
              'hidden_dim': 768,
              'output_dim': 768
          }
          # Create default config and initialize module
          config = NeuralMemoryConfig(**default_config_dict)
          neural_memory = NeuralMemoryModule(config=config)
          # Initialize dependent components
          geometry_manager = GeometryManager({'embedding_dim': neural_memory.config['input_dim']})
          # ...
      except Exception as e:
          logger.error(f"Auto-initialization failed: {e}")
  \`\`\`

### 2. TensorFlow GradientTape Optimization

**Issues Fixed:**
- `ValueError` in Neural Memory's `update_step` method
- Error related to explicitly watching `tf.Variable` objects in GradientTape

**Solution:**
- Removed unnecessary `tape.watch(var)` calls:
  \`\`\`python
  # Before:
  with tf.GradientTape() as tape:
      # Explicitly watch all inner variables
      for var in inner_vars:
          tape.watch(var)  # Unnecessary and potentially problematic
  
  # After:
  with tf.GradientTape() as tape:
      # Tape automatically watches trainable variables
      # No explicit watch calls needed
  \`\`\`

### 3. Vector Dimension Alignment

**Issues Fixed:**
- Dimension mismatch between Memory Core (768D) and Neural Memory (input_dim vs query_dim)
- `/retrieve` endpoint passing raw query instead of properly projected query

**Solution:**
- Updated `/retrieve` endpoint in `http_server.py` to use projections:
  \`\`\`python
  # Get projected query vector
  k_t, v_t, q_t = nm.get_projections(query_tensor)
  
  # Use projected query for retrieval
  retrieved_tensor = nm(q_t, training=False)
  \`\`\`
- Configured Neural Memory with matching dimensions

## Context Cascade Engine Fixes

### 1. String Formatting Error

**Issues Fixed:**
- String formatting error in `/process_memory` endpoint
- Invalid f-string format when generating feedback message

**Solution:**
- Fixed string formatting in `context_cascade_engine.py`:
  \`\`\`python
  # Before:
  f"NM Surprise (Loss:{loss:.4f if loss is not None else 'N/A'}, ...)"
  
  # After:
  loss_str = f"{loss:.4f}" if isinstance(loss, (int, float)) else 'N/A'
  f"NM Surprise (Loss:{loss_str}, ...)"
  \`\`\`

## End-to-End Testing

After implementing these fixes, we successfully validated the end-to-end flow using the `lucidia_think_trace.py` tool. The tool now successfully:

1. Stores memory in Memory Core
2. Returns memory with embedding to Context Cascade Engine
3. Updates Neural Memory with the new memory
4. Calculates surprise and applies QuickRecal boost
5. Retrieves associated memories via Neural Memory
6. Completes the full cognitive trace

## Next Steps

1. **Refine Surprise-to-Boost Logic:** The current implementation uses a simple mapping from surprise to boost; this could be enhanced with more sophisticated algorithms.

2. **Implement Real Diagnostics:** The Neural Memory server should expose more detailed diagnostic information about its internal state.

3. **Optimize Vector Dimension Handling:** Consider implementing more efficient dimension handling to avoid repeated conversions.

4. **Enhance Error Handling:** Add more comprehensive error handling and recovery mechanisms.

5. **Integration Testing:** Add automated tests for the complete memory system pipeline.

```

# docs\memory_system_robustness.md

```md
# Memory System Robustness Enhancements

## Overview

This document describes the robustness enhancements implemented in the Synthians Memory Core system, focusing on the integration of these improvements with the broader Lucidia Cognitive System architecture.

## Context: Lucidia's Memory Principles

The Lucidia Cognitive System is built on several key memory principles:

1. **Memory is weighted, not just chronological** (QuickRecal)
2. **Emotion shapes recall** (Emotional Gating)
3. **Surprise signals significance** (Neural Memory Loss/Grad → QuickRecal Boost)
4. **Ideas cluster and connect** (Assemblies)
5. **Presence emerges from adaptive memory** (Neural Memory test-time learning)

These principles depend on a reliable and consistent memory retrieval system. The improvements described in this document ensure that the core vector index - which powers similarity-based memory retrieval - maintains its integrity under various operational conditions.

## Architecture Integration

### Memory Flow and Index Role

In the Lucidia architecture, the memory flow follows this path:

\`\`\`
Input (Content/Embedding) → Enrich Metadata → Calculate QuickRecal → Store Entry → Index Embedding (FAISS)
\`\`\`

The FAISS vector index is the cornerstone of this architecture, enabling:

1. Efficient similarity search across thousands of memories
2. Association of memory IDs with their vector representations
3. Support for both Euclidean and Hyperbolic geometry spaces

### Key Dependencies

These index improvements maintain compatibility with other system components:

1. **GeometryManager**: Vector normalization and geometric calculations
2. **EmotionalGatingService**: Filtering/re-ranking based on emotional states
3. **ThresholdCalibrator**: Dynamic adjustment of similarity thresholds

## Implementation Highlights

### IndexIDMap Migration

The system now ensures all indices use FAISS's `IndexIDMap` wrapper for better ID management:

1. Automatically detects legacy indices during initialization
2. Safely migrates vectors while preserving ID associations
3. Handles edge cases like orphaned vectors through multiple extraction strategies

### Orphaned Vector Recovery

A particularly important enhancement addresses the case of "orphaned vectors" - vectors in the index that have lost their memory ID mappings:

1. Sequential extraction reconstructs vectors from the index
2. Memory file scanning attempts to recover original memory IDs
3. If original IDs can't be recovered, synthetic IDs are generated

### Automatic Repair System

The automatic repair system integrates with the core initialization process:

1. Performs integrity verification during startup
2. Selects the appropriate repair strategy based on diagnostics
3. Tracks repair success and provides detailed feedback

## Implications for Future Development

### Memory Reliability

These enhancements provide a robust foundation for future memory system capabilities:

1. **Emotional Gating**: More reliable retrieval ensures emotional context is preserved
2. **Dynamic Assemblies**: Stable index supports consistent assembly formation and update
3. **Neural Memory Integration**: Consistent vectors improve associative mapping quality

### Enabling Advanced Features

With a reliable index foundation, several advanced features become practical:

1. **Multi-dimensional filtering**: Filter memories based on multiple metadata attributes
2. **Time-based decay**: Implement sophisticated memory decay models
3. **Dynamic threshold adaptation**: Adjust retrieval thresholds based on context

## Conclusion

The implemented index repair and maintenance features significantly enhance the robustness of the memory system. By ensuring index-mapping consistency, the system now gracefully handles edge cases that previously led to data loss or retrieval failures.

These improvements align with Lucidia's core principle that "*the blueprint remembers*" - maintaining the integrity of the memory foundation that powers the cognitive system's associative capabilities.

```

# docs\NEWEST-DOCUMENTATION.md

```md
This won't just be documentation; it will be the **living specification for Lucidia's cognitive core.**

---

## Development Roadmap & Status (March 28, 2025)

**Project:** Synthians Cognitive Architecture (Lucidia)  
**Focus:** Bi-Hemispheric Memory System (Memory Core + Neural Memory)  
**Status:** Full Cognitive Cycle Operational

**Overall Goal:** Implement a robust, unified memory system enabling adaptive, long-context cognition inspired by human memory and the Titans paper. Create the infrastructure for a persistent, learning cognitive presence (Lucidia).

---

### Phase 1: Memory Core Unification & Foundation (Completed)

*   **Objective:** Consolidate core memory storage, retrieval, and relevance scoring.
*   **Status:** **DONE**
*   **Key Outcomes:**
    *   Unified `synthians_memory_core` package created.
    *   Components integrated: `SynthiansMemoryCore`, `UnifiedQuickRecallCalculator`, `GeometryManager`, `EmotionalAnalyzer/GatingService`, `MemoryPersistence`, `MemoryAssembly`, `ThresholdCalibrator`, `MetadataSynthesizer`.
    *   Robust FAISS `VectorIndex` implemented with GPU support and persistence.
    *   Core API server (`api/server.py`) established for Memory Core functions.
    *   Basic end-to-end memory lifecycle tested (Store, Retrieve, Feedback).
    *   Initial documentation drafted for core components.

---

### Phase 2: Neural Memory Module Implementation (Completed)

*   **Objective:** Replace the previous predictive trainer with the Titans-inspired `NeuralMemoryModule` capable of test-time learning.
*   **Status:** **DONE**
*   **Key Outcomes:**
    *   TensorFlow implementation of the Titans Neural Memory created.
    *   Test-time gradient updates with momentum state implemented.
    *   Projections (`WK`, `WV`, `WQ`) for geometric transformations.
    *   Adaptive gating mechanisms for learning rate control.
    *   Initial API server (`synthians_trainer_server/http_server.py`) established.
    *   Memory update and retrieval testing completed.
    *   Key dimension handling and projection fixed.

---

### Phase 3: Context Cascade Engine / Orchestration (Completed)

*   **Objective:** Connect Memory Core with Neural Memory to create a bi-directional cognitive loop.
*   **Status:** **DONE** 
*   **Key Outcomes:**
    *   `ContextCascadeEngine` implemented to orchestrate memory flow.
    *   Memory ingestion → Neural Memory update → Surprise detection → QuickRecal boosting → Retrieval cycle working.
    *   Memory ID tracking and lookup for dynamic scoring implemented.
    *   Intent ID generation for cognitive trace monitoring.
    *   Surprise metrics (loss, gradient norm) flowing properly to Memory Core.
    *   Emotional context preservation throughout processing.
    *   Cognitive diagnostics surface layer implemented (alerts, recommendations).
    *   Performance improvements (processing time reduced from ~4900ms to ~650ms).
*   **Critical Fixes (March 2025):**
    *   Added `get_memory_by_id` method to SynthiansMemoryCore.
    *   Implemented `update_memory` method for quickrecal score updates.
    *   Fixed Neural Memory dimension mismatches with adaptive validation.
    *   Corrected projection handling in retrieval path.
    *   Ensure surprise feedback properly impacts memory importance.

---

### Phase 4: Meta-Attentional Systems (Planned)

*   **Objective:** Implement and evaluate the different ways of integrating the Neural Memory with Attention, as described in Section 4 of the Titans paper (MAC, MAG, MAL).
*   **Status:** **TODO**
*   **Tasks:**
    *   Design Keras/TF layers implementing the specific attention/gating mechanisms for MAC, MAG, MAL.
    *   Integrate these layers with the `NeuralMemoryModule` and `MemoryCore` (likely within or called by the `ContextCascadeEngine`).
    *   Benchmark the different approaches on various cognitive tasks.
    *   Implement meta-learning for adaptive attention mechanism selection.

---

### Phase 5: Protocol Seal Layer (Planned)

*   **Objective:** Implement access control protocols for Lucidia's internal memory systems.
*   **Status:** **TODO**
*   **Tasks:**
    *   Design protocol abstractions for memory access patterns.
    *   Implement authentication and authorization mechanisms.
    *   Create hooks for permission verification.
    *   Add logging and audit trails for memory operations.

---

### Phase 6: Reflective Summary Module (Planned)

*   **Objective:** Enable Lucidia to explain her cognitive processes and decision-making.
*   **Status:** **TODO**
*   **Tasks:**
    *   Implement memory trace analysis for decision pathways.
    *   Create narrative generation for cognitive processes.
    *   Develop visualization tools for memory activations.
    *   Add explainability metrics and feedback mechanisms.

---

## Full Cognitive Cycle

Lucidia now implements a complete cognitive cycle connecting all components in a bi-directional feedback loop:

1. **Memory Ingestion**
   - New content/embedding received by Memory Core
   - Metadata synthesized and QuickRecal score initialized
   - Memory stored with ID in MemoryCore and Vector Index

2. **Neural Memory Update**
   - Memory embedding sent to Neural Memory module
   - Test-time learning via gradient updates occurs
   - Current memory state (M_t) updated with new association
   - Surprise metrics (loss, gradient norm) calculated

3. **Surprise Integration**
   - Surprise metrics sent back to Memory Core
   - QuickRecal score dynamically boosted based on surprise
   - Memory importance adjusted to reflect cognitive significance

4. **Memory Retrieval**
   - Query embedding sent to Neural Memory for association retrieval
   - Retrieved embedding combined with Vector Index results
   - Emotional gating applied based on current context
   - Most relevant memories returned with confidence scores

5. **Cognitive Diagnostics**
   - System-wide metrics tracked and analyzed
   - Alerts generated for anomalies (high loss, gradient issues)
   - Recommendations provided for parameter tuning
   - Emotional diversity and bias measured

This cycle operates continuously, allowing Lucidia to adapt, learn from surprises, remember what's important, and retrieve memories based on both semantic similarity and learned associations.
```

# docs\orchestrator\attention.md

```md
# Attention Mechanism in Titans Variants

**Author:** Lucidia Core Team
**Date:** 2025-03-30
**Status:** Implemented

## Overview

The attention mechanism is a core component of Lucidia's Phase 4 implementation, providing the foundation for the Titans Architecture Variants (MAC, MAG, MAL). Each variant directly incorporates TensorFlow's `tf.keras.layers.MultiHeadAttention` layer to enable sophisticated temporal context awareness and enhanced memory operations.

> *"Attention is the lens through which memory gains focus."*

## Implementation Details

The attention mechanism is implemented within each Titans variant class in `orchestrator/titans_variants.py`, utilizing TensorFlow's built-in multi-head attention layer with configuration specific to each variant's needs.

### Key Features

1. **Robust Embedding Handling**:
   - Validation of input embeddings through wrapper methods
   - Automatic dimension alignment (384D vs 768D handling) via the GeometryManager
   - Proper batching and reshaping of inputs before passing to attention mechanism

2. **Performance Optimizations**:
   - Configurable number of attention heads (default: 4)
   - Per-head dimension control (default: 32)
   - Optional dropout for regularization (default: 0.0)

3. **Variant-Specific Applications**:
   - **MAC**: Enhances memory retrieval by attending over historical memory outputs
   - **MAG**: Modifies gate values for neural memory updates by attending over historical keys
   - **MAL**: Modifies value projections by attending over historical values

4. **Integration with Sequence Context**:
   - Maintains history of recent memory operations via SequenceContextManager
   - Provides temporal context for attention operations

## Configuration

The attention mechanism is configured via the `TitansVariantConfig` class with the following parameters:

\`\`\`python
# Default configuration values
defaults = {
    "variant": TitansVariantType.NONE.value,  # NONE, MAC, MAG, or MAL
    "attention_num_heads": 4,              # Number of attention heads
    "attention_key_dim": 32,               # Dimension per head
    "attention_dropout": 0.0,              # Dropout rate
    "max_context_length": 50,             # Max sequence history length
    "max_dim_mismatch_warnings": 10,      # Rate limiting for warnings
}
\`\`\`

## Variant-Specific Implementations

### MAC (Memory-Attended Computation)

The MAC variant enhances memory retrieval by attending over historical memory outputs:

\`\`\`python
# Simplified example from MACVariant.__init__
self.attention_module = tf.keras.layers.MultiHeadAttention(
    num_heads=attention_config["num_heads"],
    key_dim=attention_config["key_dim"],
    dropout=attention_config["dropout"],
    name="MAC_Attention"
)
\`\`\`

Flow: `q_t -> M -> y_t -> Attend(q_t, K_hist, Y_hist) -> attended_y_t`

### MAG (Memory-Attended Gates)

The MAG variant modifies gate values for neural memory updates:

\`\`\`python
# Simplified example from MAGVariant.__init__
self.attention_module = tf.keras.layers.MultiHeadAttention(
    num_heads=attention_config["num_heads"],
    key_dim=attention_config["key_dim"],
    dropout=attention_config["dropout"],
    name="MAG_Attention"
)
\`\`\`

Flow: 
1. `q_t -> Attend(q_t, K_hist, K_hist) -> attention_output`
2. Call Neural Memory's `/calculate_gates` endpoint with attention output
3. Update memory with calculated gates

### MAL (Memory-Augmented Learning)

The MAL variant modifies value projections for neural memory updates:

\`\`\`python
# Simplified example from MALVariant.__init__
self.attention_module = tf.keras.layers.MultiHeadAttention(
    num_heads=attention_config["num_heads"],
    key_dim=attention_config["key_dim"],
    dropout=attention_config["dropout"],
    name="MAL_Attention"
)
\`\`\`

Flow: 
1. `q_t, K_hist, V_hist -> Attend(q_t, K_hist, V_hist) -> attended_v_t`
2. Combine `attended_v_t` with `v_t` -> `v_prime_t`
3. Update memory with `k_t` and `v_prime_t`

## Usage Example

The ContextCascadeEngine coordinates the use of attention mechanisms within the appropriate variant:

\`\`\`python
# Example configuration in ContextCascadeEngine
variant_config = TitansVariantConfig(
    variant="MAC",                # Use Memory-Attended Computation variant
    attention_num_heads=8,       # 8 attention heads
    attention_key_dim=64,        # 64 dimensions per head
    attention_dropout=0.1,       # 10% dropout for regularization
    max_context_length=100       # Remember up to 100 prior interactions
)

# Initialize the engine with this configuration
engine = ContextCascadeEngine(
    memory_core_url="http://localhost:5010",
    neural_memory_url="http://localhost:8001",
    variant_config=variant_config
)
\`\`\`

## Best Practices

1. **Sequence Length**: Balance history length with computational resources; longer sequences provide more context but require more memory and processing time.

2. **Embedding Dimension**: Ensure the embedding dimension is consistent or properly aligned with the GeometryManager when using multiple embedding models.

3. **Head Configuration**: More attention heads allow finer-grained focus but increase computational cost. The default of 4 heads with 32 dimensions per head works well for most scenarios.

4. **Variant Selection**: 
   - Use MAC for improved retrieval quality when sequence matters
   - Use MAG for dynamic adjustments to memory learning rates based on context
   - Use MAL for directly influencing what is stored in memory

```

# docs\orchestrator\cce.md

```md
# Context Cascade Engine (CCE)

*This is a placeholder document for detailed documentation on the Context Cascade Engine (CCE).*

## Overview

The Context Cascade Engine (CCE) is the central orchestrator of the Synthians Cognitive Architecture, implementing the refactored cognitive flow between the Memory Core and Neural Memory services. It manages the sequence of operations that constitute the cognitive cycle, including variant-specific steps for MAC, MAG, and MAL implementations.

## Core Functionality

### Cognitive Cycle

The CCE implements the following sequence for processing a new input (`content`, `embedding`, `metadata`):

1. **Store Memory:** CCE sends input to Memory Core (`/process_memory`). Memory Core stores it, generates metadata, calculates initial QuickRecal, and returns the validated embedding (`x_t`), `memory_id`, and `quickrecal_score`.

2. **Get Projections:** CCE sends `x_t` to Neural Memory Server (`/get_projections`). NM Server returns Key (`k_t`), Value (`v_t`), and Query (`q_t`) projections *without* updating its internal weights.

3. **Variant Pre-Update (MAG/MAL):**
   - If **MAG** is active: CCE calculates attention output (using `q_t`, historical keys `K_hist`) and calls NM Server (`/calculate_gates`) to get external gate values (`alpha_t`, `theta_t`, `eta_t`).
   - If **MAL** is active: CCE calculates attention output (using `q_t`, historical keys `K_hist`, historical values `V_hist`), combines it with `v_t` to create a modified value projection (`v'_t`).
   - If **NONE** or **MAC**: This step is skipped.

4. **Update Neural Memory:** CCE calls NM Server (`/update_memory`) providing:
   - Base: `input_embedding` (`x_t`).
   - MAG: External gate values (`external_alpha_gate`, etc.).
   - MAL: Explicit projections (`key_projection=k_t`, `value_projection=v'_t`).
   - NM Server performs the test-time update using the provided parameters and returns `loss` and `grad_norm`.

5. **Apply QuickRecal Boost:** CCE calculates a boost value based on `loss`/`grad_norm`. It calls Memory Core (`/api/memories/update_quickrecal_score`) to apply this boost to the original memory's score.

6. **Retrieve from Neural Memory:** CCE sends `x_t` to NM Server (`/retrieve`). NM Server calculates the query projection `q_t` (may differ slightly from step 2 if weights changed) and retrieves the associated raw embedding (`y_t_raw`) using its internal memory `M(q_t)`. It returns `y_t_raw` and the `query_projection` used.

7. **Variant Post-Retrieval (MAC):**
   - If **MAC** is active: CCE calculates attention output (using `q_t` from step 6, historical keys `K_hist`, historical outputs `Y_hist`), combines it with `y_t_raw` to create an attended output (`y_t_final`).
   - Otherwise, `y_t_final` is set to `y_t_raw`.

8. **Update History:** CCE adds the full context tuple `(timestamp, memory_id, x_t, k_t, v_t, q_t, y_t_final)` to the `SequenceContextManager`.

9. **Finalize:** CCE constructs and returns a response containing the `memory_id`, processing status, surprise metrics, retrieval results (`y_t_final`), QuickRecal feedback status, and variant metrics.

### SequenceContextManager

The `SequenceContextManager` maintains a history of recent cognitive operations for use in attention mechanisms:

- It stores a deque of tuples `(timestamp, memory_id, x, k, v, q, y_final)` representing the history of processed inputs and their projections/outputs.
- It provides methods for retrieving historical keys, values, queries, and outputs needed for attention calculations.
- It manages the history size to prevent memory leaks while maintaining sufficient context for attention.

### Variant Support

The CCE dynamically configures itself based on the selected Titans Architecture Variant:

- **MAC (Memory-Attention-Combined)**: Enhances Neural Memory output using attention over historical outputs.
- **MAG (Memory-Attention-Gated)**: Modulates memory update gates using attention over historical keys.
- **MAL (Memory-Attention-Layer)**: Modifies the value projection using attention over historical keys and values.

The variant can be selected via the `TITANS_VARIANT` environment variable.

## TensorFlow Integration

The CCE implements lazy loading of TensorFlow to avoid NumPy version conflicts:

\`\`\`python
def _get_tf():
    """Lazily import TensorFlow to avoid early NumPy import."""
    global _tf
    if _tf is None:
        import tensorflow as tf
        _tf = tf
    return _tf
\`\`\`

This approach ensures that `fix_numpy.py` can execute before TensorFlow tries to import NumPy.

## Surprise Feedback Loop

A key responsibility of the CCE is implementing the surprise feedback loop:

1. The Neural Memory Server's `/update_memory` endpoint returns `loss` and `grad_norm` metrics.
2. The CCE calculates a `boost` value based on these metrics (higher surprise → higher boost).
3. The CCE calls the Memory Core's `/api/memories/update_quickrecal_score` endpoint with the `memory_id` and `delta=boost`.
4. The Memory Core updates the memory's QuickRecal score and adds surprise metadata.

This mechanism reinforces memories that contained surprising or hard-to-predict information, implementing the principle that **"Surprise signals significance."**

## Configuration Options

- `memory_core_url`: URL of the Memory Core API
- `neural_memory_url`: URL of the Neural Memory Server API
- `titans_variant`: Selected variant ("MAC", "MAG", "MAL", or "NONE")
- `history_size`: Maximum number of entries in the sequence history
- `attention_temperature`: Scaling factor for attention softmax
- `surprise_boost_factor`: Scaling factor for converting surprise metrics to QuickRecal boosts

```

# docs\orchestrator\README.md

```md
# Context Cascade Engine Documentation

This directory contains documentation for the Context Cascade Engine (CCE) and its components that orchestrate the cognitive cycle.

## Contents

* [Context Cascade Engine](./cce.md): **(Placeholder)** Overview of the `ContextCascadeEngine` class that implements the refactored cognitive flow.
* [Titans Variants](./titans_variants.md): Documentation on the MAC, MAG, and MAL variants from the Titans paper and their implementation in the CCE.
* [Attention Mechanisms](./attention.md): Details on how attention is calculated and applied in the different variant implementations.
* [Sequence Context Management](./sequence_context.md): **(Placeholder)** Documentation on the `SequenceContextManager` that maintains history for attention operations.

## Technical Details

* **Variant Flow**: Different processing paths for MAC (post-retrieval attention), MAG (gated update), and MAL (value modification).
* **TensorFlow Integration**: How lazy loading of TensorFlow avoids NumPy version conflicts.
* **Surprise Feedback Loop**: How loss and gradient norm from Neural Memory are converted into QuickRecal score boosts in Memory Core.
* **History Management**: How the sequence context of embeddings, keys, values, and outputs is maintained and used for attention calculations.

```

# docs\orchestrator\sequence_context.md

```md
# Sequence Context Management

**Author:** Lucidia Core Team  
**Date:** 2025-03-30  
**Status:** Implemented

## Overview

The `SequenceContextManager` is responsible for maintaining a history of cognitive operations for use in attention mechanisms within the Titans Architecture variants. It provides a fixed-length buffer of recent processing steps including input embeddings, projections, and outputs, enabling temporal context for attention calculations.

## Implementation Details

The `SequenceContextManager` is implemented in `orchestrator/history.py` and uses a `collections.deque` with a fixed maximum length to efficiently manage the sequence history.

### Context Structure

Each context entry is stored as a tuple with the following components:

\`\`\`python
ContextTuple = Tuple[float, str, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
# (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)
\`\`\`

Where:
- `timestamp`: When the entry was processed (float)
- `memory_id`: Unique identifier of the memory (string)
- `x_t`: Original input embedding (numpy array)
- `k_t`: Key projection (numpy array)
- `v_t`: Value projection (numpy array)
- `q_t`: Query projection (numpy array)
- `y_t`: Neural memory output embedding (numpy array)

## API Reference

### Constructor

\`\`\`python
SequenceContextManager(max_length: int = 50)
\`\`\`

**Parameters:**
- `max_length`: Maximum number of context tuples to store (default: 50)

### Methods

#### add_context

\`\`\`python
def add_context(
    self,
    memory_id: str,
    x_t: np.ndarray,
    k_t: np.ndarray,
    v_t: np.ndarray,
    q_t: np.ndarray,
    y_t: np.ndarray,
    timestamp: Optional[float] = None
) -> None
\`\`\`

Adds a new context element (tuple) to the buffer.

**Parameters:**
- `memory_id`: Identifier for the memory entry
- `x_t`: Input embedding
- `k_t`: Key projection
- `v_t`: Value projection
- `q_t`: Query projection
- `y_t`: Neural memory output embedding
- `timestamp`: Optional timestamp (defaults to current time)

#### update_last_context

\`\`\`python
def update_last_context(self, y_t: np.ndarray) -> bool
\`\`\`

Updates the most recent context entry with the y_t value. This is useful when y_t is not available at the time of initial context creation.

**Parameters:**
- `y_t`: The retrieved embedding (output from Neural Memory)

**Returns:**
- `True` if update was successful, `False` otherwise

#### get_recent_history

\`\`\`python
def get_recent_history(self, count: Optional[int] = None) -> List[ContextTuple]
\`\`\`

Returns the most recent context tuples.

**Parameters:**
- `count`: Optional number of items to retrieve (defaults to all available)

**Returns:**
- List of context tuples

#### Retrieval Helper Methods

The following methods extract specific components from the history:

\`\`\`python
def get_recent_keys(self, count: Optional[int] = None) -> List[np.ndarray]
def get_recent_values(self, count: Optional[int] = None) -> List[np.ndarray]
def get_recent_queries(self, count: Optional[int] = None) -> List[np.ndarray]
def get_recent_outputs(self, count: Optional[int] = None) -> List[np.ndarray]
\`\`\`

Each method returns a list of the specific components (k_t, v_t, q_t, or y_t) from the most recent entries.

#### Convenience Methods for Attention

\`\`\`python
def get_recent_kv_pairs(self, count: Optional[int] = None) -> Tuple[List[np.ndarray], List[np.ndarray]]
def get_recent_ky_pairs(self, count: Optional[int] = None) -> Tuple[List[np.ndarray], List[np.ndarray]]
\`\`\`

These methods return pairs of components specifically needed for attention calculations:
- `get_recent_kv_pairs`: Returns (keys, values) for MAL variant
- `get_recent_ky_pairs`: Returns (keys, outputs) for MAC variant

#### Utility Methods

\`\`\`python
def __len__(self) -> int  # Returns the current number of items in the buffer
def clear(self) -> None    # Clears the context buffer
\`\`\`

## Integration with Titans Variants

The different Titans variants use the sequence context in different ways:

- **MAC (Memory-Attended Computation):**
  - Uses `get_recent_ky_pairs()` to retrieve historical keys and output embeddings
  - Applies attention between current query and history to enhance the retrieved output

- **MAG (Memory-Attended Gates):**
  - Uses `get_recent_keys()` to retrieve historical keys
  - Applies attention between current query and historical keys to calculate gate values

- **MAL (Memory-Attended Learning):**
  - Uses `get_recent_kv_pairs()` to retrieve historical keys and values
  - Applies attention to modify the value projection before neural memory update

## Usage Example

\`\`\`python
# Create a sequence context manager with max 100 entries
sequence_manager = SequenceContextManager(max_length=100)

# Add a new context entry after processing
sequence_manager.add_context(
    memory_id="mem_12345",
    x_t=input_embedding,
    k_t=key_projection,
    v_t=value_projection,
    q_t=query_projection,
    y_t=output_embedding
)

# Retrieve historical keys and values for attention
historical_keys, historical_values = sequence_manager.get_recent_kv_pairs(count=10)

# Apply attention between current query and history
attention_weights = calculate_attention(current_query, historical_keys)
attended_value = np.sum(attention_weights[:, np.newaxis] * historical_values, axis=0)
\`\`\`

## Best Practices

1. **Buffer Size Management:** Choose an appropriate `max_length` value that balances memory usage with sufficient context for attention calculations. The default of 50 is sufficient for most scenarios.

2. **Embedding Validation:** Always ensure that embeddings passed to `add_context()` are valid numpy arrays to prevent issues with attention calculations.

3. **Context Population:** Allow sufficient context to accumulate before relying heavily on attention mechanisms. Variants can handle empty or small history buffers, but their effectiveness improves with more context.

4. **Temporal Relevance:** Consider that older context entries may be less relevant. The deque automatically removes the oldest entries when full, maintaining recency.

```

# docs\orchestrator\titans_variant_refactor.md

```md
# Titans Variant Refactoring: Fixing MAG/MAL Timing

**Author:** Lucidia Core Team
**Date:** 2025-03-28
**Status:** Completed

## Problem Statement

The current implementation of the Context Cascade Engine (CCE) has a timing issue that prevents the MAG and MAL variants from properly influencing the Neural Memory update process. Specifically, the variant processing occurs *after* the `/update_memory` call they are intended to influence, rendering their modifications ineffective.

> *"The cascade must flow in the right order."*

## Previous Flow

The previous `ContextCascadeEngine.process_new_input` method followed this sequence:

1. Store input in Memory Core → Get `x_t`, `memory_id`
2. Update Neural Memory with `x_t` → Get `k_t`, `v_t`, `q_t`, `loss`, `grad_norm`
3. Update QuickRecal score with `loss`, `grad_norm`
4. Retrieve from Neural Memory → Get `y_t` (raw retrieval)
5. Process variant (MAC/MAG/MAL):
   - For MAC: Override `y_t` with attention-augmented `attended_y_t`
   - For MAG/MAL: Calculate outputs, but **too late** to affect `/update_memory`
6. Add context to history
7. Return final results

This sequence was problematic because:

- MAG is designed to modify the gate values (`alpha_t`, `theta_t`, `eta_t`) that control the Neural Memory update
- MAL is designed to modify the value projection (`v_prime_t`) before it's used in the Neural Memory update
- Both modifications need to happen *before* step 2 (the `/update_memory` call)

## Implemented Refactored Flow

The solution has been implemented by reorganizing the processing flow so that variant-specific modifications occur before the `/update_memory` call:

1. Store input in Memory Core → Get `x_t`, `memory_id`
2. **Get projections from Neural Memory** → Get `k_t`, `v_t`, `q_t` (without updating)
3. **Apply variant-specific preprocessing**:
   - If MAG: Calculate attention-based gates (`alpha_t`, `theta_t`, `eta_t`)
   - If MAL: Calculate modified value (`v_prime_t`)
4. **Update Neural Memory** with appropriate modifications:
   - If MAG: Include gate values in request
   - If MAL: Use modified value projection
   - Get `loss`, `grad_norm` from response
5. Update QuickRecal score
6. Retrieve from Neural Memory → Get `y_t` (raw retrieval)
7. **Apply post-retrieval variant processing**:
   - If MAC: Override `y_t` with attention-augmented `attended_y_t`
8. Add full context to history
9. Return final results

## Implementation Details

### 1. Modular Design

The refactored `ContextCascadeEngine.process_new_input` method now uses a series of specialized helper methods for better readability and maintainability:

\`\`\`python
async def process_new_input(self, content: str, embedding: Optional[List[float]] = None, metadata: Optional[Dict[str, Any]] = None, intent_id: Optional[str] = None):
    """Orchestrates the refactored cognitive cascade for a single input."""
    async with self.processing_lock:
        # 1. Setup Intent & Metadata
        intent_id, user_emotion = self._setup_intent_and_metadata(intent_id, metadata)
        
        # Initialize context dict for this step
        step_context = {...}  # Contains all processing state
        
        # 2. Store Memory
        store_resp = await self._store_memory(content, embedding, metadata)
        
        # 3. Get Projections (without updating memory)
        proj_resp = await self._get_projections_from_nm(step_context["x_t"])
        
        # 4. Variant Pre-Update Logic (MAG/MAL)
        if self.variant_processor and self.active_variant_type in [TitansVariantType.MAG, TitansVariantType.MAL]:
            variant_pre_result = await self._apply_variant_pre_update(step_context)
        
        # 5. Update Neural Memory
        update_resp = await self._update_neural_memory(step_context)
        
        # 6. Apply QuickRecal Boost
        feedback_resp = await self._apply_quickrecal_boost(step_context, quickrecal_initial)
        
        # 7. Retrieve from Neural Memory
        retrieve_resp = await self._retrieve_from_neural_memory(step_context["x_t"])
        
        # 8. Apply MAC Post-Retrieval Logic
        if self.variant_processor and self.active_variant_type == TitansVariantType.MAC:
            mac_resp = await self._apply_variant_post_retrieval(step_context)
        
        # 9. Update History
        await self._update_history(step_context)
        
        # 10. Finalize Response
        response = self._finalize_response({}, step_context, update_resp, retrieve_resp, feedback_resp)
        
        return response
\`\`\`

### 2. Robust Error Handling

Each helper method now includes comprehensive error handling and validation:

- Embedding validation to handle NaN/Inf values
- Type checking and conversion between numpy arrays and lists
- Graceful handling of dimension mismatches
- Proper logging of error conditions

### 3. TensorFlow Lazy Loading

To prevent NumPy version conflicts, TensorFlow is now lazy-loaded only when needed:

\`\`\`python
# Global variable for TensorFlow instance
_tf = None

def _get_tf():
    """Lazy-load TensorFlow only when needed."""
    global _tf
    if _tf is None:
        try:
            import tensorflow as tf
            _tf = tf
            logger.info("TensorFlow loaded successfully")
        except ImportError as e:
            logger.error(f"Failed to import TensorFlow: {e}")
    return _tf
\`\`\`

### 4. MAL Variant Implementation

The MAL variant now includes a `calculate_v_prime` method that modifies the value projection using attention over historical values:

\`\`\`python
async def calculate_v_prime(self, q_t: np.ndarray, v_t: np.ndarray):
    """Calculate modified value projection using attention over historical values."""
    # Get historical keys and values
    k_hist, v_hist = self.sequence_context.get_recent_kv_pairs()
    
    # Apply attention to generate attended values
    attended_v = self.attention_module(
        query=q_t,
        key=k_hist,
        value=v_hist
    )
    
    # Combine original and attended values
    v_prime = self.combine_values(v_t, attended_v)
    
    return {"v_prime": v_prime, "metrics": {...}}
\`\`\`

## Testing Results

All four Titans variants (NONE, MAC, MAG, MAL) have been tested and confirmed to function correctly:

- **NONE**: Base functionality works with default processing
- **MAC**: Successfully modifies retrieved memory output with attention
- **MAG**: Properly influences Neural Memory update with calculated gate values
- **MAL**: Correctly modifies value projections before Neural Memory update

## Conclusion

The refactored implementation successfully addresses the timing issues with the MAG and MAL variants while improving code modularity, readability, and maintainability. The additional parameter flexibility provides a solid foundation for further extensions and optimizations of the cognitive architecture.

---

**Related Documentation:**
- [MAG Variant Implementation](mag_variant_implementation.md)
- [Architecture Overview](architecture_overview.md)
- [Embedding Handling](embedding_handling.md)
- [NumPy/TensorFlow Compatibility](numpy_tensorflow_compatibility.md)

```

# docs\orchestrator\titans_variants_integration.md

```md
# Titans Architecture Variants Integration

## Progress Report

### Resolved Issues

1. **NumPy Compatibility** 
   - Fixed via lazy loading of TensorFlow in `titans_variants.py`
   - Implemented thread-safe singleton pattern for `_get_tf()`
   - Added TYPE_CHECKING to handle type annotations without triggering imports
   - Successfully eliminated the `numpy.dtype size changed` binary incompatibility error

2. **Neural Memory Configuration** 
   - Updated `query_dim` in `http_server.py` from 768 to 128 to match `key_dim`
   - Properly set other relevant dimensions in configuration
   - Fixed the core dimensional mismatch that was causing projection errors

3. **TensorFlow API Compatibility** 
   - Removed unsupported parameters from MultiHeadAttention layer
   - Removed `use_layer_norm` and `use_residual` which are not available in the current TF version
   - Updated all three variants (MAC, MAG, MAL) with compatible parameter sets

4. **MAG Variant Implementation** 
   - Fixed the `debug_logging` AttributeError in ContextCascadeEngine
   - Implemented dynamic capability detection in `/config` endpoint using inspect module
   - Added API client initialization in TitansVariantBase
   - Updated variant processor initialization to properly set neural_memory_url
   - Successfully tested the MAG variant with different inputs and verified gate adaptation

### Remaining Issues

1. **FAISS GPU Acceleration** 
   - While TensorFlow correctly identified the GPU (RTX 4090), FAISS is using the CPU version
   - "Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined" warning indicates missing GPU support
   - This is a potential optimization for future work but not blocking functionality

2. **MAL Variant Testing** 
   - While MAG variant is fully functional, comprehensive testing of MAL variant is still needed
   - Verify that external projections work correctly for the MAL variant

## Next Steps

1. **Comprehensive Testing**
   - Complete testing of MAL variant to ensure full compatibility
   - Develop benchmarks comparing performance differences between variants
   - Create regression tests to prevent future compatibility issues

2. **Documentation Finalization**
   - Complete the API documentation for each Titans variant
   - Provide examples of when to use each variant based on use case
   - Document the configuration parameters and their effects

## Implementation Details

### Lazy Loading Pattern

\`\`\`python
# Lazy-load TensorFlow to avoid NumPy incompatibility issues
_tf = None
_tf_lock = threading.Lock()

def _get_tf():
    """Lazy-load TensorFlow only when needed to avoid early NumPy conflicts"""
    global _tf
    if _tf is None:
        with _tf_lock:
            # Double-check after acquiring lock (thread-safe singleton pattern)
            if _tf is None:
                import tensorflow as tf
                _tf = tf
    return _tf
\`\`\`

### Variant Initialization

Variants are initialized with compatible MultiHeadAttention parameters:

\`\`\`python
self.attention_module = _get_tf().keras.layers.MultiHeadAttention(
    num_heads=attention_config["num_heads"],
    key_dim=attention_config["key_dim"],
    dropout=attention_config["dropout"],
    name="MAC_Attention"
)
\`\`\`

### Neural Memory Configuration

Updated configuration with proper dimension alignment:

\`\`\`python
default_config_dict = {
    # Set input_dim to match Memory Core's embedding dimension (768)
    'input_dim': 768,
    # Key and query dimensions should match for proper attention computation
    'key_dim': 128,
    'query_dim': 128,  # Match key_dim for proper dimension alignment
    'value_dim': 768,  # Output dimension matches input_dim for consistency
    'hidden_dim': 512   # Intermediate projection dimension
}
\`\`\`

### Dynamic Capability Detection

To support runtime variant capabilities detection, we've implemented a dynamic signature inspection approach:

\`\`\`python
# Dynamically determine capabilities based on implemented method signatures
# Check if update_step supports external gates and projections using inspect
update_step_sig = inspect.signature(nm.update_step)
supports_external_gates = any(param in update_step_sig.parameters 
                           for param in ["external_alpha_t", "external_theta_t", "external_eta_t"])
supports_external_projections = any(param in update_step_sig.parameters 
                                for param in ["external_k_t", "external_v_t"])

logger.info(f"Detected capabilities: supports_external_gates={supports_external_gates}, "
           f"supports_external_projections={supports_external_projections}")
\`\`\`

### MAG Variant Implementation

MAG (Memory-Attended Gates) variant modifies gate values through attention mechanisms:

\`\`\`python
# Process input and calculate gates using attention output
async def process_input(self, memory_id, x_t, k_t, v_t, q_t, y_t):
    try:
        # Use attention to determine gate values
        attention_output = self.compute_attention(q_t, k_t)
        
        # Call Neural Memory's /calculate_gates endpoint
        response = self.api_client.calculate_gates(
            attention_output=attention_output.numpy().tolist()
        )
        
        # Extract the calculated gates
        gates = response.get("gates", {})
        alpha_t = gates.get("alpha_t")
        theta_t = gates.get("theta_t")
        eta_t = gates.get("eta_t")
        
        logger.info(f"MAG variant calculated gates: alpha={alpha_t}, theta={theta_t}, eta={eta_t}")
        
        return {
            "memory_id": memory_id,
            "gates": gates,
            "metrics": {
                "attention_output_norm": float(np.linalg.norm(attention_output))
            }
        }
    except Exception as e:
        logger.error(f"Error in MAG variant processing: {str(e)}")
        return {"error": str(e)}

```

# docs\README.md

```md
# Synthians Cognitive Architecture - Documentation

Welcome to the documentation for the Synthians Cognitive Architecture, a system designed to emulate aspects of human memory and cognition.

## Overview

This documentation provides comprehensive details on the system's architecture, its core components (Memory Core, Neural Memory, Context Cascade Engine), the underlying APIs, and usage guidelines.

**Key Concepts:**

*   **Bi-Hemispheric Model:** The system loosely models the interaction between episodic/declarative memory (Memory Core - The Archive) and procedural/associative memory (Neural Memory - The Associator).
*   **QuickRecal:** A dynamic relevance score for memories, influenced by factors like recency, emotion, and surprise.
*   **Surprise Feedback:** The Neural Memory provides signals (loss, gradient norm) indicating how surprising new input is, which boosts the QuickRecal score of corresponding memories in the Core.
*   **Asynchronous Processing:** Built with `asyncio` for efficient handling of I/O-bound operations.

## Navigation

*   **[Architecture](./ARCHITECTURE.md):** High-level overview of the system's design, principles, and the Bi-Hemispheric model.
*   **[Component Guide](./COMPONENT_GUIDE.md):** Detailed breakdown of each major component (Memory Core, Neural Memory, CCE, Tools, Testing) and their roles.
*   **[API Reference & Client Usage](./api/README.md):** Documentation for the HTTP APIs and the Python client library.
    *   [API Reference](./api/API_REFERENCE.md)
    *   [Client Usage Guide](./api/client_usage.md)
*   **[Guides](./guides/README.md):** Practical guides for setup, development, and specific use cases.
*   **[Architecture Changes](./architechture-changes.md):** Log of significant architectural decisions and evolution.
*   **[Changelog](./CHANGELOG.md):** Chronological list of changes and updates to the codebase.

## Getting Started

1.  Review the **[Architecture](./ARCHITECTURE.md)** to understand the core concepts.
2.  Explore the **[Component Guide](./COMPONENT_GUIDE.md)** for details on individual parts.
3.  If interacting programmatically, consult the **[API Reference & Client Usage](./api/README.md)**.
4.  For setup and development workflows, see the **[Guides](./guides/README.md)**.

---

*This documentation is actively maintained alongside the codebase.*

```

# docs\testing\integration_testing.md

```md
# Integration Testing Guide for Synthians Cognitive System

**Author:** Lucidia Core Team  
**Date:** 2025-03-30  
**Status:** Implemented

## Overview

Integration testing for the Synthians Cognitive Architecture focuses on verifying that the three main components (Memory Core, Neural Memory Server, and Context Cascade Engine) work together correctly to implement the complete cognitive cycle, including the surprise feedback loop and variant-specific behaviors.

## Components Under Test

1. **Memory Core (`synthians_memory_core`)**: Responsible for stable, indexed storage of memories and their embeddings.
2. **Neural Memory Server (`synthians_trainer_server`)**: Implements test-time learning and associative memory retrieval.
3. **Context Cascade Engine (`orchestrator`)**: Orchestrates the cognitive flow between Memory Core and Neural Memory.

## Key Integration Points

### Memory Core u2194 Neural Memory Server (via CCE)

- **Store u2192 Update u2192 Boost Flow**: Verify that memories stored in Memory Core trigger Neural Memory updates, which generate surprise metrics that correctly boost the original memory's QuickRecal score.
- **Embedding Validation Chain**: Verify that embedding validation (NaN/Inf checks) is consistently applied across service boundaries.
- **Dimension Alignment**: Confirm that embeddings of different dimensions (384D vs 768D) are correctly aligned when passing between services.

### Context Cascade Engine Orchestration

- **Cognitive Cycle Timing**: Verify the correct sequence and timing of the refactored cognitive flow.
- **Variant-Specific Logic**: Test that MAC, MAG, and MAL variants correctly implement their attention mechanisms.
- **History Management**: Confirm that sequence history is properly maintained for attention calculations.

## Test Environment Setup

\`\`\`python
from synthians.testing import ServiceTestFixture, MockMemoryCore, MockNeuralMemory

def setup_integration_environment(variant="NONE", mock_services=False):
    """Set up an environment for integration testing."""
    if mock_services:
        # Use mocks for isolated testing
        memory_core = MockMemoryCore()
        neural_memory = MockNeuralMemory()
    else:
        # Use actual services
        memory_core = MemoryCoreClient("http://localhost:5010")
        neural_memory = NeuralMemoryClient("http://localhost:5011")
    
    # Set environment variable for Titans variant
    os.environ["TITANS_VARIANT"] = variant
    
    # Create CCE client
    cce = CCEClient(
        memory_core_url="http://localhost:5010",
        neural_memory_url="http://localhost:5011"
    )
    
    return memory_core, neural_memory, cce
\`\`\`

## Test Scenarios

### Basic Cognitive Cycle

\`\`\`python
@pytest.mark.integration
def test_basic_cognitive_cycle():
    # 1. Initialize test setup
    memory_core, neural_memory, cce = setup_integration_environment()
    
    # 2. Process a new memory through CCE
    content = "This is a test memory with specific content."
    response = cce.process_memory(content=content)
    memory_id = response.memory_id
    
    # 3. Verify memory was stored in Memory Core
    memory = memory_core.get_memory_by_id(memory_id)
    assert memory is not None
    assert memory.content == content
    
    # 4. Verify surprise metrics were returned
    assert "loss" in response.surprise_metrics
    assert "grad_norm" in response.surprise_metrics
    
    # 5. Verify QuickRecal boost was applied
    assert response.feedback_applied
    
    # 6. Verify retrieval works
    retrieved = memory_core.retrieve_memories(query=content, top_k=1)
    assert len(retrieved) > 0
    assert retrieved[0].id == memory_id
    
    # 7. Verify embedding validation worked
    embedding = memory.embedding
    assert not np.isnan(embedding).any()
    assert not np.isinf(embedding).any()
\`\`\`

### Surprise Feedback Loop

\`\`\`python
@pytest.mark.integration
def test_surprise_feedback_loop():
    # Setup
    memory_core, _, cce = setup_integration_environment()
    
    # 1. Process a routine memory (low surprise expected)
    routine_content = "This is routine information similar to existing memories."
    routine_response = cce.process_memory(content=routine_content)
    routine_id = routine_response.memory_id
    routine_surprise = routine_response.surprise_metrics["loss"]
    routine_initial_qr = memory_core.get_memory_by_id(routine_id).quickrecal_score
    
    # 2. Process a surprising memory (high surprise expected)
    surprise_content = "This is completely unexpected and novel information with unusual patterns."
    surprise_response = cce.process_memory(content=surprise_content)
    surprise_id = surprise_response.memory_id
    surprise_surprise = surprise_response.surprise_metrics["loss"]
    surprise_initial_qr = memory_core.get_memory_by_id(surprise_id).quickrecal_score
    
    # 3. Process several more routine memories to establish baseline
    for i in range(5):
        cce.process_memory(content=f"Another routine memory {i}")
    
    # 4. Verify surprising memory got larger boost
    routine_memory = memory_core.get_memory_by_id(routine_id)
    surprise_memory = memory_core.get_memory_by_id(surprise_id)
    
    routine_boost = routine_memory.quickrecal_score - routine_initial_qr
    surprise_boost = surprise_memory.quickrecal_score - surprise_initial_qr
    
    assert surprise_boost > routine_boost
    assert surprise_surprise > routine_surprise
    
    # 5. Verify that surprising memory ranks higher in retrieval despite being older
    results = memory_core.retrieve_memories(query="test information", top_k=10)
    surprise_rank = next((i for i, m in enumerate(results) if m.id == surprise_id), None)
    routine_rank = next((i for i, m in enumerate(results) if m.id == routine_id), None)
    
    assert surprise_rank is not None
    assert routine_rank is not None
    assert surprise_rank < routine_rank  # Lower rank = higher position
\`\`\`

### Embedding Dimension Handling

\`\`\`python
@pytest.mark.integration
def test_embedding_dimension_handling():
    # Setup
    memory_core, neural_memory, cce = setup_integration_environment()
    
    # 1. Create embeddings of different dimensions
    embedding_384d = np.random.rand(384).astype(np.float32)  # Simulate 384-dimensional embedding
    embedding_768d = np.random.rand(768).astype(np.float32)  # Simulate 768-dimensional embedding
    
    # Normalize embeddings for realistic testing
    embedding_384d = embedding_384d / np.linalg.norm(embedding_384d)
    embedding_768d = embedding_768d / np.linalg.norm(embedding_768d)
    
    # 2. Process memories with these embeddings through CCE
    response_384d = cce.process_memory(
        content="384d test", 
        embedding=embedding_384d.tolist()
    )
    response_768d = cce.process_memory(
        content="768d test", 
        embedding=embedding_768d.tolist()
    )
    
    # 3. Verify both were processed without errors
    assert response_384d.status == "success"
    assert response_768d.status == "success"
    
    # 4. Verify Neural Memory received appropriate embeddings
    # This requires a method to check the projections used
    nm_history = neural_memory.get_processing_history()
    
    # 5. Verify retrieval works with mixed dimensions
    results_384d_query = memory_core.retrieve_memories(
        query_embedding=embedding_384d.tolist(),
        top_k=5
    )
    results_768d_query = memory_core.retrieve_memories(
        query_embedding=embedding_768d.tolist(),
        top_k=5
    )
    
    assert len(results_384d_query) > 0
    assert len(results_768d_query) > 0
    
    # 6. Verify that the 384d embedding retrieves the 384d memory and vice versa
    assert response_384d.memory_id in [m.id for m in results_384d_query]
    assert response_768d.memory_id in [m.id for m in results_768d_query]
\`\`\`

### Variant-Specific Tests

#### MAC Variant Test

\`\`\`python
@pytest.mark.integration
def test_mac_variant():
    # Setup with MAC variant enabled
    memory_core, neural_memory, cce = setup_integration_environment(variant="MAC")
    
    # 1. Process a sequence of related memories to build history
    base_content = "The quick brown fox jumps over the lazy dog."
    memories = []
    for i in range(5):
        modified = base_content.replace("fox", f"fox {i}")
        response = cce.process_memory(content=modified)
        memories.append(response.memory_id)
    
    # 2. Process a query memory that should trigger attention
    query_content = "A quick brown animal jumps over a lazy canine."
    query_response = cce.process_memory(
        content=query_content, 
        include_variant_metrics=True
    )
    
    # 3. Verify attention weights are distributed as expected
    assert "attention_weights" in query_response.variant_metrics
    weights = query_response.variant_metrics["attention_weights"]
    
    # Weights should sum to approximately 1.0
    assert abs(sum(weights) - 1.0) < 0.001
    
    # 4. Confirm attended output differs from raw output
    assert "raw_output" in query_response.variant_metrics
    assert "attended_output" in query_response.variant_metrics
    
    raw = np.array(query_response.variant_metrics["raw_output"])
    attended = np.array(query_response.variant_metrics["attended_output"])
    
    # Calculate cosine similarity between raw and attended outputs
    similarity = np.dot(raw, attended) / (np.linalg.norm(raw) * np.linalg.norm(attended))
    
    # Outputs should be similar but not identical
    assert 0.7 < similarity < 0.99
\`\`\`

#### MAG Variant Test

\`\`\`python
@pytest.mark.integration
def test_mag_variant():
    # Setup with MAG variant enabled
    memory_core, neural_memory, cce = setup_integration_environment(variant="MAG")
    
    # 1. Process a sequence of memories to build history
    for i in range(5):
        cce.process_memory(content=f"Memory {i} in the sequence.")
    
    # 2. Process a test memory with metrics collection
    response = cce.process_memory(
        content="Test memory for MAG variant.",
        include_variant_metrics=True
    )
    
    # 3. Verify external gate values are calculated
    assert "external_alpha_gate" in response.variant_metrics
    assert "external_theta_gate" in response.variant_metrics
    assert "external_eta_gate" in response.variant_metrics
    
    # 4. Verify gates are within valid ranges
    alpha = response.variant_metrics["external_alpha_gate"]
    theta = response.variant_metrics["external_theta_gate"]
    eta = response.variant_metrics["external_eta_gate"]
    
    assert 0 <= alpha <= 1
    assert theta > 0
    assert 0 <= eta <= 1
    
    # 5. Process a similar memory and check for lower alpha (less forgetting)
    similar_response = cce.process_memory(
        content="Very similar test memory for MAG variant.",
        include_variant_metrics=True
    )
    
    similar_alpha = similar_response.variant_metrics["external_alpha_gate"]
    assert similar_alpha < alpha  # Similar content should trigger less forgetting
\`\`\`

#### MAL Variant Test

\`\`\`python
@pytest.mark.integration
def test_mal_variant():
    # Setup with MAL variant enabled
    memory_core, neural_memory, cce = setup_integration_environment(variant="MAL")
    
    # 1. Process a sequence of memories to build history
    for i in range(5):
        cce.process_memory(content=f"MAL test memory {i}.")
    
    # 2. Process a test memory with metrics collection
    response = cce.process_memory(
        content="Final test memory for MAL variant.",
        include_variant_metrics=True
    )
    
    # 3. Verify value projection was modified
    assert "original_value_projection" in response.variant_metrics
    assert "modified_value_projection" in response.variant_metrics
    
    original_v = np.array(response.variant_metrics["original_value_projection"])
    modified_v = np.array(response.variant_metrics["modified_value_projection"])
    
    # 4. Verify the modification is meaningful but not extreme
    # Calculate cosine similarity between original and modified value projections
    similarity = np.dot(original_v, modified_v) / (np.linalg.norm(original_v) * np.linalg.norm(modified_v))
    
    # Should be similar but not identical
    assert 0.7 < similarity < 0.99
    
    # 5. Verify that the attention mechanism is working
    assert "attention_weights" in response.variant_metrics
    weights = response.variant_metrics["attention_weights"]
    assert abs(sum(weights) - 1.0) < 0.001  # Weights should sum to 1.0
\`\`\`

## Test Fixtures

### Mock Services

For isolated testing, mock implementations of each service can be used:

\`\`\`python
class MockMemoryCore:
    def __init__(self):
        self.memories = {}
        self.quickrecal_updates = []
    
    async def process_memory(self, content, embedding=None, metadata=None):
        memory_id = str(uuid.uuid4())
        self.memories[memory_id] = {
            "id": memory_id,
            "content": content,
            "embedding": embedding or np.random.rand(384).tolist(),
            "metadata": metadata or {},
            "quickrecal_score": 0.5
        }
        return {
            "memory_id": memory_id,
            "status": "success"
        }
    
    async def update_quickrecal_score(self, memory_id, delta):
        if memory_id in self.memories:
            self.memories[memory_id]["quickrecal_score"] += delta
            self.quickrecal_updates.append((memory_id, delta))
            return {"status": "success"}
        return {"status": "error", "message": "Memory not found"}
    
    async def get_memory_by_id(self, memory_id):
        return self.memories.get(memory_id)
    
    async def retrieve_memories(self, query=None, query_embedding=None, top_k=10):
        # Simple mock implementation
        memories = list(self.memories.values())[:top_k]
        return memories
\`\`\`

### Integration Test Fixture

A fixture that sets up all three services for integration testing:

\`\`\`python
@pytest.fixture
async def integrated_services(variant="NONE"):
    # Start all three services with test configuration
    memory_core_proc = await start_memory_core_server(test_config)
    neural_memory_proc = await start_neural_memory_server(test_config)
    
    # Set environment variable for Titans variant
    os.environ["TITANS_VARIANT"] = variant
    
    cce_proc = await start_cce_server(test_config)
    
    # Wait for services to be ready
    await wait_for_service("http://localhost:5010/api/health")
    await wait_for_service("http://localhost:5011/api/health")
    await wait_for_service("http://localhost:5012/api/health")
    
    # Yield the clients
    yield {
        "memory_core": MemoryCoreClient("http://localhost:5010"),
        "neural_memory": NeuralMemoryClient("http://localhost:5011"),
        "cce": CCEClient("http://localhost:5012")
    }
    
    # Cleanup
    for proc in [cce_proc, neural_memory_proc, memory_core_proc]:
        proc.terminate()
        await proc.wait()
\`\`\`

## Test Data

### Controlled Test Sequences

Predefined sequences of inputs with expected outputs for deterministic testing:

\`\`\`python
test_sequences = [
    # Sequence 1: Routine information
    {
        "name": "routine_sequence",
        "inputs": [
            "The weather today is sunny with a high of 75 degrees.",
            "Traffic was normal on the highway this morning.",
            "The stock market closed with modest gains yesterday."
        ],
        "expected": {
            "avg_surprise": 0.2,  # Low surprise expected
            "max_quickrecal_boost": 0.1  # Small boosts expected
        }
    },
    # Sequence 2: Novel information
    {
        "name": "novel_sequence",
        "inputs": [
            "Scientists discovered a new particle that defies known physics.",
            "An earthquake of magnitude 9.5 struck in the middle of the desert.",
            "A previously unknown species of large mammals was found in the Amazon."
        ],
        "expected": {
            "avg_surprise": 0.6,  # High surprise expected
            "max_quickrecal_boost": 0.4  # Large boosts expected
        }
    }
]
\`\`\`

## Continuous Integration

Integration tests should be run automatically as part of the CI/CD pipeline:

\`\`\`yaml
# Example GitHub Actions workflow
name: Integration Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  integration-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
        pip install -e .
    - name: Start services
      run: |
        python -m synthians.scripts.start_services --test-mode
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
\`\`\`

## Best Practices

1. **End-to-End Focus**: Integration tests should focus on end-to-end behavior, not implementation details.

2. **Isolation**: Each test should clean up after itself to prevent interference between tests.

3. **Fixtures Over Setup**: Use pytest fixtures to set up and tear down test environments consistently.

4. **Parameterization**: Use pytest's parameterize feature to test multiple configurations and variants.

5. **Logging**: Enable detailed logging during tests to make debugging easier:

\`\`\`python
@pytest.fixture(autouse=True)
def enable_test_logging():
    # Set up logging for tests
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    yield
    # Reset logging after test
\`\`\`

6. **Timing Sensitivity**: Include timeouts and retries to handle network-related timing issues in distributed services.

7. **Variant Coverage**: Ensure tests cover all variants and their specific behaviors.

```

# docs\testing\README.md

```md
# Testing Documentation

This directory contains documentation related to testing the Synthians cognitive system.

## Contents

* [Testing Improvements](./TESTING_IMPROVEMENTS.md): Details on recent enhancements to the test framework, including async improvements and fixture fixes.
* [Test Coverage](./test_coverage.md): **(Placeholder)** Analysis of current test coverage and areas that need additional tests.
* [Integration Testing](./integration_testing.md): **(Placeholder)** Guidelines for performing integration tests across the three services.

## Technical Details

* **Test Framework**: The project uses pytest with various plugins for testing, including async testing support.
* **Mock Services**: How mock implementations of services are used for isolated component testing.
* **Test Data**: How test data is generated and managed for consistent test execution.
* **Continuous Integration**: How tests are integrated into the development workflow.
* **Debugging Tests**: Tips for diagnosing and fixing test failures.

```

# docs\testing\test_coverage.md

```md
# Test Coverage Analysis for Synthians Cognitive System

**Author:** Lucidia Core Team  
**Date:** 2025-03-30  
**Status:** Implemented

## Overview

This document analyzes the current test coverage across the Synthians cognitive system and identifies areas that need additional testing. It serves as a guide for test development prioritization and tracking the overall quality of the test suite.

## Coverage Statistics

### Memory Core (`synthians_memory_core`)

| Component | Coverage % | Critical Paths Tested | Gaps |
|-----------|-----------|------------------------|------|
| SynthiansMemoryCore | 85% | process_new_memory, retrieve_memories | Assemblies, emotion_preprocessing |
| MemoryVectorIndex | 90% | search, add_with_ids, load, save | verify_integrity edge cases |
| UnifiedQuickRecallCalculator | 75% | calculate_quickrecal, basic factors | HPC-QR complex factors |
| GeometryManager | 95% | Validation, normalization, alignment | Hyperbolic geometry |
| EmotionalGatingService | 70% | Basic gating, filtering | Complex emotional patterns |
| MetadataSynthesizer | 80% | Basic enrichment | Custom metadata handlers |
| MemoryPersistence | 85% | Save/load operations | Concurrent access, recovery |

### Neural Memory Server (`synthians_trainer_server`)

| Component | Coverage % | Critical Paths Tested | Gaps |
|-----------|-----------|------------------------|------|
| NeuralMemoryModule | 80% | get_projections, update_memory, retrieve | Outer loop training |
| MemoryMLP | 85% | Forward pass, gradient calculation | Custom initialization |
| Server API | 90% | All endpoints basic functionality | Error handling edge cases |
| MetricsStore | 60% | Basic metrics collection | Aggregation, alerting |

### Context Cascade Engine (`orchestrator`)

| Component | Coverage % | Critical Paths Tested | Gaps |
|-----------|-----------|------------------------|------|
| ContextCascadeEngine | 75% | Basic orchestration, surprise feedback | Complex error recovery |
| TitansVariantBase | 80% | Basic functionality | - |
| MAC Implementation | 70% | Attention calculation | Tuning parameters |
| MAG Implementation | 65% | Gate calculation | Edge cases |
| MAL Implementation | 65% | Value modification | Edge cases |
| SequenceContextManager | 85% | History management | - |

## Test Types and Distribution

| Test Type | Count | Description |
|-----------|-------|-------------|
| Unit Tests | 527 | Tests for individual functions and classes |
| Component Tests | 143 | Tests for component interactions within a service |
| Integration Tests | 68 | Tests for cross-service interactions |
| End-to-End Tests | 12 | Tests for complete cognitive cycle flows |
| Performance Tests | 8 | Tests for performance benchmarks and regressions |

## Recent Testing Improvements

1. **Retrieval Pipeline Tests**:
   - Added tests with forced lower threshold (0.3) to validate improved recall sensitivity
   - Added tests for NaN/Inf validation in candidate memory retrieval
   - Added explicit threshold parameter tests

2. **Embedding Validation Tests**:
   - Added tests for detecting and handling NaN/Inf values
   - Added tests for vector alignment with dimension mismatches (384D vs 768D)
   - Added tests for zero vector substitution for invalid embeddings

3. **Metadata Enrichment Tests**:
   - Added tests for memory UUID in metadata
   - Added tests for content length tracking
   - Added tests for consistent metadata application

4. **Emotion Analysis Tests**:
   - Added tests for API-passed emotion data respect
   - Added tests for conditional emotion analysis

5. **Sequence Context Manager Tests**:
   - Added tests for context retrieval methods
   - Added tests for buffer overflow handling
   - Added validation for invalid embedding handling

## Priority Testing Gaps

### High Priority

1. **Titans Variant Integration Tests**:
   - Need dedicated tests for MAC, MAG, MAL effects
   - Need tests across service boundaries with these variants enabled
   - Need performance comparison tests

2. **Surprise Feedback Loop**:
   - Need comprehensive end-to-end tests of the boost mechanism
   - Need tests with varying surprise levels and expected QuickRecal boosts

3. **Embedding Dimension Handling**:
   - Need more extensive tests for mixed dimension handling throughout the system
   - Need stress tests with rapidly alternating dimensions

### Medium Priority

1. **Outer Loop Training**:
   - Tests for the Neural Memory's `/train_outer` endpoint
   - Tests for projection weight optimization

2. **MetricsStore**:
   - Tests for metrics aggregation and analysis
   - Tests for alert threshold detection

3. **Error Recovery**:
   - Tests for system behavior when one service fails
   - Tests for recovery mechanisms

### Low Priority

1. **Performance Benchmarks**:
   - Standard test suite for performance comparison across releases
   - Memory usage tracking tests

2. **Configuration Testing**:
   - Tests for all configuration parameters and combinations
   - Tests for environment variable overrides

## Test Development Roadmap

### Phase 1: Critical Path Coverage (Completed)

- Ensure all basic functionality has test coverage
- Focus on recent bug fixes having tests
- Establish basic integration test fixtures

### Phase 2: Variant Integration Tests (Current)

- Develop comprehensive tests for MAC, MAG, MAL variants
- Test surprise feedback loop end-to-end
- Test embedding dimension handling across service boundaries

### Phase 3: Edge Cases & Recovery (Next)

- Add tests for error conditions and recovery
- Stress tests for concurrent operations
- Boundary condition tests

### Phase 4: Performance & Benchmarking (Planned)

- Establish standard performance tests
- Create memory and CPU usage benchmarks
- Measure cognitive cycle latency under various conditions

## Test Coverage Tools and Reporting

### Code Coverage Tools

\`\`\`python
# Install coverage tools
pip install pytest-cov

# Run tests with coverage reporting
pytest --cov=synthians_memory_core --cov=orchestrator --cov-report=html tests/

# Generate HTML report
# Report will be available in htmlcov/ directory
\`\`\`

### Coverage Report Interpretation

The coverage report includes several key metrics:

1. **Statement Coverage**: Percentage of code statements executed during testing
2. **Branch Coverage**: Percentage of conditional branches (if/else) executed
3. **Path Coverage**: Percentage of possible execution paths tested

Code with high statement coverage but low branch/path coverage may indicate insufficient edge case testing.

### Continuous Integration Coverage

Our CI pipeline runs coverage analysis on each pull request and enforces:

- Minimum 80% statement coverage for new code
- No decrease in overall coverage
- Coverage reports uploaded as artifacts

## Mutation Testing

In addition to standard coverage, we employ mutation testing to evaluate test quality:

\`\`\`python
# Install mutation testing tool
pip install pytest-mutate

# Run mutation tests on a specific module
pytest-mutate synthians_memory_core/core/memory_core.py
\`\`\`

Mutation testing makes small modifications to the code ("mutants") and checks if tests detect the change. A high mutant kill rate indicates robust tests.

## Best Practices for Test Development

1. **Prioritize Critical Paths**: Focus on the most important functionalities first
2. **Test Edge Cases**: Include boundary conditions and error cases
3. **Isolate Tests**: Each test should be independent and deterministic
4. **Mock Dependencies**: Use mocks for external services to isolate test scope
5. **Test Real-World Scenarios**: Include tests that reflect actual usage patterns
6. **Keep Tests Fast**: Optimize slow tests to maintain developer productivity
7. **Parameterize Similar Tests**: Use parameterization for testing similar scenarios
8. **Document Test Purpose**: Include clear docstrings explaining what each test verifies

## Test Skip Policies

Tests may be skipped under specific conditions:

\`\`\`python
@pytest.mark.skipif(os.environ.get("SKIP_SLOW_TESTS") == "1", reason="Slow test")
def test_large_dataset_processing():
    # Test implementation
    pass
\`\`\`

Valid reasons for skipping tests:
- Environment-specific tests not applicable to all setups
- Very slow tests during rapid development cycles
- Tests for features behind feature flags

All skipped tests must have a clear explanation and should be periodically reviewed.

```

# docs\testing\TESTING_IMPROVEMENTS.md

```md
# Memory Core Testing Improvements

## Overview

This document outlines the improvements made to the testing infrastructure for the Synthians Memory Core component, addressing deprecation warnings and task cancellation issues that were previously occurring during test execution.

## Key Improvements

### 1. Test Fixture Enhancements

#### Memory Core Fixture Optimization

The `memory_core` fixture in `test_memory_core_updates.py` has been redesigned to prevent background tasks from starting during unit tests:

- Disabled persistence and decay background tasks by setting long intervals (`3600 * 24`)
- Implemented proper cleanup to ensure all resources are released after tests
- Added robust directory removal with retry logic to handle potential file system locking issues
- Replaced async locks with dummy versions for testing to prevent blocking during tests

\`\`\`python
# Example of the improved fixture configuration
core = SynthiansMemoryCore(
    config={
        'embedding_dim': 384,
        'storage_path': test_dir,
        'vector_index_type': 'L2',
        'use_gpu': False,
        # Disable background tasks for unit testing updates
        'persistence_interval': 3600 * 24,
        'decay_interval': 3600 * 24,
    }
)
\`\`\`

### 2. Background Task Management

#### Persistence Loop Improvements

The `_persistence_loop` method in `SynthiansMemoryCore` has been modified to prevent "no running event loop" errors during shutdown:

- Removed the final save attempt from the `finally` block that was causing errors during test teardown
- Improved shutdown sequence to ensure all tasks are properly cancelled

### 3. Event Loop Handling

#### Removal of Deprecated Fixtures

- Removed the custom `event_loop` fixture from `conftest.py` to eliminate deprecation warnings
- Updated to use pytest-asyncio's current recommended practices for async testing

### 4. Logging Enhancements

- Updated the `Logger` class to support both legacy and standard logging patterns
- Added better exception handling with `exc_info` support
- Made the logger more flexible with both context/message and standard logging calls

## Test Coverage

The following tests now run successfully without warnings or errors:

1. `test_get_memory_by_id` - Tests basic memory retrieval
2. `test_update_quickrecal_score` - Verifies QuickRecal score updates
3. `test_update_metadata` - Tests metadata update functionality
4. `test_update_invalid_fields` - Verifies error handling for invalid updates
5. `test_update_nonexistent_memory` - Tests error handling for non-existent memories
6. `test_update_persistence` - Verifies that updates are correctly persisted
7. `test_quickrecal_updated_timestamp` - Ensures timestamp update in metadata

## Remaining Considerations

### Configuration Options

The pytest-asyncio plugin still shows a configuration warning about `asyncio_default_fixture_loop_scope` being unset. This can be addressed by setting the configuration explicitly in `pytest.ini` or `conftest.py`:

\`\`\`python
# In conftest.py
def pytest_configure(config):
    config.option.asyncio_default_fixture_loop_scope = "function"
\`\`\`

Or in a pytest.ini file:

\`\`\`ini
[pytest]
asyncio_default_fixture_loop_scope = function
\`\`\`

## Integration with Bi-Hemispheric Architecture

These testing improvements ensure the reliability of the Memory Core component, which is crucial for the Bi-Hemispheric Cognitive Architecture as it:

1. Provides stable testing for the persistence mechanism used by the system
2. Ensures the memory update endpoints function correctly for the surprise feedback loop
3. Validates the QuickRecal scoring mechanism essential for memory relevance 

Together, these improvements maintain the integrity of the testing infrastructure while allowing for continued development of the cognitive architecture.

```

# docs\trainer\metrics_store.md

```md
# Metrics and Diagnostics

The `synthians_trainer_server.metrics_store.MetricsStore` class is responsible for collecting and storing operational statistics from the Neural Memory server.

## Purpose

Tracking metrics allows for:

*   **Monitoring:** Observing the server's performance and health (e.g., request counts, processing times).
*   **Debugging:** Identifying bottlenecks or issues.
*   **Analysis:** Understanding the behavior of the neural memory model (e.g., average loss, gradient norms).

## Key Component: `MetricsStore`

*   **Functionality:**
    *   Provides methods to increment counters (`increment_request_count`), record timings (`record_processing_time`), and store specific values (`record_loss`, `record_grad_norm`).
    *   Stores metrics in memory, often using dictionaries or specialized data structures.
    *   Periodically calculates averages or aggregates (e.g., average processing time over the last minute).
*   **Integration:**
    *   Instantiated within the main FastAPI application.
    *   Accessed by endpoint handlers to record metrics after processing requests (e.g., `/update_memory`, `/retrieve`).

## Collected Metrics (Examples)

*   Total requests for each endpoint (`/update_memory`, `/retrieve`).
*   Average processing time for each endpoint.
*   Average loss (`ℓ`) calculated during `/update_memory` calls.
*   Average gradient norm (`∇ℓ`) calculated during `/update_memory` calls.
*   Number of successful updates vs. errors.
*   Current memory usage or other system-level stats (potentially).

## Diagnostic Endpoints

The server typically exposes endpoints to retrieve these collected metrics:

*   `/metrics`: Often returns metrics in a standard format (like Prometheus exposition format) for scraping by monitoring systems.
*   `/status` or `/health`: Provides a basic health check and potentially key operational statistics.
*   `/diagnostics` (Optional): Might return a more detailed, human-readable summary of the metrics.

## Importance

Monitoring and diagnostics are crucial for maintaining a reliable and performant service, especially for a component like the Neural Memory that undergoes continuous adaptation.

```

# docs\trainer\neural_memory.md

```md
# Neural Memory Module (`NeuralMemoryModule`)

The core of the `synthians_trainer_server` is the `NeuralMemoryModule`, a TensorFlow/Keras model that implements an adaptive associative memory.

## Concept: Test-Time Learning

Unlike traditional models trained offline, this module adapts its internal weights (`M`) *during operation* based on the stream of incoming data. This allows it to continuously learn associations and adapt to changing patterns without requiring explicit retraining phases.

The implementation is heavily inspired by the concepts presented in the "Transformers are Meta-Learners" (Titans) paper, particularly focusing on the associative memory aspect.

## Architecture

\`\`\`mermaid
graph TD
    Input(Input Embedding x_t) --> ProjK(Proj K - WK)
    Input --> ProjV(Proj V - WV)
    Input --> ProjQ(Proj Q - WQ)

    ProjK --> Key(Key k_t)
    ProjV --> Value(Value v_t)
    ProjQ --> Query(Query q_t)

    Key --> Memory(Memory M)
    Query --> Memory

    subgraph "Update (/update_memory)"
        Memory -- Recall --> PredictedValue(Predicted v_hat)
        Value --> LossFn(Loss ||v_t - v_hat||²)
        LossFn --> Gradient(∇ℓ w.r.t. M)
        Gradient --> Momentum(Momentum S_t)
        Momentum --> UpdateM(Update M_t)
        Gates(Gates α, θ, η) --> Momentum
        Gates --> UpdateM
    end

    subgraph "Retrieve (/retrieve)"
        Memory -- Recall --> RetrievedValue(Retrieved v_ret)
    end
\`\`\`

**Key Components:**

1.  **Projection Layers (WK, WV, WQ):** Linear layers that project the input embedding `x_t` into different spaces to create the Key (`k_t`), Value (`v_t`), and Query (`q_t`) vectors.
2.  **Memory Network (M):** The core associative memory, typically implemented as a Multi-Layer Perceptron (MLP). Its weights are the parameters that are continuously updated.
3.  **Gates (α, θ, η):** Learnable or fixed parameters controlling the learning dynamics:
    *   `α`: Forget Rate Gate (how much of the old memory `M_{t-1}` to keep).
    *   `θ`: Inner Learning Rate Gate (how much influence the current gradient `∇ℓ` has).
    *   `η`: Momentum Decay Gate (how much momentum `S_{t-1}` persists).
4.  **Momentum State (S):** Tracks the recent history of gradient updates, helping to stabilize learning.

## Operations

### 1. Update (`/update_memory` endpoint)

*   **Input:** `embedding` (representing `x_t`).
*   **Process:**
    1.  Calculate `k_t` and `v_t` using `WK` and `WV`.
    2.  Recall the predicted value `pred_v = M_{t-1}(k_t)`. Pass the *current* key through the *current* memory `M`.
    3.  Calculate the loss `ℓ = ||pred_v - v_t||² / 2` (associative error).
    4.  Compute the gradient `∇ℓ` of the loss with respect to the weights of `M`.
    5.  Update the momentum: `S_t = η_t * S_{t-1} - θ_t * ∇ℓ`.
    6.  Update the memory weights: `M_t = (1 - α_t) * M_{t-1} + S_t`.
*   **Output:** `loss` and `grad_norm` (surprise metrics).

### 2. Retrieve (`/retrieve` endpoint)

*   **Input:** `query_embedding` (representing `x_t`).
*   **Process:**
    1.  Calculate `q_t` using `WQ`.
    2.  Pass the query `q_t` through the *current* memory `M_t`: `retrieved_embedding = M_t(q_t)`.
    3.  This uses the memory in a feed-forward manner **without** updating its weights.
*   **Output:** `retrieved_embedding`.

## Importance

This module allows the system to:

*   Form associations between concepts (embeddings) over time.
*   Adapt its internal representations based on ongoing experience.
*   Provide a mechanism for generating surprise signals (`loss`, `grad_norm`) that indicate novel or unexpected information, which can be used to influence other parts of the system (like QuickRecall scoring).

```

# docs\trainer\README.md

```md
# Neural Memory Server Documentation

This directory provides documentation for the `synthians_trainer_server` package, which implements the adaptive associative memory component of the Synthians architecture.

## Contents

*   [Neural Memory Module](./neural_memory.md): Describes the core TensorFlow/Keras model (`NeuralMemoryModule`) implementing test-time learning based on the Titans paper, including its internal structure (Memory MLP `M`, projections, gates) and update mechanisms.
*   [Metrics & Diagnostics](./metrics_store.md): Explains the `MetricsStore` used for collecting operational statistics and the diagnostic endpoints provided by the server.

Refer to the main [Architecture](../ARCHITECTURE.md) and [Component Guide](../COMPONENT_GUIDE.md) for how this server fits into the overall system and interacts with the Context Cascade Engine.

```

# docs\trainer\surprise_detector.md

```md
# Surprise Detection

*This is a placeholder document for detailed documentation on the SurpriseDetector component.*

## Overview

The `SurpriseDetector` is responsible for quantifying the level of surprise or unexpectedness in the Neural Memory's predictions. It implements the principle that **"Surprise signals significance"** by measuring how much a new input deviates from the system's expectations based on prior learning.

## Core Functionality

### Surprise Measurement

The `SurpriseDetector` calculates surprise metrics based on the difference between predicted and actual values:

- **Loss-based Surprise**: Measures the magnitude of prediction error
- **Gradient-based Surprise**: Measures the magnitude of required weight updates
- **Distribution-based Surprise**: Compares current metrics to historical distributions

### Primary Metrics

#### Loss Value

The loss value represents the direct prediction error:

\`\`\`python
def calculate_loss(predicted_value, actual_value):
    """Calculate L2 loss between prediction and actual value."""
    return 0.5 * np.sum((predicted_value - actual_value) ** 2)
\`\`\`

Higher loss values indicate greater deviation from expectations, suggesting that the input contains information that the system had not adequately learned to predict.

#### Gradient Norm

The gradient norm measures the magnitude of the update needed to accommodate the new information:

\`\`\`python
def calculate_gradient_norm(gradient):
    """Calculate the L2 norm of the gradient."""
    return np.linalg.norm(gradient)
\`\`\`

Larger gradient norms indicate that more significant weight changes are needed to incorporate the new information, suggesting higher surprise or novelty.

### Normalization & Calibration

Raw surprise metrics can vary widely in scale, so the `SurpriseDetector` normalizes and calibrates them:

- **Historical Calibration**: Comparing current metrics to a moving window of recent values
- **Z-score Normalization**: Expressing surprise in terms of standard deviations from the mean
- **Min-Max Scaling**: Mapping surprise values to a fixed range (e.g., 0-1)

## Integration with QuickRecal Boost

The surprise metrics are used by the Context Cascade Engine to calculate QuickRecal boosts:

\`\`\`python
# Example of how surprise metrics are converted to QuickRecal boosts
def calculate_boost(loss, grad_norm, boost_factor=0.1):
    # Combine loss and gradient norm, with optional weighting
    combined_surprise = loss + 0.5 * grad_norm
    
    # Scale to appropriate boost range
    boost = boost_factor * combined_surprise
    
    # Optional: Apply non-linear transformation (e.g., sigmoid)
    # boost = sigmoid(boost) * max_boost
    
    return boost
\`\`\`

These boosts are applied to the original memory's QuickRecal score in the Memory Core, reinforcing memories that contained surprising or novel information.

## Technical Implementation

The `SurpriseDetector` functionality is primarily implemented within the Neural Memory Server's `/update_memory` endpoint, which:

1. Calculates the predicted value based on the current memory state
2. Computes the loss between the prediction and the actual value
3. Calculates the gradient of the loss with respect to the memory weights
4. Measures the gradient norm
5. Returns both the loss and gradient norm as surprise metrics

## Configuration Options

- `surprise_normalization`: Method for normalizing surprise metrics ("raw", "z-score", "min-max")
- `history_window_size`: Number of recent updates to consider for historical calibration
- `outlier_threshold`: Z-score threshold above which metrics are considered outliers
- `surprise_minimum_threshold`: Minimum value for surprise to be considered significant

```

# emotion_analyzer.py

```py
import asyncio
import os
import time
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
import numpy as np

from .custom_logger import logger

class EmotionAnalyzer:
    """
    Handles emotion analysis using a dual-mode approach:
    1. Primary: RoBERTa-based GoEmotions transformer model
    2. Fallback: Lightweight keyword-based approach
    
    Ensures consistent emotion detection structure regardless of the mode used.
    """
    
    def __init__(self, model_path: Optional[str] = None, device: Optional[str] = None):
        """
        Initialize the EmotionAnalyzer with a transformer model if available.
        
        Args:
            model_path: Path to the emotion model, if None will check for environment variable
            device: Device to use for inference (cuda, cpu). If None, will auto-detect.
        """
        # Auto-detect device if not specified
        if device is None:
            # Check for CUDA availability at runtime - default to CPU if not available
            try:
                import torch
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
                logger.info("EmotionAnalyzer", f"Auto-detected device: {self.device}")
            except ImportError:
                self.device = "cpu"
                logger.info("EmotionAnalyzer", "Torch not available, defaulting to CPU device")
        else:
            self.device = device
            
        # Model path can come from multiple sources with increasing precedence:
        # 1. Default path relative to the project
        # 2. Environment variable EMOTION_MODEL_PATH
        # 3. Explicitly provided model_path parameter
        default_paths = [
            "models/roberta-base-go_emotions",  # Default relative path
            "/app/models/emotion",             # Common Docker mount point
            "/data/models/emotion",            # Alternative Docker volume
        ]
        
        # Determine the model path with proper precedence
        env_path = os.environ.get("EMOTION_MODEL_PATH")
        self.model_path = model_path or env_path or next((p for p in default_paths if os.path.exists(p)), default_paths[0])
        logger.info("EmotionAnalyzer", f"Using model path: {self.model_path}")
        
        # Model will be loaded on first use, not during initialization
        self.model = None
        self.model_loaded = False
        self.model_load_attempted = False
        
        # Track analysis stats
        self.stats = {
            "primary_calls": 0,
            "fallback_calls": 0,
            "errors": 0,
            "avg_time_ms": 0,
            "total_calls": 0
        }
    
    def _initialize_model(self):
        """
        Load the transformer-based emotion model if available.
        Returns True if model loaded successfully, False otherwise.
        """
        # Skip if we've already attempted to load and failed
        if self.model_loaded:
            return True
            
        if self.model_load_attempted and not self.model_loaded:
            logger.debug("EmotionAnalyzer", "Previous model load attempt failed, using fallback")
            return False
            
        self.model_load_attempted = True
        
        try:
            # Only import transformers if we're actually going to use it
            from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
            import torch
            
            # Check both if the path exists AND if it contains expected model files
            path_exists = os.path.exists(self.model_path)
            model_files_exist = False
            
            if path_exists:
                # Check for key files that indicate a Hugging Face model
                expected_files = ['config.json', 'pytorch_model.bin']
                model_files_exist = any(os.path.exists(os.path.join(self.model_path, f)) for f in expected_files)
                
            # Log what we found about the model path
            if path_exists and model_files_exist:
                logger.info("EmotionAnalyzer", f"Found model files at {self.model_path}")
            elif path_exists:
                logger.warning("EmotionAnalyzer", f"Path {self.model_path} exists but doesn't contain model files")
            else:
                logger.warning("EmotionAnalyzer", f"Model path {self.model_path} does not exist")
            
            # If model files exist locally, use them; otherwise try to download from Hugging Face Hub
            if path_exists and model_files_exist:
                # Load from local path
                logger.info("EmotionAnalyzer", f"Loading local model from {self.model_path}")
                tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only=True)
                model = AutoModelForSequenceClassification.from_pretrained(self.model_path, local_files_only=True)
            else:
                # Try to download model from Hugging Face Hub
                try:
                    logger.info("EmotionAnalyzer", "Local model not found, downloading from Hugging Face Hub")
                    # Use a fallback model ID - GoEmotions on Hugging Face
                    model_id = "joeddav/distilbert-base-uncased-go-emotions-student"
                    tokenizer = AutoTokenizer.from_pretrained(model_id)
                    model = AutoModelForSequenceClassification.from_pretrained(model_id)
                    
                    # Save the model to the specified path for future use
                    if path_exists:
                        logger.info("EmotionAnalyzer", f"Saving downloaded model to {self.model_path}")
                        model.save_pretrained(self.model_path)
                        tokenizer.save_pretrained(self.model_path)
                except Exception as download_error:
                    logger.error("EmotionAnalyzer", f"Error downloading model: {str(download_error)}")
                    return False
            
            # Create the pipeline with the loaded model
            self.model = pipeline(
                "text-classification",
                model=model,
                tokenizer=tokenizer,
                device=0 if self.device == "cuda" else -1,
                top_k=None  # Return all emotion scores
            )
            
            self.model_loaded = True
            logger.info("EmotionAnalyzer", "Emotion model loaded successfully")
            return True
            
        except Exception as e:
            logger.error("EmotionAnalyzer", f"Error loading emotion model: {str(e)}")
            self.model = None
            self.model_loaded = False
            self.stats["errors"] += 1
            return False
    
    async def analyze(self, text: str) -> Dict[str, Any]:
        """
        Analyze emotions in the given text.
        Attempts to use the transformer model first, and falls back to keyword analysis if needed.
        
        Args:
            text: The text to analyze
            
        Returns:
            Dict containing emotions and the dominant emotion
        """
        start_time = time.time()
        
        try:
            # Try to load the model on first use if not already loaded
            if not self.model and not self.model_load_attempted:
                logger.info("EmotionAnalyzer", "First-time model loading during analyze call")
                model_loaded = self._initialize_model()
                if model_loaded:
                    logger.info("EmotionAnalyzer", "Successfully loaded model on first use")
                else:
                    logger.warning("EmotionAnalyzer", "Failed to load model on first use, falling back to keywords")
            
            # Attempt primary analysis if model is available
            if self.model is not None:
                logger.debug("EmotionAnalyzer", "Using transformer-based analysis")
                result = await self._analyze_with_transformer(text)
                self.stats["primary_calls"] += 1
            else:
                # Fall back to keyword analysis
                logger.debug("EmotionAnalyzer", "Using keyword-based analysis fallback")
                result = await self._analyze_with_keywords(text)
                self.stats["fallback_calls"] += 1
            
            # Update stats
            elapsed_ms = (time.time() - start_time) * 1000
            self.stats["avg_time_ms"] = (
                (self.stats["avg_time_ms"] * (self.stats["primary_calls"] + self.stats["fallback_calls"] - 1) + elapsed_ms) /
                (self.stats["primary_calls"] + self.stats["fallback_calls"])
            )
            self.stats["total_calls"] += 1
            
            return result
        except Exception as e:
            elapsed_ms = (time.time() - start_time) * 1000
            logger.error("EmotionAnalyzer", f"Error in emotion analysis: {str(e)}")
            self.stats["errors"] += 1
            
            # Always return a valid response, even in case of errors
            return {
                "dominant_emotion": "neutral",
                "emotions": {"neutral": 1.0},
                "error": str(e)
            }
    
    async def _analyze_with_transformer(self, text: str) -> Dict[str, Any]:
        """
        Analyze emotions using the transformer model.
        """
        # Execute the model in a thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        raw_results = await loop.run_in_executor(None, lambda: self.model(text))
        
        # Convert the transformer output format to our expected format
        # The model returns a list of dictionaries with 'label' and 'score'
        emotion_results = {}
        for result_list in raw_results:
            for item in result_list:
                label = item['label']
                score = float(item['score'])  # Ensure score is float
                emotion_results[label] = score
        
        # Find the dominant emotion based on score
        if emotion_results:
            dominant_emotion = max(emotion_results.items(), key=lambda x: x[1])[0]
        else:
            dominant_emotion = "neutral"
            emotion_results["neutral"] = 0.5
        
        return {
            "emotions": emotion_results,
            "dominant_emotion": dominant_emotion
        }
    
    async def _analyze_with_keywords(self, text: str) -> Dict[str, Any]:
        """
        Fallback emotion analysis using keyword matching.
        Much less accurate but works without any models.
        """
        # Simple keyword-based emotion detection
        emotion_keywords = {
            "joy": ["happy", "joy", "delighted", "glad", "pleased", "excited", "thrilled"],
            "sadness": ["sad", "unhappy", "depressed", "down", "miserable", "upset", "disappointed"],
            "anger": ["angry", "mad", "furious", "annoyed", "irritated", "enraged", "frustrated"],
            "fear": ["afraid", "scared", "frightened", "terrified", "anxious", "worried", "nervous"],
            "surprise": ["surprised", "amazed", "astonished", "shocked", "stunned"],
            "disgust": ["disgusted", "repulsed", "revolted", "sickened"],
            "neutral": ["ok", "fine", "neutral", "average", "normal"]
        }
        
        text = text.lower()
        emotion_scores = {emotion: 0.1 for emotion in emotion_keywords}  # Base score
        
        # Simple keyword matching
        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    emotion_scores[emotion] += 0.15  # Increment score for each match
        
        # Normalize scores
        max_score = max(emotion_scores.values())
        if max_score > 0.1:  # If we found any matches
            for emotion in emotion_scores:
                emotion_scores[emotion] = min(emotion_scores[emotion] / max_score, 1.0)
        else:
            # If no matches, default to neutral
            emotion_scores["neutral"] = 0.5
        
        # Find dominant emotion
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        return {
            "emotions": emotion_scores,
            "dominant_emotion": dominant_emotion
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get usage statistics for the emotion analyzer.
        """
        total_calls = self.stats["primary_calls"] + self.stats["fallback_calls"]
        
        return {
            "total_calls": self.stats["total_calls"],
            "primary_calls": self.stats["primary_calls"],
            "fallback_calls": self.stats["fallback_calls"],
            "primary_percentage": (self.stats["primary_calls"] / max(total_calls, 1)) * 100,
            "fallback_percentage": (self.stats["fallback_calls"] / max(total_calls, 1)) * 100,
            "errors": self.stats["errors"],
            "avg_time_ms": round(self.stats["avg_time_ms"], 2),
            "model_loaded": self.model_loaded,
            "model_path": self.model_path
        }

```

# emotional_intelligence.py

```py
# synthians_memory_core/emotional_intelligence.py

import logging
import numpy as np
from typing import Dict, List, Optional, Any

from .custom_logger import logger # Use the shared custom logger
from .emotion_analyzer import EmotionAnalyzer as _EmotionAnalyzer  # Import with alias to avoid name conflicts

# Maintain backward compatibility by re-exporting the class
# This prevents import errors in existing code that imports from this module
class EmotionalAnalyzer(_EmotionAnalyzer):
    """Re-export of the EmotionAnalyzer class from emotion_analyzer.py for backward compatibility."""
    pass

# Export EmotionalAnalyzer for backward compatibility
__all__ = ['EmotionalAnalyzer', 'EmotionalGatingService']

# NOTE: The EmotionalAnalyzer class implementation has been moved to emotion_analyzer.py
# This file now only contains the EmotionalGatingService class and a compatibility wrapper

class EmotionalGatingService:
    """Applies emotional gating to memory retrieval."""
    def __init__(self, emotion_analyzer, config: Optional[Dict] = None):
        """Initialize the emotional gating service.
        
        Args:
            emotion_analyzer: An instance of EmotionAnalyzer from emotion_analyzer.py
            config: Configuration parameters for the gating service
        """
        self.emotion_analyzer = emotion_analyzer
        self.config = config or {}
        
        # Configuration with defaults
        self.emotion_weight = self.config.get('emotional_weight', 0.3)
        self.memory_gate_min_factor = self.config.get('gate_min_factor', 0.5)
        self.cognitive_bias = self.config.get('cognitive_bias', 0.2)
        
        logger.info("EmotionalGatingService", "Initialized with config", {
            "emotion_weight": self.emotion_weight,
            "gate_min_factor": self.memory_gate_min_factor,
            "cognitive_bias": self.cognitive_bias,
            "has_analyzer": self.emotion_analyzer is not None
        })

        # Simplified compatibility - similar emotions are compatible
        self.emotion_compatibility = {
            "joy": {"joy", "excitement", "gratitude", "satisfaction", "content"},
            "sadness": {"sadness", "grief", "disappointment", "melancholy"},
            "anger": {"anger", "frustration", "irritation"},
            "fear": {"fear", "anxiety", "nervousness"},
            "surprise": {"surprise", "amazement", "astonishment"},
            "disgust": {"disgust", "displeasure"},
            "trust": {"trust", "respect", "admiration"},
            "neutral": {"neutral", "calm", "focused"}
        }
        # Add reverse compatibility and self-compatibility
        for emotion, compatible_set in list(self.emotion_compatibility.items()):
             compatible_set.add(emotion) # Self-compatible
             for compatible_emotion in compatible_set:
                  if compatible_emotion not in self.emotion_compatibility:
                       self.emotion_compatibility[compatible_emotion] = set()
                  self.emotion_compatibility[compatible_emotion].add(emotion)
        # Ensure neutral is compatible with everything
        all_emotions = set(self.emotion_compatibility.keys())
        self.emotion_compatibility["neutral"] = all_emotions
        for emotion in all_emotions:
             self.emotion_compatibility[emotion].add("neutral")

    async def gate_memories(self,
                           memories: List[Dict[str, Any]],
                           user_emotion: Optional[Dict[str, Any]],
                           cognitive_load: float = 0.5) -> List[Dict[str, Any]]:
        """Filter and re-rank memories based on emotional context."""
        if not memories or user_emotion is None:
            return memories # No gating if no user emotion provided

        user_dominant = user_emotion.get("dominant_emotion", "neutral")
        user_valence = user_emotion.get("sentiment_value", 0.0)
        user_intensity = user_emotion.get("intensity", 0.0)

        gated_memories = []
        for memory in memories:
            mem_emotion_context = memory.get("metadata", {}).get("emotional_context")
            if not mem_emotion_context:
                 # If no emotion data, assign neutral resonance
                 memory["emotional_resonance"] = 0.5
                 gated_memories.append(memory)
                 continue

            memory_dominant = mem_emotion_context.get("dominant_emotion", "neutral")
            memory_valence = mem_emotion_context.get("sentiment_value", 0.0)
            memory_intensity = mem_emotion_context.get("intensity", 0.0)

            # 1. Cognitive Defense (Simplified)
            if self.config.get('cognitive_defense_enabled', True) and user_valence < -0.5 and user_intensity > 0.6:
                 # If user is highly negative, filter out extremely negative memories
                 if memory_valence < -0.7 and memory_intensity > 0.8:
                      logger.debug("EmotionalGatingService", "Cognitive defense filtered out negative memory", {"memory_id": memory.get("id")})
                      continue # Skip this memory

            # 2. Calculate Emotional Resonance
            # Compatibility score (1 if compatible, 0 if not, 0.5 if neutral involved)
            user_compatibles = self.emotion_compatibility.get(user_dominant, set())
            mem_compatibles = self.emotion_compatibility.get(memory_dominant, set())

            if user_dominant == "neutral" or memory_dominant == "neutral":
                 emotion_compatibility = 0.7 # Neutral is somewhat compatible with everything
            elif memory_dominant in user_compatibles:
                 emotion_compatibility = 1.0 # Direct or similar emotion
            elif user_compatibles.intersection(mem_compatibles):
                 emotion_compatibility = 0.6 # Related emotions
            else:
                 emotion_compatibility = 0.1 # Unrelated emotions

            # Valence alignment (1 for same sign, 0 for opposite, 0.5 if one is neutral)
            if (user_valence > 0.1 and memory_valence > 0.1) or \
               (user_valence < -0.1 and memory_valence < -0.1):
                 valence_alignment = 1.0
            elif (user_valence > 0.1 and memory_valence < -0.1) or \
                 (user_valence < -0.1 and memory_valence > 0.1):
                 valence_alignment = 0.0
            else: # One or both are neutral
                 valence_alignment = 0.5

            # Combined resonance score
            emotional_resonance = (emotion_compatibility * 0.6 + valence_alignment * 0.4)
            memory["emotional_resonance"] = emotional_resonance

            # 3. Cognitive Load Adjustment (Simplified)
            # Higher load makes less resonant memories less likely
            if cognitive_load > 0.7 and emotional_resonance < (0.4 + 0.4 * cognitive_load):
                logger.debug("EmotionalGatingService", f"Memory filtered by high cognitive load ({cognitive_load})", {"memory_id": memory.get("id")})
                continue # Skip less resonant memories under high load

            gated_memories.append(memory)

        # 4. Re-rank based on combined score
        weight = self.emotion_weight
        for memory in gated_memories:
             original_score = memory.get("relevance_score", 0.0) # Use relevance_score if available
             resonance = memory.get("emotional_resonance", 0.5)
             memory["final_score"] = (1 - weight) * original_score + weight * resonance

        gated_memories.sort(key=lambda x: x["final_score"], reverse=True)

        logger.info("EmotionalGatingService", f"Gated memories from {len(memories)} to {len(gated_memories)}", {"user_emotion": user_dominant})
        return gated_memories

```

# geometry_manager.py

```py
# synthians_memory_core/geometry_manager.py

import numpy as np
import torch
import math
from enum import Enum
from typing import Optional, Tuple, List, Union, Dict, Any

from .custom_logger import logger # Use the shared custom logger

class GeometryType(Enum):
    EUCLIDEAN = "euclidean"
    HYPERBOLIC = "hyperbolic"
    SPHERICAL = "spherical"
    MIXED = "mixed"

class GeometryManager:
    """Centralized handling of embedding geometry, transformations, and calculations."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = {
            'embedding_dim': 768,
            'geometry_type': GeometryType.EUCLIDEAN,
            'curvature': -1.0, # Relevant for Hyperbolic/Spherical
            'alignment_strategy': 'truncate', # or 'pad' or 'project'
            'normalization_enabled': True,
             **(config or {})
        }
        # Ensure geometry_type is enum
        if isinstance(self.config['geometry_type'], str):
            try:
                self.config['geometry_type'] = GeometryType(self.config['geometry_type'].lower())
            except ValueError:
                 logger.warning("GeometryManager", f"Invalid geometry type {self.config['geometry_type']}, defaulting to EUCLIDEAN.")
                 self.config['geometry_type'] = GeometryType.EUCLIDEAN

        # Warning counters
        self.dim_mismatch_warnings = 0
        self.max_dim_mismatch_warnings = 10
        self.nan_inf_warnings = 0
        self.max_nan_inf_warnings = 10

        logger.info("GeometryManager", "Initialized", self.config)

    def _validate_vector(self, vector: Union[np.ndarray, List[float], torch.Tensor], name: str = "Vector") -> Optional[np.ndarray]:
        """Validate and convert vector to numpy array."""
        if vector is None:
            logger.warning("GeometryManager", f"{name} is None")
            return None

        if isinstance(vector, list):
            vector = np.array(vector, dtype=np.float32)
        elif isinstance(vector, torch.Tensor):
            vector = vector.detach().cpu().numpy().astype(np.float32)
        elif not isinstance(vector, np.ndarray):
            logger.warning("GeometryManager", f"Unsupported vector type {type(vector)} for {name}, attempting conversion.")
            try:
                vector = np.array(vector, dtype=np.float32)
            except Exception as e:
                 logger.error("GeometryManager", f"Failed to convert {name} to numpy array", {"error": str(e)})
                 return None

        # Check for NaN/Inf
        if np.isnan(vector).any() or np.isinf(vector).any():
            if self.nan_inf_warnings < self.max_nan_inf_warnings:
                 logger.warning("GeometryManager", f"{name} contains NaN or Inf values. Replacing with zeros.")
                 self.nan_inf_warnings += 1
                 if self.nan_inf_warnings == self.max_nan_inf_warnings:
                      logger.warning("GeometryManager", "Max NaN/Inf warnings reached, suppressing further warnings.")
            return np.zeros_like(vector) # Replace invalid vector with zeros

        return vector

    def align_vectors(self, vec_a: np.ndarray, vec_b: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Align two vectors to the configured embedding dimension."""
        # Validate inputs
        vec_a = self._validate_vector(vec_a, "Vector A")
        if vec_a is None:
            vec_a = np.zeros(self.config['embedding_dim'], dtype=np.float32)
            
        vec_b = self._validate_vector(vec_b, "Vector B")
        if vec_b is None:
            vec_b = np.zeros(self.config['embedding_dim'], dtype=np.float32)
            
        target_dim = self.config['embedding_dim']
        dim_a = vec_a.shape[0]
        dim_b = vec_b.shape[0]

        aligned_a = vec_a
        aligned_b = vec_b

        strategy = self.config['alignment_strategy']

        if dim_a != target_dim:
            if self.dim_mismatch_warnings < self.max_dim_mismatch_warnings:
                 logger.warning("GeometryManager", f"Vector A dimension mismatch: got {dim_a}, expected {target_dim}. Applying strategy: {strategy}")
                 self.dim_mismatch_warnings += 1
                 if self.dim_mismatch_warnings == self.max_dim_mismatch_warnings:
                      logger.warning("GeometryManager", "Max dimension mismatch warnings reached.")

            if strategy == 'pad':
                if dim_a < target_dim:
                    aligned_a = np.pad(vec_a, (0, target_dim - dim_a))
                else: # Truncate if padding isn't the strategy and dim > target
                    aligned_a = vec_a[:target_dim]
            elif strategy == 'truncate':
                if dim_a > target_dim:
                    aligned_a = vec_a[:target_dim]
                else: # Pad if truncating isn't the strategy and dim < target
                     aligned_a = np.pad(vec_a, (0, target_dim - dim_a))
            # Add 'project' strategy later if needed
            else: # Default to truncate/pad based on relative size
                if dim_a > target_dim: aligned_a = vec_a[:target_dim]
                else: aligned_a = np.pad(vec_a, (0, target_dim - dim_a))


        if dim_b != target_dim:
             if self.dim_mismatch_warnings < self.max_dim_mismatch_warnings:
                 logger.warning("GeometryManager", f"Vector B dimension mismatch: got {dim_b}, expected {target_dim}. Applying strategy: {strategy}")
                 # No warning count increment here, handled by vec_a check

             if strategy == 'pad':
                if dim_b < target_dim:
                    aligned_b = np.pad(vec_b, (0, target_dim - dim_b))
                else: aligned_b = vec_b[:target_dim]
             elif strategy == 'truncate':
                 if dim_b > target_dim: aligned_b = vec_b[:target_dim]
                 else: aligned_b = np.pad(vec_b, (0, target_dim - dim_b))
             else: # Default
                 if dim_b > target_dim: aligned_b = vec_b[:target_dim]
                 else: aligned_b = np.pad(vec_b, (0, target_dim - dim_b))

        return aligned_a, aligned_b

    def _align_vectors(self, vec_a: np.ndarray, vec_b: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Backward compatibility method that forwards to align_vectors.
        
        Several components are calling this method with underscore, but the implementation
        is named 'align_vectors' (without underscore). This method ensures backward compatibility.
        """
        return self.align_vectors(vec_a, vec_b)

    def normalize_embedding(self, vector: np.ndarray) -> np.ndarray:
        """L2 normalize a vector."""
        # Ensure input is numpy array
        vector = self._validate_vector(vector, "Vector to Normalize")
        if vector is None:
            # Return zero vector of appropriate dimension if validation failed
            return np.zeros(self.config.get('embedding_dim', 768), dtype=np.float32)

        if not self.config['normalization_enabled']:
             return vector
        norm = np.linalg.norm(vector)
        if norm < 1e-9:
            logger.debug("GeometryManager", "normalize_embedding received zero vector, returning as is.")
            return vector
        return vector / norm

    def _normalize(self, vector: np.ndarray) -> np.ndarray:
        """Backward compatibility method that forwards to normalize_embedding.
        
        Several components are calling this method with underscore, but the implementation
        is named 'normalize_embedding' (without underscore). This method ensures backward compatibility.
        """
        # Ensure vector is numpy array before calling
        validated_vector = self._validate_vector(vector, "Vector for _normalize")
        if validated_vector is None:
            # Return zero vector if validation fails
            return np.zeros(self.config.get('embedding_dim', 768), dtype=np.float32)
        return self.normalize_embedding(validated_vector)

    def _to_hyperbolic(self, euclidean_vector: np.ndarray) -> np.ndarray:
        """Project Euclidean vector to Poincaré ball."""
        norm = np.linalg.norm(euclidean_vector)
        if norm == 0: return euclidean_vector
        curvature = abs(self.config['curvature']) # Ensure positive for scaling
        if curvature == 0: curvature = 1.0 # Avoid division by zero if Euclidean is accidentally chosen
        # Adjusted scaling: tanh maps [0, inf) -> [0, 1)
        scale_factor = np.tanh(norm / 2.0) # Removed curvature influence here, seems standard
        hyperbolic_vector = (euclidean_vector / norm) * scale_factor
        # Ensure norm is strictly less than 1
        hyp_norm = np.linalg.norm(hyperbolic_vector)
        if hyp_norm >= 1.0:
            hyperbolic_vector = hyperbolic_vector * (0.99999 / hyp_norm)
        return hyperbolic_vector

    def _from_hyperbolic(self, hyperbolic_vector: np.ndarray) -> np.ndarray:
        """Project Poincaré ball vector back to Euclidean."""
        norm = np.linalg.norm(hyperbolic_vector)
        if norm >= 1.0:
            logger.warning("GeometryManager", "Hyperbolic vector norm >= 1, cannot project back accurately.", {"norm": norm})
            # Project onto the boundary and then back
            norm = 0.99999
            hyperbolic_vector = (hyperbolic_vector / np.linalg.norm(hyperbolic_vector)) * norm
        if norm == 0: return hyperbolic_vector
        # Inverse of tanh is arctanh
        original_norm_approx = np.arctanh(norm) * 2.0 # Approximation without curvature
        return (hyperbolic_vector / norm) * original_norm_approx

    def euclidean_distance(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Calculate Euclidean distance."""
        aligned_a, aligned_b = self.align_vectors(vec_a, vec_b)
        return np.linalg.norm(aligned_a - aligned_b)

    def hyperbolic_distance(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Calculate Hyperbolic (Poincaré) distance."""
        aligned_a, aligned_b = self.align_vectors(vec_a, vec_b)
        norm_a_sq = np.sum(aligned_a**2)
        norm_b_sq = np.sum(aligned_b**2)

        # Ensure vectors are strictly inside the unit ball
        if norm_a_sq >= 1.0: aligned_a = aligned_a * (0.99999 / np.sqrt(norm_a_sq)); norm_a_sq=np.sum(aligned_a**2)
        if norm_b_sq >= 1.0: aligned_b = aligned_b * (0.99999 / np.sqrt(norm_b_sq)); norm_b_sq=np.sum(aligned_b**2)

        euclidean_dist_sq = np.sum((aligned_a - aligned_b)**2)
        denominator = (1 - norm_a_sq) * (1 - norm_b_sq)

        if denominator < 1e-15: # Prevent division by zero or extreme values
            # If denominator is tiny, points are near boundary. If points are also close, distance is small. If far, distance is large.
            if euclidean_dist_sq < 1e-9: return 0.0
            else: return np.inf # Effectively infinite distance

        argument = 1 + (2 * euclidean_dist_sq / denominator)

        # Clamp argument to handle potential floating point issues near 1.0
        argument = max(1.0, argument)

        # Calculate distance with curvature
        curvature = abs(self.config['curvature'])
        if curvature <= 1e-9: curvature = 1.0 # Treat 0 curvature as Euclidean-like case within arccosh framework
        distance = np.arccosh(argument) / np.sqrt(curvature)

        return float(distance)

    def spherical_distance(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Calculate Spherical distance (angle)."""
        aligned_a, aligned_b = self.align_vectors(vec_a, vec_b)
        norm_a = np.linalg.norm(aligned_a)
        norm_b = np.linalg.norm(aligned_b)
        if norm_a < 1e-9 or norm_b < 1e-9: return np.pi # Max distance if one vector is zero
        cos_angle = np.dot(aligned_a, aligned_b) / (norm_a * norm_b)
        # Clamp to valid range for arccos
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        return float(np.arccos(cos_angle))

    def mixed_distance(self, vec_a: np.ndarray, vec_b: np.ndarray, weights: Tuple[float, float, float] = (0.4, 0.4, 0.2)) -> float:
        """Calculate a weighted mixed distance."""
        euc_dist = self.euclidean_distance(vec_a, vec_b)
        hyp_dist = self.hyperbolic_distance(self._to_hyperbolic(vec_a), self._to_hyperbolic(vec_b))
        sph_dist = self.spherical_distance(vec_a, vec_b)
        # Normalize distances before combining (rough normalization)
        # Max Euclidean dist is 2, max spherical is pi
        euc_norm = euc_dist / 2.0
        sph_norm = sph_dist / np.pi
        # Hyperbolic distance can be large, use exp(-dist) for similarity-like scaling
        hyp_norm = np.exp(-hyp_dist * 0.5) # Scaled exponential decay

        # Combine weighted distances (treating hyp_norm as similarity, so use 1-hyp_norm)
        mixed_dist = weights[0] * euc_norm + weights[1] * (1.0 - hyp_norm) + weights[2] * sph_norm
        return float(mixed_dist)

    def calculate_distance(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Calculate distance based on configured geometry."""
        vec_a = self._validate_vector(vec_a, "Vector A")
        vec_b = self._validate_vector(vec_b, "Vector B")
        if vec_a is None or vec_b is None: return np.inf # Return infinite distance if validation failed

        geom_type = self.config['geometry_type']
        if geom_type == GeometryType.EUCLIDEAN:
            return self.euclidean_distance(vec_a, vec_b)
        elif geom_type == GeometryType.HYPERBOLIC:
            # Assume vectors are Euclidean, project them first
            hyp_a = self._to_hyperbolic(vec_a)
            hyp_b = self._to_hyperbolic(vec_b)
            return self.hyperbolic_distance(hyp_a, hyp_b)
        elif geom_type == GeometryType.SPHERICAL:
            return self.spherical_distance(vec_a, vec_b)
        elif geom_type == GeometryType.MIXED:
            return self.mixed_distance(vec_a, vec_b)
        else:
            logger.warning("GeometryManager", f"Unknown geometry type {geom_type}, using Euclidean.")
            return self.euclidean_distance(vec_a, vec_b)

    def calculate_similarity(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Calculate similarity between two vectors based on the configured geometry type.
        
        Returns cosine similarity (1.0 = identical, 0.0 = orthogonal, -1.0 = opposite)
        """
        # Validate inputs
        vec_a = self._validate_vector(vec_a, "Vector A for similarity")
        if vec_a is None:
            return 0.0
            
        vec_b = self._validate_vector(vec_b, "Vector B for similarity")
        if vec_b is None:
            return 0.0
            
        # Align vectors to same dimension
        aligned_a, aligned_b = self.align_vectors(vec_a, vec_b)
        
        # Normalize both vectors
        norm_a = np.linalg.norm(aligned_a)
        norm_b = np.linalg.norm(aligned_b)
        
        # Handle zero vectors
        if norm_a < 1e-9 or norm_b < 1e-9:
            return 0.0
        
        # Calculate cosine similarity
        norm_a_inv = 1.0 / norm_a
        norm_b_inv = 1.0 / norm_b
        dot_product = np.dot(aligned_a, aligned_b)
        similarity = dot_product * norm_a_inv * norm_b_inv
        
        # Ensure result is in valid range [-1.0, 1.0]
        return float(np.clip(similarity, -1.0, 1.0))

    def transform_to_geometry(self, vector: np.ndarray) -> np.ndarray:
        """Transform a vector into the configured geometry space (e.g., Poincaré ball)."""
        vector = self._validate_vector(vector, "Input Vector")
        if vector is None: return np.zeros(self.config['embedding_dim'])

        geom_type = self.config['geometry_type']
        if geom_type == GeometryType.HYPERBOLIC:
            return self._to_hyperbolic(vector)
        elif geom_type == GeometryType.SPHERICAL:
            # Project onto unit sphere (normalize)
            return self.normalize_embedding(vector)
        else: # Euclidean or Mixed (no specific projection needed for Euclidean part)
            return vector

    def transform_from_geometry(self, vector: np.ndarray) -> np.ndarray:
        """Transform a vector from the configured geometry space back to Euclidean."""
        vector = self._validate_vector(vector, "Input Vector")
        if vector is None: return np.zeros(self.config['embedding_dim'])

        geom_type = self.config['geometry_type']
        if geom_type == GeometryType.HYPERBOLIC:
            return self._from_hyperbolic(vector)
        else: # Spherical, Euclidean, Mixed - assume normalization or no transformation needed
            return vector

```

# gpu_setup.py

```py
#!/usr/bin/env python
# synthians_memory_core/gpu_setup.py

import os
import sys
import subprocess
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GPU-Setup")


def check_gpu_available():
    """Check if CUDA is available."""
    try:
        # Try to import torch and check CUDA availability
        import torch
        cuda_available = torch.cuda.is_available()
        logger.info(f"PyTorch CUDA available: {cuda_available}")
        
        if cuda_available:
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "Unknown"
            logger.info(f"Found {device_count} CUDA device(s). Using: {device_name}")
            return True
        else:
            # Try nvidia-smi as a backup check
            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            if result.returncode == 0:
                logger.info("nvidia-smi detected GPU, but PyTorch CUDA not available.")
                # Still return True as FAISS might be able to use it
                return True
            else:
                logger.info("No CUDA devices detected through nvidia-smi")
                return False
    except (ImportError, FileNotFoundError):
        logger.warning("Could not check CUDA availability through PyTorch or nvidia-smi")
        return False


def install_faiss_gpu():
    """Install FAISS with GPU support."""
    try:
        # Try to import faiss-gpu first to see if it's already installed
        try:
            import faiss
            if hasattr(faiss, 'get_num_gpus') and faiss.get_num_gpus() > 0:
                logger.info(f"FAISS-GPU already installed. Available GPUs: {faiss.get_num_gpus()}")
                return True
            else:
                logger.info("FAISS is installed but no GPUs detected by FAISS")
        except ImportError:
            logger.info("FAISS not installed yet, proceeding with installation")
        
        # First uninstall faiss-cpu if it exists
        logger.info("Uninstalling faiss-cpu if present...")
        subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "faiss-cpu"], 
                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Install faiss-gpu
        logger.info("Installing faiss-gpu...")
        result = subprocess.run(
            [sys.executable, "-m", "pip", "install", "faiss-gpu"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        if result.returncode != 0:
            logger.error(f"Failed to install faiss-gpu: {result.stderr.decode()}")
            return False
        
        # Verify installation
        try:
            import faiss
            logger.info(f"FAISS version: {faiss.__version__}")
            if hasattr(faiss, 'get_num_gpus'):
                gpu_count = faiss.get_num_gpus()
                logger.info(f"FAISS detected {gpu_count} GPUs")
                return gpu_count > 0
            else:
                logger.warning("FAISS installed but get_num_gpus not available")
                return False
        except ImportError:
            logger.error("Failed to import FAISS after installation")
            return False
            
    except Exception as e:
        logger.error(f"Error during FAISS-GPU installation: {str(e)}")
        return False


def install_faiss_cpu():
    """Install FAISS CPU version as fallback."""
    try:
        # Check if faiss is already installed
        try:
            import faiss
            logger.info(f"FAISS already installed (CPU version). Version: {faiss.__version__}")
            return True
        except ImportError:
            logger.info("FAISS not installed yet, proceeding with CPU installation")
        
        # Install faiss-cpu
        logger.info("Installing faiss-cpu...")
        result = subprocess.run(
            [sys.executable, "-m", "pip", "install", "faiss-cpu>=1.7.4"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        if result.returncode != 0:
            logger.error(f"Failed to install faiss-cpu: {result.stderr.decode()}")
            return False
        
        # Verify installation
        try:
            import faiss
            logger.info(f"FAISS CPU version: {faiss.__version__}")
            return True
        except ImportError:
            logger.error("Failed to import FAISS after installation")
            return False
            
    except Exception as e:
        logger.error(f"Error during FAISS-CPU installation: {str(e)}")
        return False


def setup_faiss():
    """Set up FAISS with GPU support if available, otherwise use CPU version."""
    logger.info("Checking for GPU availability...")
    if check_gpu_available():
        logger.info("GPU detected, installing FAISS with GPU support")
        if install_faiss_gpu():
            logger.info("Successfully installed FAISS with GPU support")
            return True
        else:
            logger.warning("Failed to install FAISS with GPU support, falling back to CPU version")
            return install_faiss_cpu()
    else:
        logger.info("No GPU detected, installing FAISS CPU version")
        return install_faiss_cpu()


if __name__ == "__main__":
    logger.info("=== FAISS GPU Setup Script ===")
    success = setup_faiss()
    if success:
        logger.info("FAISS setup completed successfully")
        sys.exit(0)
    else:
        logger.error("FAISS setup failed")
        sys.exit(1)

```

# hpc_quickrecal.py

```py
# synthians_memory_core/hpc_quickrecal.py

import os
import math
import logging
import json
import time
import asyncio
import traceback
import numpy as np
import torch
from enum import Enum
from typing import Dict, Any, List, Optional, Tuple, Union

from .geometry_manager import GeometryManager, GeometryType # Import from the unified manager
from .custom_logger import logger # Use the shared custom logger

# Renamed from FactorKeys for clarity
class QuickRecallFactor(Enum):
    RECENCY = "recency"
    EMOTION = "emotion"
    EXTENDED_EMOTION = "extended_emotion" # For buffer-based emotion
    RELEVANCE = "relevance" # e.g., similarity to query
    OVERLAP = "overlap" # Redundancy penalty
    R_GEOMETRY = "r_geometry" # Geometric novelty/distance
    CAUSAL_NOVELTY = "causal_novelty" # Surprise based on causal model/prediction
    SELF_ORG = "self_org" # Based on SOM or similar clustering
    IMPORTANCE = "importance" # Explicitly assigned importance
    PERSONAL = "personal" # Related to user's personal info
    SURPRISE = "surprise" # General novelty or unexpectedness
    DIVERSITY = "diversity" # Difference from other recent memories
    COHERENCE = "coherence" # Logical consistency with existing knowledge
    INFORMATION = "information" # Information density or value

class QuickRecallMode(Enum):
    STANDARD = "standard"
    HPC_QR = "hpc_qr" # Original HPC-QR formula using alpha, beta, etc.
    MINIMAL = "minimal" # Basic recency, relevance, emotion
    CUSTOM = "custom" # User-defined weights

class UnifiedQuickRecallCalculator:
    """Unified calculator for memory importance using HPC-QR principles."""

    def __init__(self, config: Optional[Dict[str, Any]] = None, geometry_manager: Optional[GeometryManager] = None):
        self.config = {
            'mode': QuickRecallMode.STANDARD,
            'factor_weights': {},
            'time_decay_rate': 0.1,
            'novelty_threshold': 0.45,
            'min_qr_score': 0.0,
            'max_qr_score': 1.0,
            'history_window': 100,
            'embedding_dim': 768,
             # HPC-QR specific weights (used in HPC_QR mode or as fallback)
            'alpha': 0.35, 'beta': 0.35, 'gamma': 0.2, 'delta': 0.1,
             # Factor configs
            'personal_keywords': ['my name', 'i live', 'my birthday', 'my job', 'my family'],
            'emotion_intensifiers': ['very', 'really', 'extremely', 'so'],
             **(config or {})
        }
        self.geometry_manager = geometry_manager or GeometryManager(self.config) # Use provided or create new
        self._init_factor_weights()
        self.history = {'calculated_qr': [], 'timestamps': [], 'factor_values': {f: [] for f in QuickRecallFactor}}
        self.total_calculations = 0
        logger.info("UnifiedQuickRecallCalculator", f"Initialized with mode: {self.config['mode'].value}")

    def _init_factor_weights(self):
        """Initialize weights based on mode."""
        default_weights = {f: 0.1 for f in QuickRecallFactor if f != QuickRecallFactor.OVERLAP} # Default equal weights
        default_weights[QuickRecallFactor.OVERLAP] = -0.1 # Overlap is a penalty

        mode = self.config['mode']
        if mode == QuickRecallMode.STANDARD:
            self.factor_weights = {
                QuickRecallFactor.RELEVANCE: 0.25, QuickRecallFactor.RECENCY: 0.15,
                QuickRecallFactor.EMOTION: 0.15, QuickRecallFactor.IMPORTANCE: 0.1,
                QuickRecallFactor.PERSONAL: 0.1, QuickRecallFactor.SURPRISE: 0.1,
                QuickRecallFactor.DIVERSITY: 0.05, QuickRecallFactor.COHERENCE: 0.05,
                QuickRecallFactor.INFORMATION: 0.05, QuickRecallFactor.OVERLAP: -0.1,
                 # Include HPC-QR factors with small default weights
                QuickRecallFactor.R_GEOMETRY: 0.0, QuickRecallFactor.CAUSAL_NOVELTY: 0.0,
                QuickRecallFactor.SELF_ORG: 0.0
            }
        elif mode == QuickRecallMode.MINIMAL:
             self.factor_weights = {
                QuickRecallFactor.RECENCY: 0.4, QuickRecallFactor.RELEVANCE: 0.4,
                QuickRecallFactor.EMOTION: 0.2, QuickRecallFactor.OVERLAP: -0.1
            }
        elif mode == QuickRecallMode.CUSTOM:
            # Ensure all factors are present, use defaults if missing
            user_weights = self.config.get('factor_weights', {})
            self.factor_weights = default_weights.copy()
            for factor, weight in user_weights.items():
                 if isinstance(factor, str): factor = QuickRecallFactor(factor.lower())
                 if factor in self.factor_weights: self.factor_weights[factor] = weight
        else: # Default to standard weights if mode is unrecognized (including HPC_QR for now)
            self.factor_weights = default_weights.copy()

        # Normalize weights (excluding overlap penalty)
        positive_weight_sum = sum(w for f, w in self.factor_weights.items() if f != QuickRecallFactor.OVERLAP and w > 0)
        if positive_weight_sum > 0 and abs(positive_weight_sum - 1.0) > 1e-6 :
             scale = 1.0 / positive_weight_sum
             for f in self.factor_weights:
                  if f != QuickRecallFactor.OVERLAP and self.factor_weights[f] > 0:
                       self.factor_weights[f] *= scale
        logger.debug("UnifiedQuickRecallCalculator", f"Initialized factor weights for mode {mode.value}", self.factor_weights)

    async def calculate(
        self,
        embedding_or_text: Union[str, np.ndarray, torch.Tensor, List[float]],
        text: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> float:
        """Calculate the composite QuickRecal score."""
        start_time = time.time()
        context = context or {}

        # --- Prepare Embedding ---
        embedding = None
        if isinstance(embedding_or_text, str):
            text_content = embedding_or_text
            # Generate embedding if needed (consider moving this outside for performance)
            # embedding = await self._generate_embedding(text_content) # Assume embedding gen is handled externally or mocked
            logger.debug("UnifiedQuickRecallCalculator", "Calculating score based on text, embedding generation assumed external.")
        else:
            embedding = self.geometry_manager._validate_vector(embedding_or_text, "Input Embedding")
            text_content = text or context.get("text", "") # Get text if available

        if embedding is not None:
             embedding = self.geometry_manager._normalize(embedding) # Ensure normalized

        # --- Calculate Factors ---
        factor_values = {}
        tasks = []

        # Helper to run potentially async factor calculations
        async def calculate_factor(factor, func, *args):
             try:
                 # Check if the function is a coroutine function or returns a coroutine
                 if asyncio.iscoroutinefunction(func):
                     val = await func(*args)
                 else:
                     # Regular function, don't await
                     val = func(*args)
                 factor_values[factor] = float(np.clip(val, 0.0, 1.0))
             except Exception as e:
                 logger.error("UnifiedQuickRecallCalculator", f"Error calculating factor {factor.value}", {"error": str(e)})
                 factor_values[factor] = 0.0 # Default on error

        # Context-based factors (fast)
        # Use the sync versions directly since they're quick
        factor_values[QuickRecallFactor.RECENCY] = self._calculate_recency(context)
        factor_values[QuickRecallFactor.RELEVANCE] = self._calculate_relevance(context)
        factor_values[QuickRecallFactor.IMPORTANCE] = self._calculate_importance(text_content, context)
        factor_values[QuickRecallFactor.PERSONAL] = self._calculate_personal(text_content, context)

        # Text-based factors (potentially slower)
        if text_content:
             tasks.append(calculate_factor(QuickRecallFactor.EMOTION, self._calculate_emotion, text_content, context))
             tasks.append(calculate_factor(QuickRecallFactor.INFORMATION, self._calculate_information, text_content, context))
             tasks.append(calculate_factor(QuickRecallFactor.COHERENCE, self._calculate_coherence, text_content, context))
        else:
             factor_values[QuickRecallFactor.EMOTION] = 0.0
             factor_values[QuickRecallFactor.INFORMATION] = 0.0
             factor_values[QuickRecallFactor.COHERENCE] = 0.0

        # Embedding-based factors (potentially slowest)
        if embedding is not None:
             # Use external momentum if provided
             external_momentum = context.get('external_momentum', None)
             tasks.append(calculate_factor(QuickRecallFactor.SURPRISE, self._calculate_surprise, embedding, external_momentum))
             tasks.append(calculate_factor(QuickRecallFactor.DIVERSITY, self._calculate_diversity, embedding, external_momentum))
             tasks.append(calculate_factor(QuickRecallFactor.OVERLAP, self._calculate_overlap, embedding, external_momentum))
             # HPC-QR specific factors
             tasks.append(calculate_factor(QuickRecallFactor.R_GEOMETRY, self._calculate_r_geometry, embedding, external_momentum))
             tasks.append(calculate_factor(QuickRecallFactor.CAUSAL_NOVELTY, self._calculate_causal_novelty, embedding, context)) # Causal needs context
             tasks.append(calculate_factor(QuickRecallFactor.SELF_ORG, self._calculate_self_org, embedding, context)) # SOM needs context (or internal SOM state)
        else:
             factor_values[QuickRecallFactor.SURPRISE] = 0.5
             factor_values[QuickRecallFactor.DIVERSITY] = 0.5
             factor_values[QuickRecallFactor.OVERLAP] = 0.0
             factor_values[QuickRecallFactor.R_GEOMETRY] = 0.5
             factor_values[QuickRecallFactor.CAUSAL_NOVELTY] = 0.5
             factor_values[QuickRecallFactor.SELF_ORG] = 0.5

        # Run potentially async calculations
        if tasks:
            await asyncio.gather(*tasks)

        # --- Combine Factors ---
        final_score = 0.0
        if self.config['mode'] == QuickRecallMode.HPC_QR:
            # Use the original alpha, beta, gamma, delta formula
            final_score = (
                self.config['alpha'] * factor_values.get(QuickRecallFactor.R_GEOMETRY, 0.0) +
                self.config['beta'] * factor_values.get(QuickRecallFactor.CAUSAL_NOVELTY, 0.0) +
                self.config['gamma'] * factor_values.get(QuickRecallFactor.SELF_ORG, 0.0) -
                self.config['delta'] * factor_values.get(QuickRecallFactor.OVERLAP, 0.0)
            )
            # Add other factors with small weights if needed, or keep it pure HPC-QR
            final_score += 0.05 * factor_values.get(QuickRecallFactor.RECENCY, 0.0)
            final_score += 0.05 * factor_values.get(QuickRecallFactor.EMOTION, 0.0)

        else:
            # Use weighted sum based on mode/custom weights
            for factor, value in factor_values.items():
                weight = self.factor_weights.get(factor, 0.0)
                # Overlap is a penalty
                if factor == QuickRecallFactor.OVERLAP:
                    final_score -= abs(weight) * value
                else:
                    final_score += weight * value

        # Apply time decay
        time_decay = self._calculate_time_decay(context)
        final_score *= time_decay

        # Clamp score
        final_score = float(np.clip(final_score, self.config['min_qr_score'], self.config['max_qr_score']))

        # Update history and stats
        self._update_history(final_score, factor_values)
        self.total_calculations += 1
        calculation_time = (time.time() - start_time) * 1000
        logger.debug("UnifiedQuickRecallCalculator", f"Score calculated: {final_score:.4f}", {"time_ms": calculation_time, "mode": self.config['mode'].value, "factors": {f.value: v for f,v in factor_values.items()}})

        return final_score

    def calculate_sync(
        self,
        embedding_or_text: Union[str, np.ndarray, torch.Tensor, List[float]],
        context: Optional[Dict[str, Any]] = None
    ) -> float:
        """Synchronous version of calculate for use in environments where asyncio.run() causes issues."""
        start_time = time.time()
        context = context or {}

        # --- Prepare Embedding ---
        embedding = None
        if isinstance(embedding_or_text, str):
            text_content = embedding_or_text
            logger.debug("UnifiedQuickRecallCalculator", "Calculating score based on text only in sync mode.")
        else:
            embedding = self.geometry_manager._validate_vector(embedding_or_text, "Input Embedding")
            text_content = context.get("text", "") # Get text if available

        if embedding is not None:
            embedding = self.geometry_manager._normalize(embedding) # Ensure normalized

        # --- Calculate Factors ---
        factor_values = {}

        # Context-based factors (fast)
        factor_values[QuickRecallFactor.RECENCY] = self._calculate_recency(context)
        factor_values[QuickRecallFactor.RELEVANCE] = self._calculate_relevance(context)
        factor_values[QuickRecallFactor.IMPORTANCE] = self._calculate_importance(text_content, context)
        factor_values[QuickRecallFactor.PERSONAL] = self._calculate_personal(text_content, context)

        # Text-based factors
        if text_content:
            # Use synchronous versions or set defaults
            try:
                factor_values[QuickRecallFactor.EMOTION] = self._calculate_emotion_sync(text_content, context)
            except:
                factor_values[QuickRecallFactor.EMOTION] = 0.0
                
            factor_values[QuickRecallFactor.INFORMATION] = 0.5  # Default value
            factor_values[QuickRecallFactor.COHERENCE] = 0.5    # Default value
        else:
            factor_values[QuickRecallFactor.EMOTION] = 0.0
            factor_values[QuickRecallFactor.INFORMATION] = 0.0
            factor_values[QuickRecallFactor.COHERENCE] = 0.0

        # Embedding-based factors
        if embedding is not None:
            # Use external momentum if provided
            external_momentum = context.get('external_momentum', None)
            factor_values[QuickRecallFactor.SURPRISE] = self._calculate_surprise_sync(embedding, external_momentum)
            factor_values[QuickRecallFactor.DIVERSITY] = self._calculate_diversity_sync(embedding, external_momentum)
            factor_values[QuickRecallFactor.OVERLAP] = self._calculate_overlap_sync(embedding, external_momentum)
            # HPC-QR specific factors
            factor_values[QuickRecallFactor.R_GEOMETRY] = self._calculate_r_geometry_sync(embedding, external_momentum)
            factor_values[QuickRecallFactor.CAUSAL_NOVELTY] = self._calculate_causal_novelty_sync(embedding, context)
            factor_values[QuickRecallFactor.SELF_ORG] = self._calculate_self_org_sync(embedding, context)
        else:
            factor_values[QuickRecallFactor.SURPRISE] = 0.5
            factor_values[QuickRecallFactor.DIVERSITY] = 0.5
            factor_values[QuickRecallFactor.OVERLAP] = 0.0
            factor_values[QuickRecallFactor.R_GEOMETRY] = 0.5
            factor_values[QuickRecallFactor.CAUSAL_NOVELTY] = 0.5
            factor_values[QuickRecallFactor.SELF_ORG] = 0.5

        # --- Combine Factors ---
        final_score = 0.0
        if self.config['mode'] == QuickRecallMode.HPC_QR:
            # Use the original alpha, beta, gamma, delta formula
            final_score = (
                self.config['alpha'] * factor_values.get(QuickRecallFactor.R_GEOMETRY, 0.0) +
                self.config['beta'] * factor_values.get(QuickRecallFactor.CAUSAL_NOVELTY, 0.0) +
                self.config['gamma'] * factor_values.get(QuickRecallFactor.SELF_ORG, 0.0) -
                self.config['delta'] * factor_values.get(QuickRecallFactor.OVERLAP, 0.0)
            )
            # Add other factors with small weights
            final_score += 0.05 * factor_values.get(QuickRecallFactor.RECENCY, 0.0)
            final_score += 0.05 * factor_values.get(QuickRecallFactor.EMOTION, 0.0)
        else:
            # Use weighted sum based on mode/custom weights
            for factor, value in factor_values.items():
                weight = self.factor_weights.get(factor, 0.0)
                # Overlap is a penalty
                if factor == QuickRecallFactor.OVERLAP:
                    final_score -= abs(weight) * value
                else:
                    final_score += weight * value

        # Apply time decay
        time_decay = self._calculate_time_decay(context)
        final_score *= time_decay

        # Clamp score
        final_score = float(np.clip(final_score, self.config['min_qr_score'], self.config['max_qr_score']))

        # Update history and stats
        self._update_history(final_score, factor_values)
        self.total_calculations += 1
        calculation_time = (time.time() - start_time) * 1000
        logger.debug("UnifiedQuickRecallCalculator", f"Score calculated (sync): {final_score:.4f}", {"time_ms": calculation_time, "mode": self.config['mode'].value})

        return final_score
        
    # Synchronous versions of the async calculation methods
    def _calculate_emotion_sync(self, text: str, context: Dict[str, Any]) -> float:
        # Simple fallback implementation
        return 0.5
        
    def _calculate_surprise_sync(self, embedding: np.ndarray, external_momentum=None) -> float:
        # Handle the dimension mismatch with safe alignment
        try:
            # Similar implementation to async version but synchronous
            if self.momentum_buffer is None or len(self.momentum_buffer) == 0:
                return 0.5  # Default value when no history
                
            # Calculate vector distance, handle dimension mismatch
            distances = []
            for vec in self.momentum_buffer:
                try:
                    # Use existing alignment functionality
                    aligned_vec, aligned_embedding = self._align_vectors_for_comparison(vec, embedding, log_warnings=False)
                    dist = self.geometry_manager.calculate_distance(aligned_vec, aligned_embedding)
                    distances.append(dist)
                except Exception:
                    distances.append(0.5)  # Default on error
                    
            if not distances:
                return 0.5
                
            # Calculate surprise based on minimum distance (most similar)
            min_dist = min(distances)
            surprise = min_dist / self.config.get('surprise_normalization', 2.0)
            return float(np.clip(surprise, 0.0, 1.0))
        except Exception as e:
            logger.warning("UnifiedQuickRecallCalculator", f"Error in surprise calc: {str(e)}")
            return 0.5
    
    def _calculate_diversity_sync(self, embedding: np.ndarray, external_momentum=None) -> float:
        # Simple implementation that handles dimension mismatches
        try:
            return self._calculate_surprise_sync(embedding, external_momentum) * 0.8  # Simplified
        except Exception:
            return 0.5
    
    def _calculate_overlap_sync(self, embedding: np.ndarray, external_momentum=None) -> float:
        # Simple implementation
        return 0.0  # Default no overlap
    
    def _calculate_r_geometry_sync(self, embedding: np.ndarray, external_momentum=None) -> float:
        # Simple implementation
        return 0.6  # Default moderate geometric novelty
    
    def _calculate_causal_novelty_sync(self, embedding: np.ndarray, context: Dict[str, Any]) -> float:
        # Simple implementation
        return 0.5  # Default causal novelty
    
    def _calculate_self_org_sync(self, embedding: np.ndarray, context: Dict[str, Any]) -> float:
        # Simple implementation
        return 0.5  # Default self-organization

    # --- Factor Calculation Methods ---
    # (Implementations adapted from your previous code, using GeometryManager where needed)

    def _calculate_recency(self, context: Dict[str, Any]) -> float:
        timestamp = context.get('timestamp', time.time())
        age_seconds = time.time() - timestamp
        # Exponential decay with a half-life of ~3 days
        decay_factor = np.exp(-age_seconds / (3 * 86400))
        return float(decay_factor)

    def _calculate_relevance(self, context: Dict[str, Any]) -> float:
        # Relevance might come from an external source (e.g., query similarity)
        return float(context.get('relevance', context.get('similarity', 0.5)))

    def _calculate_importance(self, text: str, context: Dict[str, Any]) -> float:
        # Explicit importance or keyword-based
        explicit_importance = context.get('importance', context.get('significance', 0.0))
        if text:
             keywords = ['important', 'remember', 'critical', 'key', 'significant']
             keyword_score = sum(1 for k in keywords if k in text.lower()) / 3.0
             return float(np.clip(max(explicit_importance, keyword_score), 0.0, 1.0))
        return float(explicit_importance)

    def _calculate_personal(self, text: str, context: Dict[str, Any]) -> float:
        # Check for personal keywords
        if text:
            count = sum(1 for k in self.config.get('personal_keywords', []) if k in text.lower())
            return float(np.clip(count / 3.0, 0.0, 1.0))
        return 0.0

    async def _calculate_emotion(self, text: str, context: Dict[str, Any]) -> float:
         # Reuse context's emotion data if available, otherwise analyze
         if 'emotion_data' in context and context['emotion_data']:
             intensity = context['emotion_data'].get('intensity', 0.0) # Assumes intensity 0-1
             valence_abs = abs(context['emotion_data'].get('sentiment_value', 0.0)) # Assumes valence -1 to 1
             return float(np.clip((intensity + valence_abs) / 2.0, 0.0, 1.0))
         # Placeholder: Simple keyword analysis if no analyzer
         if text:
              count = sum(1 for k in self.config.get('emotional_keywords', []) if k in text.lower())
              intensity = sum(1 for k in self.config.get('emotion_intensifiers', []) if k in text.lower())
              return float(np.clip((count + intensity) / 5.0, 0.0, 1.0))
         return 0.0

    async def _calculate_surprise(self, embedding: np.ndarray, external_momentum) -> float:
        # Novelty compared to recent memories (momentum)
        if external_momentum is None or len(external_momentum) == 0: return 0.5
        similarities = []
        for mem_emb in external_momentum[-5:]: # Compare with last 5
             sim = self.geometry_manager.calculate_similarity(embedding, mem_emb)
             similarities.append(sim)
        max_sim = max(similarities) if similarities else 0.0
        surprise = 1.0 - max_sim # Higher surprise if less similar to recent items
        return float(np.clip(surprise, 0.0, 1.0))

    async def _calculate_diversity(self, embedding: np.ndarray, external_momentum) -> float:
         # Novelty compared to the entire buffer (or a sample)
         if external_momentum is None or len(external_momentum) < 2: return 0.5
         # Sample if buffer is large
         sample_size = min(50, len(external_momentum))
         indices = np.random.choice(len(external_momentum), sample_size, replace=False)
         sample_momentum = [external_momentum[i] for i in indices]

         similarities = []
         for mem_emb in sample_momentum:
              sim = self.geometry_manager.calculate_similarity(embedding, mem_emb)
              similarities.append(sim)
         avg_sim = np.mean(similarities) if similarities else 0.0
         diversity = 1.0 - avg_sim # Higher diversity if less similar on average
         return float(np.clip(diversity, 0.0, 1.0))

    async def _calculate_overlap(self, embedding: np.ndarray, external_momentum) -> float:
         # Similar to surprise, but focused on maximum similarity as redundancy measure
         if external_momentum is None or len(external_momentum) == 0: return 0.0
         similarities = []
         for mem_emb in external_momentum[-10:]: # Check against more recent items for overlap
              sim = self.geometry_manager.calculate_similarity(embedding, mem_emb)
              similarities.append(sim)
         max_sim = max(similarities) if similarities else 0.0
         # Overlap is directly related to max similarity
         return float(np.clip(max_sim, 0.0, 1.0))

    async def _calculate_r_geometry(self, embedding: np.ndarray, external_momentum) -> float:
         # Distance from the center of the momentum buffer
         if external_momentum is None or len(external_momentum) < 3: return 0.5
         # Calculate centroid
         aligned_embeddings = []
         target_dim = self.config['embedding_dim']
         for emb in external_momentum:
              validated = self.geometry_manager._validate_vector(emb)
              if validated is not None:
                   aligned, _ = self.geometry_manager._align_vectors(validated, np.zeros(target_dim))
                   aligned_embeddings.append(aligned)

         if not aligned_embeddings: return 0.5
         centroid = np.mean(aligned_embeddings, axis=0)
         # Calculate distance from embedding to centroid
         distance = self.geometry_manager.calculate_distance(embedding, centroid)
         # Convert distance to score (larger distance = more novel = higher score)
         # Use exponential decay on distance
         geometry_score = np.exp(-distance * 0.5) # Adjust scaling factor as needed
         return float(np.clip(geometry_score, 0.0, 1.0))

    async def _calculate_causal_novelty(self, embedding: np.ndarray, context: Dict[str, Any]) -> float:
         # Placeholder - Requires a causal model
         # Simulates prediction based on context and compares with actual embedding
         predicted_embedding = embedding + np.random.randn(*embedding.shape) * 0.1 # Simulate slight prediction error
         novelty = 1.0 - self.geometry_manager.calculate_similarity(embedding, predicted_embedding)
         return float(np.clip(novelty, 0.0, 1.0))

    async def _calculate_self_org(self, embedding: np.ndarray, context: Dict[str, Any]) -> float:
         # Placeholder - Requires SOM or similar structure
         # Simulates finding BMU distance
         distance = np.random.rand() * 2.0 # Random distance 0-2
         self_org_score = np.exp(-distance * 0.5)
         return float(np.clip(self_org_score, 0.0, 1.0))

    def _calculate_information(self, text: str, context: Dict[str, Any]) -> float:
         # Simple length and keyword based information score
         if not text: return 0.0
         length_score = np.clip(len(text.split()) / 100.0, 0.0, 1.0)
         # Add bonus for specific informational keywords if needed
         return float(length_score)

    def _calculate_coherence(self, text: str, context: Dict[str, Any]) -> float:
         # Placeholder - Requires more advanced NLP or context checking
         # Simulate based on sentence structure (longer sentences might be more coherent)
         if not text: return 0.5
         sentences = [s for s in text.split('.') if s.strip()]
         if not sentences: return 0.5
         avg_len = sum(len(s.split()) for s in sentences) / len(sentences)
         coherence_score = np.clip(avg_len / 30.0, 0.0, 1.0) # Assuming avg sentence length target is ~30 words
         return float(coherence_score)

    def _calculate_user_attention(self, context: Dict[str, Any]) -> float:
         # Placeholder - Requires input from UI/interaction layer
         return float(context.get('user_attention', 0.0))

    def _calculate_time_decay(self, context: Dict[str, Any]) -> float:
        """Exponential time decay, clamped at min_time_decay."""
        timestamp = context.get('timestamp', time.time())
        elapsed_days = (time.time() - timestamp) / 86400.0
        decay_factor = np.exp(-self.config['time_decay_rate'] * elapsed_days)
        return float(max(self.config.get('min_time_decay', 0.02), decay_factor))

    def _update_history(self, score: float, factor_values: Dict[QuickRecallFactor, float]):
        """Update score history."""
        self.history['calculated_qr'].append(score)
        self.history['timestamps'].append(time.time())
        for factor, value in factor_values.items():
            self.history['factor_values'][factor].append(value)

        # Trim history
        hw = self.config['history_window']
        if len(self.history['calculated_qr']) > hw:
            self.history['calculated_qr'].pop(0)
            self.history['timestamps'].pop(0)
            for factor in self.history['factor_values']:
                if len(self.history['factor_values'][factor]) > hw:
                    self.history['factor_values'][factor].pop(0)

    def get_stats(self) -> Dict[str, Any]:
        """Retrieve calculator statistics."""
        qr_scores = self.history['calculated_qr']
        factor_stats = {}
        for factor, values in self.history['factor_values'].items():
             if values:
                  factor_stats[factor.value] = {
                       'average': float(np.mean(values)),
                       'stddev': float(np.std(values)),
                       'weight': self.factor_weights.get(factor, 0.0)
                  }

        return {
            'mode': self.config['mode'].value,
            'total_calculations': self.total_calculations,
            'avg_qr_score': float(np.mean(qr_scores)) if qr_scores else 0.0,
            'std_qr_score': float(np.std(qr_scores)) if qr_scores else 0.0,
            'history_size': len(qr_scores),
            'factors': factor_stats
        }

```

# interruption\__init__.py

```py
# synthians_memory_core/interruption/__init__.py

from .memory_handler import InterruptionAwareMemoryHandler

__all__ = ['InterruptionAwareMemoryHandler']

```

# interruption\memory_handler.py

```py
# synthians_memory_core/interruption/memory_handler.py

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional, Union, Callable, Awaitable
import json
import aiohttp
import numpy as np

class InterruptionAwareMemoryHandler:
    """
    Specialized handler for transcripts that enriches memory entries with interruption metadata.
    This bridges the voice system's interruption tracking with the memory system.
    """

    def __init__(self, 
                 api_url: str = "http://localhost:8000"):
        """
        Initialize the memory handler with API connection details.
        
        Args:
            api_url: Base URL for the memory API
        """
        self.logger = logging.getLogger("InterruptionAwareMemoryHandler")
        self.api_url = api_url.rstrip('/')
        
    async def __call__(self, 
                       text: str, 
                       transcript_sequence: int = 0,
                       timestamp: float = 0,
                       confidence: float = 1.0,
                       **metadata) -> Dict[str, Any]:
        """
        Process a transcript, enriching it with interruption metadata, and send to memory API.
        This method accepts transcripts and additional metadata from voice processing.
        
        Args:
            text: The transcript text to process
            transcript_sequence: Sequence number of this transcript
            timestamp: Unix timestamp when transcript was received
            confidence: STT confidence score
            **metadata: Additional metadata, including interruption data
            
        Returns:
            Response from the memory API as a dictionary
        """
        try:
            self.logger.info(f"Processing transcript {transcript_sequence}: {text[:50]}...")
            
            # Prepare audio metadata from transcript info
            audio_metadata = {
                "timestamp": timestamp,
                "confidence": confidence,
                "sequence": transcript_sequence,
                "source": "voice_interaction"
            }
            
            # Add interruption metadata if available
            if "was_interrupted" in metadata:
                audio_metadata["was_interrupted"] = metadata["was_interrupted"]
                audio_metadata["user_interruptions"] = metadata.get("user_interruptions", 1)
                
                if "interruption_timestamps" in metadata:
                    audio_metadata["interruption_timestamps"] = metadata["interruption_timestamps"]
                    
                if "session_id" in metadata:
                    audio_metadata["session_id"] = metadata["session_id"]
            
            # Prepare request to memory API
            request_data = {
                "text": text,
                "audio_metadata": audio_metadata
            }
            
            # Use the new transcription feature extraction endpoint
            async with aiohttp.ClientSession() as session:
                self.logger.info(f"Sending transcript to memory API: {self.api_url}/process_transcription")
                async with session.post(
                    f"{self.api_url}/process_transcription", 
                    json=request_data,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        self.logger.info(f"Memory created/updated with ID: {result.get('memory_id')}")
                        return result
                    else:
                        error_text = await response.text()
                        self.logger.error(f"Memory API error: {response.status} - {error_text}")
                        return {"success": False, "error": error_text}
                        
        except Exception as e:
            self.logger.error(f"Error processing transcript: {str(e)}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _validate_embedding(self, embedding):
        """
        Validate that an embedding is properly formed without NaN or Inf values.
        Implements the same validation logic as in memory_core/tools.py.
        
        Args:
            embedding: The embedding vector to validate (np.ndarray or list)
            
        Returns:
            bool: True if the embedding is valid, False otherwise
        """
        if embedding is None:
            return False
            
        # Convert to numpy array if needed
        if isinstance(embedding, list):
            embedding = np.array(embedding, dtype=np.float32)
            
        # Check for NaN or Inf values
        if np.isnan(embedding).any() or np.isinf(embedding).any():
            return False
            
        return True

    @staticmethod
    def get_reflection_prompt(interruption_data: Dict[str, Any]) -> Optional[str]:
        """
        Generate a reflection prompt based on interruption patterns to help guide memory retrieval.
        
        Args:
            interruption_data: Dictionary containing interruption metadata
            
        Returns:
            Optional reflection prompt string or None if no reflection needed
        """
        was_interrupted = interruption_data.get("was_interrupted", False)
        interruption_count = interruption_data.get("user_interruptions", 0)
        
        # No reflection needed for normal conversation flow
        if not was_interrupted and interruption_count == 0:
            return None
            
        # Generate prompts based on interruption patterns
        if was_interrupted:
            if interruption_count > 5:
                return "You seem to be interrupting frequently. Would you like me to pause more often to let you speak?"
            else:
                return "I noticed you interrupted. Was there something specific you wanted to address?"
        
        # General high interruption pattern but not this specific utterance
        if interruption_count > 3:
            return "I've noticed several interruptions in our conversation. Would you prefer if I spoke in shorter segments?"
            
        return None

```

# interruption\README.md

```md
# Interruption Tracking and Analysis Module

## Overview

The interruption module provides a bridge between Lucidia's voice interaction system and the memory core. It captures conversational rhythm, interruption patterns, and speaking behaviors to enhance the semantic understanding of conversations with rich contextual metadata.

## Key Components

### InterruptionAwareMemoryHandler

A specialized handler that processes transcripts with interruption metadata and stores them in the memory system with rich contextual information.

\`\`\`python
from synthians_memory_core.interruption import InterruptionAwareMemoryHandler

# Initialize the handler
handler = InterruptionAwareMemoryHandler(api_url="http://localhost:8000")

# Process a transcript with interruption data
await handler(
    text="I wanted to explain something important.",
    was_interrupted=True,
    user_interruptions=2,
    interruption_timestamps=[1678945330.45, 1678945342.12]
)
\`\`\`

## Integration with VoiceStateManager

The interruption module is designed to work with the `VoiceStateManager` from the voice_core package. The VoiceStateManager tracks interruptions in real-time and provides this data when processing transcripts.

### Configuration

To connect the VoiceStateManager with the InterruptionAwareMemoryHandler:

\`\`\`python
from voice_core.state.voice_state_manager import VoiceStateManager
from synthians_memory_core.interruption import InterruptionAwareMemoryHandler

# Initialize components
state_manager = VoiceStateManager()
memory_handler = InterruptionAwareMemoryHandler(api_url="http://localhost:8000")

# Register the memory handler as the transcript handler
state_manager.register_transcript_handler(memory_handler)
\`\`\`

## Memory Processing Flow

1. VoiceStateManager detects and tracks interruptions during conversation
2. When a transcript is processed, interruption metadata is attached
3. InterruptionAwareMemoryHandler sends this enriched data to the memory API
4. TranscriptionFeatureExtractor processes the text and metadata
5. The memory is stored with rich conversational context

## Using Interruption Data for Reflection

The module provides utilities to generate reflection prompts based on interruption patterns:

\`\`\`python
from synthians_memory_core.interruption import InterruptionAwareMemoryHandler

# For a memory with high interruption count
prompt = InterruptionAwareMemoryHandler.get_reflection_prompt({
    "was_interrupted": True,
    "user_interruptions": 6
})
# Returns: "You seem to be interrupting frequently. Would you like me to pause more often to let you speak?"
\`\`\`

## Compatibility with Embedding Handling

This module is fully compatible with Lucidia's robust embedding handling system:

- Works with both 384 and 768 dimension embeddings
- Properly handles vector alignment during comparison operations
- Validates embeddings to prevent NaN/Inf values
- Provides graceful fallbacks when embedding generation fails

## Metadata Structure

The interruption metadata schema includes:

\`\`\`json
{
  "was_interrupted": true,            // Whether this specific utterance was interrupted
  "user_interruptions": 3,           // Total interruptions in the current session
  "interruption_timestamps": [       // Timestamps of interruptions (relative to session start)
    12.5, 24.1, 38.8
  ],
  "session_id": "abc123",            // Unique ID for the current conversation session
  "interruption_severity": "medium", // Classification of interruption pattern severity
  "requires_reflection": true        // Whether this memory might benefit from reflection
}
\`\`\`

## Best Practices

1. **Session Management**: Generate a new session ID for each distinct conversation
2. **Timestamp Precision**: Store interruption timestamps as relative times (seconds from session start)
3. **Aggregation**: Consider aggregating interruption patterns across multiple sessions for deeper insights
4. **Memory Retrieval**: Use interruption metadata as a factor in memory prioritization

```

# memory_core\trainer_integration.py

```py
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import datetime
import logging
import numpy as np

from synthians_memory_core.memory_structures import MemoryEntry
from synthians_memory_core import SynthiansMemoryCore
from synthians_memory_core.geometry_manager import GeometryManager

logger = logging.getLogger(__name__)

class SequenceEmbedding(BaseModel):
    """Representation of an embedding in a sequence for trainer integration."""
    id: str
    embedding: List[float]
    timestamp: str
    quickrecal_score: Optional[float] = None
    emotion: Optional[Dict[str, float]] = None
    dominant_emotion: Optional[str] = None
    importance: Optional[float] = None
    topic: Optional[str] = None
    user: Optional[str] = None

class SequenceEmbeddingsResponse(BaseModel):
    """Response model for a sequence of embeddings."""
    embeddings: List[SequenceEmbedding]
    
class UpdateQuickRecalScoreRequest(BaseModel):
    """Request to update the quickrecal score of a memory based on surprise."""
    memory_id: str
    delta: float
    predicted_embedding: Optional[List[float]] = None
    reason: Optional[str] = None
    embedding_delta: Optional[List[float]] = None

class TrainerIntegrationManager:
    """Manages integration between the Memory Core and the Sequence Trainer.
    
    This class bridges the gap between the memory storage system and the
    predictive sequence model, enabling bidirectional communication for:
    - Feeding memory embeddings to the trainer in sequence
    - Updating memory retrieval scores based on prediction surprises
    """
    
    def __init__(self, memory_core: SynthiansMemoryCore):
        """Initialize with reference to the memory core."""
        self.memory_core = memory_core
        
        # Get the embedding dimension from the main memory core config for consistency
        embedding_dim = self.memory_core.config.get('embedding_dim', 768)  # Default to 768 if not found
        
        # Correctly initialize GeometryManager with a config dictionary
        self.geometry_manager = GeometryManager(config={
            'embedding_dim': embedding_dim,
            'normalization_enabled': True,
            'alignment_strategy': 'truncate'
        })
    
    async def get_sequence_embeddings(self, 
                                topic: Optional[str] = None, 
                                user: Optional[str] = None,
                                emotion: Optional[str] = None,
                                min_importance: Optional[float] = None,
                                limit: int = 100,
                                min_quickrecal_score: Optional[float] = None,
                                start_timestamp: Optional[str] = None,
                                end_timestamp: Optional[str] = None,
                                sort_by: str = "timestamp") -> SequenceEmbeddingsResponse:
        """Retrieve a sequence of embeddings from the memory core,
        ordered by timestamp or quickrecal score.
        
        Args:
            topic: Optional topic filter
            user: Optional user filter
            emotion: Optional dominant emotion filter
            min_importance: Optional minimum importance threshold
            limit: Maximum number of embeddings to retrieve
            min_quickrecal_score: Minimum quickrecal score threshold
            start_timestamp: Optional start time boundary
            end_timestamp: Optional end time boundary
            sort_by: Field to sort by ("timestamp" or "quickrecal_score")
            
        Returns:
            SequenceEmbeddingsResponse with ordered list of embeddings
        """
        # Convert timestamp strings to datetime objects if provided
        start_dt = None
        end_dt = None
        if start_timestamp:
            try:
                start_dt = datetime.datetime.fromisoformat(start_timestamp)
            except ValueError:
                logger.warning(f"Invalid start_timestamp format: {start_timestamp}")
        
        if end_timestamp:
            try:
                end_dt = datetime.datetime.fromisoformat(end_timestamp)
            except ValueError:
                logger.warning(f"Invalid end_timestamp format: {end_timestamp}")
        
        # Query the memory entries
        query = {}
        
        # Add filters if specified
        if topic:
            query["metadata.topic"] = topic
        
        if user:
            query["metadata.user"] = user
            
        if emotion:
            query["metadata.dominant_emotion"] = emotion
            
        if min_importance is not None:
            query["metadata.importance"] = {"$gte": min_importance}
            
        # Add quickrecal score filter if specified
        if min_quickrecal_score is not None:
            query["quickrecal_score"] = {"$gte": min_quickrecal_score}
            
        # Add timestamp filters if specified
        if start_dt or end_dt:
            timestamp_query = {}
            if start_dt:
                timestamp_query["$gte"] = start_dt
            if end_dt:
                timestamp_query["$lte"] = end_dt
            if timestamp_query:
                query["timestamp"] = timestamp_query
        
        # Determine sort field and order
        sort_field = "timestamp"
        if sort_by == "quickrecal_score":
            sort_field = "quickrecal_score"
            sort_order = "desc"  # Higher scores first for quickrecal
        else:
            sort_order = "asc"   # Chronological order for timestamps
        
        # Retrieve the memories, ordered by specified field
        memories = await self.memory_core.get_memories(
            query=query,
            sort_by=sort_field,
            sort_order=sort_order,
            limit=limit
        )
        
        # Convert memories to sequence embeddings
        sequence_embeddings = []
        for memory in memories:
            # Skip memories without embeddings
            if not memory.embedding:
                continue
                
            # Standardize embedding using the geometry manager
            standardized_embedding = self.geometry_manager.standardize_embedding(memory.embedding)
                
            # Extract metadata
            metadata = memory.metadata or {}
            
            sequence_embeddings.append(SequenceEmbedding(
                id=str(memory.id),
                embedding=standardized_embedding.tolist(),
                timestamp=memory.timestamp.isoformat(),
                quickrecal_score=memory.quickrecal_score,
                emotion=metadata.get("emotions"),
                dominant_emotion=metadata.get("dominant_emotion"),
                importance=metadata.get("importance"),
                topic=metadata.get("topic"),
                user=metadata.get("user")
            ))
            
        return SequenceEmbeddingsResponse(embeddings=sequence_embeddings)
    
    async def update_quickrecal_score(self, request: UpdateQuickRecalScoreRequest) -> Dict[str, Any]:
        """Update the quickrecal score of a memory based on surprise feedback.
        
        Args:
            request: The update request containing memory_id, delta, and additional context
            
        Returns:
            Dict with status of the update operation
        """
        memory_id = request.memory_id
        delta = request.delta
        
        # Retrieve the memory
        memory = await self.memory_core.get_memory_by_id_async(memory_id)
        if not memory:
            return {"status": "error", "message": f"Memory with ID {memory_id} not found"}
        
        # Calculate new quickrecal score
        current_score = memory.quickrecal_score or 0.0
        new_score = min(1.0, max(0.0, current_score + delta))  # Ensure score stays between 0 and 1
        
        # Prepare updates for the memory
        updates = {"quickrecal_score": new_score}
        
        # Add surprise metadata if provided
        if request.reason or request.embedding_delta or request.predicted_embedding:
            # Get existing metadata or initialize empty dict
            metadata = memory.metadata or {}
            
            # Create or update surprise tracking
            surprise_events = metadata.get("surprise_events", [])
            new_event = {
                "timestamp": datetime.datetime.utcnow().isoformat(),
                "delta": delta,
                "previous_score": current_score,
                "new_score": new_score
            }
            
            # Calculate embedding delta if both memory embedding and predicted embedding are available
            if memory.embedding is not None and request.predicted_embedding and not request.embedding_delta:
                # Use the geometry manager to calculate the delta between predicted and actual embeddings
                embedding_delta = self.geometry_manager.generate_embedding_delta(
                    predicted=request.predicted_embedding,
                    actual=memory.embedding
                )
                new_event["embedding_delta"] = embedding_delta
                
                # Calculate surprise score based on vector comparison
                surprise_score = self.geometry_manager.calculate_surprise(
                    predicted=request.predicted_embedding,
                    actual=memory.embedding
                )
                new_event["calculated_surprise"] = surprise_score
            
            # Add optional fields if provided
            if request.reason:
                new_event["reason"] = request.reason
            if request.embedding_delta:
                new_event["embedding_delta"] = request.embedding_delta
            if request.predicted_embedding:
                new_event["predicted_embedding"] = request.predicted_embedding
                
            # Add the new event to the list
            surprise_events.append(new_event)
            
            # Update metadata with new surprise events
            metadata["surprise_events"] = surprise_events
            
            # Add surprise count or increment it
            metadata["surprise_count"] = metadata.get("surprise_count", 0) + 1
            
            # Update the memory with the new metadata
            updates["metadata"] = metadata
        
        # Update the memory
        updated = await self.memory_core.update_memory(
            memory_id=memory_id,
            updates=updates
        )
        
        if updated:
            result = {
                "status": "success", 
                "memory_id": memory_id,
                "previous_score": current_score,
                "new_score": new_score,
                "delta": delta
            }
            
            # Include additional fields if they were in the request
            if request.reason:
                result["reason"] = request.reason
            if request.embedding_delta:
                result["embedding_delta_norm"] = np.linalg.norm(np.array(request.embedding_delta))
                
            return result
        else:
            return {"status": "error", "message": f"Failed to update quickrecal score for memory {memory_id}"}

```

# memory_persistence.py

```py
# synthians_memory_core/memory_persistence.py

import os
import json
import logging
import asyncio
import time
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import numpy as np
import torch
import aiofiles # Use aiofiles for async file operations
import uuid
from .memory_structures import MemoryEntry, MemoryAssembly # Use the unified structure
from .custom_logger import logger # Use the shared custom logger
from datetime import datetime
import copy

class MemoryPersistence:
    """Handles disk-based memory operations with robustness."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = {
            'storage_path': Path('/app/memory/stored'), # Consistent Docker path
            'backup_dir': 'backups',
            'index_filename': 'memory_index.json',
            'max_backups': 5,
            'safe_write': True, # Use atomic writes
            **(config or {})
        }
        self.storage_path = Path(self.config['storage_path'])
        self.backup_path = self.storage_path / self.config['backup_dir']
        self.index_path = self.storage_path / self.config['index_filename']
        self.memory_index: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
        self.stats = {'saves': 0, 'loads': 0, 'deletes': 0, 'backups': 0, 'errors': 0}
        self._initialized = False  # Add initialization flag

        # Ensure directories exist
        try:
            self.storage_path.mkdir(parents=True, exist_ok=True)
            self.backup_path.mkdir(exist_ok=True)
        except Exception as e:
             logger.error("MemoryPersistence", "Failed to create storage directories", {"path": self.storage_path, "error": str(e)})
             raise # Initialization failure is critical

        logger.info("MemoryPersistence", "Initialized (index loading deferred)", {"storage_path": str(self.storage_path)})

    async def initialize(self):
        """Load the memory index asynchronously."""
        if self._initialized:
            return
        logger.info("MemoryPersistence", "Initializing (loading index)...")
        await self._load_index()
        self._initialized = True
        logger.info("MemoryPersistence", "Initialization complete.")

    async def _load_index(self):
        """Load the memory index from disk."""
        async with self._lock:
             if not self.index_path.exists():
                 logger.info("MemoryPersistence", "Memory index file not found, starting fresh.", {"path": str(self.index_path)})
                 self.memory_index = {}
                 return

             try:
                 async with aiofiles.open(self.index_path, 'r') as f:
                     content = await f.read()
                     loaded_index = json.loads(content)
                 # Basic validation
                 if isinstance(loaded_index, dict):
                     self.memory_index = loaded_index
                     logger.info("MemoryPersistence", f"Loaded memory index with {len(self.memory_index)} entries.", {"path": str(self.index_path)})
                 else:
                      logger.error("MemoryPersistence", "Invalid index file format, starting fresh.", {"path": str(self.index_path)})
                      self.memory_index = {}
             except Exception as e:
                 logger.error("MemoryPersistence", "Error loading memory index, starting fresh.", {"path": str(self.index_path), "error": str(e)})
                 self.memory_index = {} # Start fresh on error

    async def _save_index_no_lock(self):
        """Save the memory index to disk atomically, without acquiring a lock.
        This method should only be called when the caller already holds self._lock."""
        try:
            logger.debug("MemoryPersistence", "Saving memory index to disk")
            temp_path = self.index_path.with_suffix('.tmp')
            async with aiofiles.open(temp_path, 'w') as f:
                await f.write(json.dumps(self.memory_index, indent=2))
            await asyncio.to_thread(os.replace, temp_path, self.index_path)
            self.stats['last_index_update'] = time.time()
            logger.debug("MemoryPersistence", "Memory index saved successfully")
            return True
        except asyncio.TimeoutError:
            logger.error("MemoryPersistence", "Timeout saving memory index")
            return False
        except Exception as e:
            logger.error("MemoryPersistence", "Error saving memory index", {"path": str(self.index_path), "error": str(e)})
            # Attempt to remove potentially corrupted temp file
            if await asyncio.to_thread(os.path.exists, temp_path):
                try: await asyncio.to_thread(os.remove, temp_path)
                except Exception: pass
            return False

    async def _save_index(self):
        """Save the memory index to disk atomically with lock acquisition."""
        async with self._lock:
            await self._save_index_no_lock()

    async def save_memory(self, memory: MemoryEntry) -> bool:
        """Save a single memory entry to disk."""
        try:
            # Check if we're being called during shutdown/cleanup with no event loop
            try:
                loop = asyncio.get_running_loop()
                if not loop.is_running():
                    logger.warning("MemoryPersistence", f"Attempted to save memory {memory.id} with no running event loop")
                    return False
            except RuntimeError:
                # No running event loop - this happens during test cleanup sometimes
                logger.warning("MemoryPersistence", f"Error saving memory {memory.id}: no running event loop")
                return False
            
            logger.debug("MemoryPersistence", f"Saving memory {memory.id} - acquiring lock")
            async with self._lock:
                try:
                    # Create a unique ID if one doesn't exist
                    if not hasattr(memory, 'id') or memory.id is None:
                        memory.id = f"mem_{uuid.uuid4().hex[:12]}"
                    
                    # Ensure the storage directory exists
                    memory_dir = self.storage_path 
                    memory_dir.mkdir(exist_ok=True, parents=True)
                    
                    # Generate a filename based on the memory ID
                    file_path = memory_dir / f"{memory.id}.json"
                    
                    # Convert the memory to a serializable dict
                    memory_dict = memory.to_dict()

                    # Write the memory to disk
                    async with aiofiles.open(file_path, 'w') as f:
                        # Ensure complex numbers or other non-serializables are handled
                        def default_serializer(obj):
                             if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,
                                                 np.int16, np.int32, np.int64, np.uint8,
                                                 np.uint16, np.uint32, np.uint64)):
                                 return int(obj)
                             elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
                                 return float(obj)
                             elif isinstance(obj, (np.ndarray,)): # Handle complex arrays if needed
                                 return obj.tolist()
                             elif isinstance(obj, set):
                                 return list(obj)
                             elif isinstance(obj, datetime):
                                 return obj.isoformat()
                             try:
                                  # Fallback for other types
                                  return str(obj)
                             except:
                                  return "[Unserializable Object]"
                        await f.write(json.dumps(memory_dict, indent=2, default=default_serializer))
                    
                    # Update the memory index
                    self.memory_index[memory.id] = {
                        'path': str(file_path.relative_to(self.storage_path)),
                        'timestamp': memory.timestamp.isoformat() if hasattr(memory.timestamp, 'isoformat') else str(memory.timestamp) if hasattr(memory, 'timestamp') else time.time(),
                        'quickrecal': memory.quickrecal_score if hasattr(memory, 'quickrecal_score') else 0.5,
                        'type': 'memory'  # Default type since memory_type doesn't exist
                    }
                    
                    # Save the memory index - call internal version without lock since we already have it
                    await asyncio.wait_for(self._save_index_no_lock(), timeout=5)
                    
                    self.stats['saves'] += 1
                    self.stats['successful_saves'] = self.stats.get('successful_saves', 0) + 1
                    logger.debug("MemoryPersistence", f"Memory {memory.id} saved to disk, releasing lock")
                    return True
                except Exception as e:
                    logger.error("MemoryPersistence", f"Error saving memory {getattr(memory, 'id', 'unknown')}: {str(e)}")
                    self.stats['saves'] += 1
                    self.stats['failed_saves'] = self.stats.get('failed_saves', 0) + 1
                    return False
        except asyncio.TimeoutError:
            logger.error("MemoryPersistence", f"Timeout saving memory {getattr(memory, 'id', 'unknown')}")
            return False

    async def load_memory(self, memory_id: str) -> Optional[MemoryEntry]:
        """Load a single memory entry from disk."""
        logger.debug("MemoryPersistence.load_memory", f"Attempting to load memory {memory_id}")
        async with self._lock:
             try:
                  if memory_id not in self.memory_index:
                      # Fallback: check filesystem directly (maybe index is outdated)
                      file_path = self.storage_path / f"{memory_id}.json"
                      if not await asyncio.to_thread(os.path.exists, file_path):
                           logger.warning("MemoryPersistence.load_memory", f"Memory {memory_id} not found in index or filesystem.")
                           return None
                      # If found directly, update index info
                      self.memory_index[memory_id] = {'path': f"{memory_id}.json"}
                  else:
                      file_path = self.storage_path / self.memory_index[memory_id]['path']

                  # Check primary path first
                  if not await asyncio.to_thread(os.path.exists, file_path):
                       # Try backup path
                       backup_path = file_path.with_suffix('.bak')
                       if await asyncio.to_thread(os.path.exists, backup_path):
                            logger.warning("MemoryPersistence.load_memory", f"Using backup file for {memory_id}", {"path": str(backup_path)})
                            file_path = backup_path
                       else:
                            logger.error("MemoryPersistence.load_memory", f"Memory file not found for {memory_id}", {"path": str(file_path)})
                            # Remove from index if file is missing
                            if memory_id in self.memory_index: del self.memory_index[memory_id]
                            return None

                  logger.debug("MemoryPersistence.load_memory", f"Reading file: {file_path}")
                  async with aiofiles.open(file_path, 'r') as f:
                      content = await f.read()

                  logger.debug("MemoryPersistence.load_memory", f"Parsing JSON for {memory_id}")
                  memory_dict = json.loads(content)

                  logger.debug("MemoryPersistence.load_memory", f"Creating MemoryEntry from dict for {memory_id}")
                  memory = MemoryEntry.from_dict(memory_dict)

                  logger.debug("MemoryPersistence.load_memory", f"Successfully loaded memory {memory_id}")
                  self.stats['loads'] = self.stats.get('loads', 0) + 1
                  self.stats['successful_loads'] = self.stats.get('successful_loads', 0) + 1
                  return memory

             except FileNotFoundError:
                  logger.warning("MemoryPersistence.load_memory", f"Memory file not found for {memory_id} (FileNotFoundError)")
                  return None
             except json.JSONDecodeError as e:
                  logger.error("MemoryPersistence.load_memory", f"JSON Decode Error loading memory {memory_id}", {"path": str(file_path), "error": str(e)})
                  return None
             except Exception as e:
                  logger.error("MemoryPersistence.load_memory", f"Unexpected Error loading memory {memory_id}", {"path": str(file_path), "error": str(e)}, exc_info=True)
                  self.stats['loads'] = self.stats.get('loads', 0) + 1
                  self.stats['failed_loads'] = self.stats.get('failed_loads', 0) + 1
                  return None

    async def delete_memory(self, memory_id: str) -> bool:
        """Delete a memory file from disk."""
        async with self._lock:
             try:
                 if memory_id not in self.memory_index:
                     # Check filesystem directly as fallback
                     file_path_direct = self.storage_path / f"{memory_id}.json"
                     if not await asyncio.to_thread(os.path.exists, file_path_direct):
                          logger.warning("MemoryPersistence", f"Memory {memory_id} not found for deletion")
                          return False
                     # If found directly, update index info
                     self.memory_index[memory_id] = {'path': f"{memory_id}.json"}
                 else:
                     file_path = self.storage_path / self.memory_index[memory_id]['path']

                 deleted = False
                 if await asyncio.to_thread(os.path.exists, file_path):
                     await asyncio.to_thread(os.remove, file_path)
                     deleted = True
                 if await asyncio.to_thread(os.path.exists, file_path.with_suffix('.bak')):
                     await asyncio.to_thread(os.remove, file_path.with_suffix('.bak'))
                     deleted = True # Mark deleted even if only backup existed

                 if deleted:
                     del self.memory_index[memory_id]
                     await self._save_index() # Update index after deletion
                     self.stats['deletes'] = self.stats.get('deletes', 0) + 1
                     return True
                 else:
                      # File didn't exist, remove from index anyway
                      del self.memory_index[memory_id]
                      await self._save_index()
                      return False # Indicate file wasn't actually deleted

             except Exception as e:
                 logger.error("MemoryPersistence", f"Error deleting memory {memory_id}", {"error": str(e)})
                 self.stats['errors'] = self.stats.get('errors', 0) + 1
                 return False

    async def load_all(self) -> List[MemoryEntry]:
        """Load all memories listed in the index."""
        if not self._initialized:
            await self.initialize()  # Ensure index is loaded
            
        all_memories = []
        logger.info("MemoryPersistence.load_all", "Acquiring lock...")
        async with self._lock:
            memory_ids = list(self.memory_index.keys())
            total_to_load = len(memory_ids)
            logger.info("MemoryPersistence.load_all", f"Lock acquired. Found {total_to_load} IDs in index. Starting load.")

            # Consider batching if loading many memories
            batch_size = 50 # Reduced batch size for more granular logging
            loaded_count = 0
            for i in range(0, total_to_load, batch_size):
                 batch_ids = memory_ids[i:i+batch_size]
                 logger.info("MemoryPersistence.load_all", f"Processing batch {i//batch_size + 1}/{(total_to_load + batch_size - 1)//batch_size} (IDs: {batch_ids[:5]}...)")
                 load_tasks = []
                 for mid in batch_ids:
                     logger.debug("MemoryPersistence.load_all", f"Creating load task for ID: {mid}")
                     load_tasks.append(self.load_memory(mid)) # load_memory already logs internally now

                 results = await asyncio.gather(*load_tasks, return_exceptions=True) # Catch exceptions per task

                 batch_loaded_count = 0
                 for j, result in enumerate(results):
                     batch_mem_id = batch_ids[j]
                     if isinstance(result, MemoryEntry):
                         all_memories.append(result)
                         batch_loaded_count += 1
                     elif isinstance(result, Exception):
                          logger.error("MemoryPersistence.load_all", f"Error loading memory {batch_mem_id} within batch", {"error": str(result)}, exc_info=result)
                     else:
                          logger.warning("MemoryPersistence.load_all", f"load_memory for {batch_mem_id} returned None or unexpected type: {type(result)}")

                 loaded_count += batch_loaded_count
                 logger.info("MemoryPersistence.load_all", f"Batch {i//batch_size + 1} complete. Loaded {batch_loaded_count} memories in batch. Total loaded: {loaded_count}/{total_to_load}")
                 await asyncio.sleep(0.05) # Slightly longer yield

            logger.info("MemoryPersistence.load_all", f"Finished loading loop. Loaded {len(all_memories)} memories successfully. Releasing lock.")
        return all_memories

    async def create_backup(self) -> bool:
        """Create a timestamped backup of the memory storage."""
        async with self._lock:
             try:
                 timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                 backup_instance_path = self.backup_path / f"backup_{timestamp}"
                 # Use shutil.copytree for directory backup
                 await asyncio.to_thread(shutil.copytree, self.storage_path, backup_instance_path, ignore=shutil.ignore_patterns('backups'))
                 self.stats['last_backup'] = time.time()
                 self.stats['backup_count'] = self.stats.get('backup_count', 0) + 1
                 logger.info("MemoryPersistence", f"Created backup at {backup_instance_path}")
                 await self._prune_backups()
                 return True
             except Exception as e:
                 logger.error("MemoryPersistence", "Error creating backup", {"error": str(e)})
                 self.stats['errors'] = self.stats.get('errors', 0) + 1
                 return False

    async def _prune_backups(self):
        """Remove old backups, keeping only the most recent ones."""
        try:
             backups = sorted(
                 [d for d in self.backup_path.iterdir() if d.is_dir() and d.name.startswith('backup_')],
                 key=lambda d: d.stat().st_mtime
             )
             num_to_keep = self.config['max_backups']
             if len(backups) > num_to_keep:
                 for old_backup in backups[:-num_to_keep]:
                     await asyncio.to_thread(shutil.rmtree, old_backup)
                     logger.info("MemoryPersistence", f"Pruned old backup {old_backup.name}")
        except Exception as e:
            logger.error("MemoryPersistence", "Error pruning backups", {"error": str(e)})

    async def save_assembly(self, assembly: 'MemoryAssembly') -> bool:
        """Save a memory assembly to disk.
        
        Args:
            assembly: The MemoryAssembly object to save
            
        Returns:
            bool: Success status
        """
        if not assembly or not assembly.assembly_id:
            logger.error("MemoryPersistence", "Cannot save assembly: Invalid or empty assembly object")
            return False
            
        try:
            # Create assemblies directory if it doesn't exist
            assembly_dir = self.storage_path / 'assemblies'
            assembly_dir.mkdir(exist_ok=True, parents=True)
            
            # Generate a filename based on the assembly ID
            file_path = assembly_dir / f"{assembly.assembly_id}.json"
            
            # Convert the assembly to a serializable dict
            assembly_dict = assembly.to_dict()
            
            # Validate critical fields before serialization
            if not assembly_dict.get('assembly_id') or not assembly_dict.get('name'):
                logger.error("MemoryPersistence", "Cannot save assembly: Missing required fields", 
                            {"id": assembly.assembly_id})
                return False

            # Write the assembly to disk
            async with aiofiles.open(file_path, 'w') as f:
                # Use the same serializer as for memories
                def default_serializer(obj):
                    if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,
                                        np.int16, np.int32, np.int64, np.uint8,
                                        np.uint16, np.uint32, np.uint64)):
                        return int(obj)
                    elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
                        return float(obj)
                    elif isinstance(obj, (np.ndarray,)):
                        return obj.tolist()
                    elif isinstance(obj, set):
                        return list(obj)
                    elif isinstance(obj, datetime):
                        return obj.isoformat()
                    try:
                        # Fallback for other types
                        return str(obj)
                    except:
                        return "[Unserializable Object]"
                try:
                    json_data = json.dumps(assembly_dict, indent=2, default=default_serializer)
                    await f.write(json_data)
                except (TypeError, OverflowError) as json_err:
                    logger.error("MemoryPersistence", f"JSON serialization error for assembly {assembly.assembly_id}", 
                                {"error": str(json_err), "type": type(json_err).__name__})
                    return False
            
            # Update the memory index with assembly info
            self.memory_index[assembly.assembly_id] = {
                'path': str(file_path.relative_to(self.storage_path)),
                'timestamp': assembly.creation_time,
                'type': 'assembly',
                'name': assembly.name
            }
            
            # Save the memory index
            await self._save_index()
            
            self.stats['assembly_saves'] = self.stats.get('assembly_saves', 0) + 1
            logger.info("MemoryPersistence", f"Saved assembly {assembly.assembly_id}", {"name": assembly.name})
            return True
        except Exception as e:
            logger.error("MemoryPersistence", f"Error saving assembly {assembly.assembly_id}", 
                        {"error": str(e), "type": type(e).__name__})
            self.stats['failed_assembly_saves'] = self.stats.get('failed_assembly_saves', 0) + 1
            return False

    async def load_assembly(self, assembly_id: str, geometry_manager) -> Optional['MemoryAssembly']:
        """Load a memory assembly from disk.
        
        Args:
            assembly_id: ID of the assembly to load
            geometry_manager: GeometryManager instance required for assembly initialization
            
        Returns:
            MemoryAssembly or None if not found
        """
        if not assembly_id:
            logger.error("MemoryPersistence", "Cannot load assembly: Invalid or empty assembly_id")
            return None
            
        if not geometry_manager:
            logger.error("MemoryPersistence", f"Cannot load assembly {assembly_id}: GeometryManager is required")
            return None
            
        async with self._lock:
            try:
                # Check if in index first
                if assembly_id in self.memory_index and self.memory_index[assembly_id].get('type') == 'assembly':
                    file_path = self.storage_path / self.memory_index[assembly_id]['path']
                else:
                    # Fallback: check filesystem directly
                    file_path = self.storage_path / 'assemblies' / f"{assembly_id}.json"
                    if not await asyncio.to_thread(os.path.exists, file_path):
                        logger.warning("MemoryPersistence", f"Assembly {assembly_id} not found")
                        return None
                    # If found directly, update index
                    self.memory_index[assembly_id] = {
                        'path': f"assemblies/{assembly_id}.json",
                        'type': 'assembly'
                    }

                # Read and parse the assembly file
                try:
                    async with aiofiles.open(file_path, 'r') as f:
                        content = await f.read()
                        
                    try:
                        assembly_dict = json.loads(content)
                    except json.JSONDecodeError as json_err:
                        logger.error("MemoryPersistence", f"JSON parsing error for assembly {assembly_id}", 
                                    {"error": str(json_err), "file": str(file_path)})
                        return None
                        
                    # Validate required fields
                    if not assembly_dict.get('assembly_id') or 'memories' not in assembly_dict:
                        logger.error("MemoryPersistence", f"Invalid assembly data format for {assembly_id}",
                                    {"missing_fields": [k for k in ['assembly_id', 'memories'] if k not in assembly_dict]})
                        return None
                    
                    # Create assembly from dict with error handling
                    try:   
                        assembly = MemoryAssembly.from_dict(assembly_dict, geometry_manager)
                    except (KeyError, ValueError, TypeError) as e:
                        logger.error("MemoryPersistence", f"Error reconstructing assembly {assembly_id} from dict", 
                                    {"error": str(e), "type": type(e).__name__})
                        return None
                        
                    self.stats['assembly_loads'] = self.stats.get('assembly_loads', 0) + 1
                    logger.info("MemoryPersistence", f"Loaded assembly {assembly_id}", {"name": assembly.name})
                    return assembly
                    
                except FileNotFoundError:
                    logger.warning("MemoryPersistence", f"Assembly file not found for {assembly_id}", 
                                 {"expected_path": str(file_path)})
                    # Remove from index if file doesn't exist
                    if assembly_id in self.memory_index:
                        del self.memory_index[assembly_id]
                        await self._save_index()
                    return None

            except Exception as e:
                logger.error("MemoryPersistence", f"Error loading assembly {assembly_id}", 
                            {"error": str(e), "type": type(e).__name__})
                self.stats['failed_assembly_loads'] = self.stats.get('failed_assembly_loads', 0) + 1
                return None

    async def list_assemblies(self) -> List[Dict[str, Any]]:
        """List all memory assemblies.
        
        Returns:
            List of assembly metadata dictionaries
        """
        async with self._lock:
            try:
                assemblies = []
                for memory_id, info in self.memory_index.items():
                    if info.get('type') == 'assembly':
                        assemblies.append({
                            'id': memory_id,
                            'path': info['path'],
                            'timestamp': info.get('timestamp', 0)
                        })
                return assemblies
            except Exception as e:
                logger.error("MemoryPersistence", "Error listing assemblies", {"error": str(e)})
                return []

    async def delete_assembly(self, assembly_id: str) -> bool:
        """Delete a memory assembly.
        
        Args:
            assembly_id: ID of the assembly to delete
            
        Returns:
            bool: Success status
        """
        async with self._lock:
            try:
                if assembly_id not in self.memory_index or self.memory_index[assembly_id].get('type') != 'assembly':
                    logger.warning("MemoryPersistence", f"Assembly {assembly_id} not found for deletion")
                    return False
                    
                file_path = self.storage_path / self.memory_index[assembly_id]['path']
                if await asyncio.to_thread(os.path.exists, file_path):
                    await asyncio.to_thread(os.remove, file_path)
                    
                # Remove from index
                del self.memory_index[assembly_id]
                await self._save_index()
                
                self.stats['assembly_deletes'] = self.stats.get('assembly_deletes', 0) + 1
                logger.info("MemoryPersistence", f"Deleted assembly {assembly_id}")
                return True
            except Exception as e:
                logger.error("MemoryPersistence", f"Error deleting assembly {assembly_id}", {"error": str(e)})
                self.stats['failed_assembly_deletes'] = self.stats.get('failed_assembly_deletes', 0) + 1
                return False

    async def shutdown(self):
        """Ensure final index save and perform cleanup."""
        logger.info("MemoryPersistence", "Shutting down...")
        try:
            loop = asyncio.get_running_loop()
            if not loop.is_running():
                logger.warning("MemoryPersistence", "No running event loop during shutdown")
                return
        except RuntimeError:
            logger.warning("MemoryPersistence", "No running event loop during shutdown")
            return
        await self._save_index()  # Ensure final save
        logger.info("MemoryPersistence", "Shutdown complete.")

    def get_stats(self) -> Dict[str, Any]:
        """Get persistence statistics."""
        if not self._initialized:
            # Avoid calling initialize here as it might block if called sync
            logger.warning("MemoryPersistence", "Stats requested before initialization, index count may be inaccurate.")
        
        # Return stats with possibly unloaded index count
        return {
            "total_indexed_items": len(self.memory_index),
            "initialized": self._initialized,
            "last_index_update": self.stats.get('last_index_update', 0),
            "last_backup": self.stats.get('last_backup', 0),
            "saves": self.stats.get('saves', 0),
            "loads": self.stats.get('loads', 0),
            "deletes": self.stats.get('deletes', 0),
            "backups": self.stats.get('backups', 0),
            "errors": self.stats.get('errors', 0)
        }

```

# memory_structures.py

```py
# synthians_memory_core/memory_structures.py

import time
import uuid
import numpy as np
import torch
from typing import Dict, Any, Optional, List, Union, Set
from dataclasses import dataclass, field
from datetime import datetime, timezone  # Add datetime imports

from .custom_logger import logger # Use the shared custom logger

@dataclass
class MemoryEntry:
    """Standardized container for a single memory entry."""
    content: str
    embedding: Optional[np.ndarray] = None
    id: str = field(default_factory=lambda: f"mem_{uuid.uuid4().hex[:12]}")
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    quickrecal_score: float = 0.5
    quickrecal_updated: Optional[datetime] = None  # Add missing field
    metadata: Dict[str, Any] = field(default_factory=dict)
    access_count: int = 0
    last_access_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    # Hyperbolic specific
    hyperbolic_embedding: Optional[np.ndarray] = None

    def __post_init__(self):
        self.quickrecal_score = max(0.0, min(1.0, self.quickrecal_score))
        
        # Convert timestamp/last_access_time from potential float on init if needed
        if isinstance(self.timestamp, (int, float)):
            self.timestamp = datetime.fromtimestamp(self.timestamp, timezone.utc)
        if isinstance(self.last_access_time, (int, float)):
            self.last_access_time = datetime.fromtimestamp(self.last_access_time, timezone.utc)

        # Ensure embedding is numpy array
        if self.embedding is not None and not isinstance(self.embedding, np.ndarray):
            if isinstance(self.embedding, torch.Tensor):
                self.embedding = self.embedding.detach().cpu().numpy()
            elif isinstance(self.embedding, list):
                self.embedding = np.array(self.embedding, dtype=np.float32)
            else:
                logger.warning("MemoryEntry", f"Unsupported embedding type {type(self.embedding)} for ID {self.id}, clearing.")
                self.embedding = None

        if self.hyperbolic_embedding is not None and not isinstance(self.hyperbolic_embedding, np.ndarray):
            if isinstance(self.hyperbolic_embedding, torch.Tensor):
                self.hyperbolic_embedding = self.hyperbolic_embedding.detach().cpu().numpy()
            elif isinstance(self.hyperbolic_embedding, list):
                 self.hyperbolic_embedding = np.array(self.hyperbolic_embedding, dtype=np.float32)
            else:
                logger.warning("MemoryEntry", f"Unsupported hyperbolic embedding type {type(self.hyperbolic_embedding)} for ID {self.id}, clearing.")
                self.hyperbolic_embedding = None

    def record_access(self):
        self.access_count += 1
        self.last_access_time = datetime.now(timezone.utc)

    def get_effective_quickrecal(self, decay_rate: float = 0.05) -> float:
        """Calculate effective QuickRecal score with time decay."""
        # Calculate age using datetime objects
        age_seconds = (datetime.now(timezone.utc) - self.timestamp).total_seconds()
        age_days = age_seconds / 86400
        if age_days < 1: return self.quickrecal_score
        importance_factor = 0.5 + (0.5 * self.quickrecal_score)
        effective_decay_rate = decay_rate / max(0.1, importance_factor)  # Avoid division by zero
        decay_factor = np.exp(-effective_decay_rate * (age_days - 1))
        return max(0.0, min(1.0, self.quickrecal_score * decay_factor))

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "id": self.id,
            "content": self.content,
            "embedding": self.embedding.tolist() if self.embedding is not None else None,
            "timestamp": self.timestamp.isoformat() if hasattr(self.timestamp, 'isoformat') else self.timestamp,
            "quickrecal_score": self.quickrecal_score,
            "quickrecal_updated": self.quickrecal_updated.isoformat() if hasattr(self.quickrecal_updated, 'isoformat') and self.quickrecal_updated else None,
            "metadata": self.metadata,
            "access_count": self.access_count,
            "last_access_time": self.last_access_time.isoformat() if hasattr(self.last_access_time, 'isoformat') else self.last_access_time,
            "hyperbolic_embedding": self.hyperbolic_embedding.tolist() if self.hyperbolic_embedding is not None else None
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryEntry':
        """Create memory from dictionary."""
        mem_id = data.get("id", "unknown_id") # Get ID for logging
        logger.debug("MemoryEntry.from_dict", f"Creating entry for ID: {mem_id}")
        
        try:
            embedding = np.array(data["embedding"], dtype=np.float32) if data.get("embedding") else None
            hyperbolic = np.array(data["hyperbolic_embedding"], dtype=np.float32) if data.get("hyperbolic_embedding") else None
        except Exception as e:
            logger.error("MemoryEntry.from_dict", f"Error processing embedding for ID {mem_id}", {"error": str(e)})
            embedding = None
            hyperbolic = None
            
        # Handle legacy 'significance' field
        quickrecal = data.get("quickrecal_score", data.get("significance", 0.5))

        # Helper to parse timestamp (float or ISO string)
        def parse_datetime(ts_data, field_name):
            if ts_data is None: return None
            try:
                if isinstance(ts_data, str):
                    # Handle potential Z suffix for UTC
                    if ts_data.endswith('Z'): ts_data = ts_data[:-1] + '+00:00'
                    return datetime.fromisoformat(ts_data)
                elif isinstance(ts_data, (int, float)):
                    return datetime.fromtimestamp(ts_data, timezone.utc)
                # --- ADDED Logging ---
                logger.warning("MemoryEntry.from_dict", f"Unsupported timestamp type for {field_name} in ID {mem_id}: {type(ts_data)}")
                return None
            except Exception as e:
                 # --- ADDED Logging ---
                 logger.error("MemoryEntry.from_dict", f"Error parsing {field_name} for ID {mem_id}", {"value": ts_data, "error": str(e)})
                 return None

        timestamp = parse_datetime(data.get("timestamp"), "timestamp") or datetime.now(timezone.utc)
        last_access = parse_datetime(data.get("last_access_time"), "last_access_time") or datetime.now(timezone.utc)
        qr_updated = parse_datetime(data.get("quickrecal_updated"), "quickrecal_updated")

        try:
            entry = cls(
                content=data["content"],
                embedding=embedding,
                id=mem_id, # Use pre-fetched ID
                timestamp=timestamp,
                quickrecal_score=quickrecal,
                quickrecal_updated=qr_updated,
                metadata=data.get("metadata", {}),
                access_count=data.get("access_count", 0),
                last_access_time=last_access,
                hyperbolic_embedding=hyperbolic
            )
            logger.debug("MemoryEntry.from_dict", f"Successfully created entry for ID: {mem_id}")
            return entry
        except Exception as e:
             # --- ADDED Logging ---
             logger.error("MemoryEntry.from_dict", f"Error during final object creation for ID {mem_id}", {"error": str(e)}, exc_info=True)
             raise # Re-raise after logging if creation fails fundamentally

class MemoryAssembly:
    """Represents a group of related memories forming a coherent assembly."""
    def __init__(self,
                 geometry_manager, # Pass GeometryManager for consistency
                 assembly_id: str = None,
                 name: str = None,
                 description: str = None):
        self.geometry_manager = geometry_manager
        self.assembly_id = assembly_id or f"asm_{uuid.uuid4().hex[:12]}"
        self.name = name or f"Assembly-{self.assembly_id[:8]}"
        self.description = description or ""
        self.creation_time = datetime.now(timezone.utc)
        self.last_access_time = self.creation_time
        self.access_count = 0

        self.memories: Set[str] = set()  # IDs of memories in this assembly
        self.composite_embedding: Optional[np.ndarray] = None
        self.hyperbolic_embedding: Optional[np.ndarray] = None
        self.emotion_profile: Dict[str, float] = {}
        self.keywords: Set[str] = set()
        self.activation_level: float = 0.0
        self.activation_decay_rate: float = 0.05

    def add_memory(self, memory: MemoryEntry):
        """Add a memory and update assembly properties."""
        if memory.id in self.memories:
            return False
        self.memories.add(memory.id)

        # --- Update Composite Embedding ---
        if memory.embedding is not None:
            target_dim = self.geometry_manager.config['embedding_dim']
            # Align memory embedding to target dimension
            mem_emb = memory.embedding
            if mem_emb.shape[0] != target_dim:
                 aligned_mem_emb, _ = self.geometry_manager._align_vectors(mem_emb, np.zeros(target_dim))
            else:
                 aligned_mem_emb = mem_emb

            normalized_mem_emb = self.geometry_manager._normalize(aligned_mem_emb)

            if self.composite_embedding is None:
                self.composite_embedding = normalized_mem_emb
            else:
                # Align composite embedding if needed (should already be target_dim)
                if self.composite_embedding.shape[0] != target_dim:
                     aligned_comp_emb, _ = self.geometry_manager._align_vectors(self.composite_embedding, np.zeros(target_dim))
                else:
                     aligned_comp_emb = self.composite_embedding

                normalized_composite = self.geometry_manager._normalize(aligned_comp_emb)

                # Simple averaging (could be weighted later)
                n = len(self.memories)
                self.composite_embedding = ((n - 1) * normalized_composite + normalized_mem_emb) / n
                # Re-normalize
                self.composite_embedding = self.geometry_manager._normalize(self.composite_embedding)

            # Update hyperbolic embedding if enabled
            if self.geometry_manager.config['geometry_type'] == GeometryType.HYPERBOLIC:
                self.hyperbolic_embedding = self.geometry_manager._to_hyperbolic(self.composite_embedding)

        # --- Update Emotion Profile ---
        mem_emotion = memory.metadata.get("emotional_context", {})
        if mem_emotion:
            self._update_emotion_profile(mem_emotion)

        # --- Update Keywords ---
        # Simple keyword extraction (could use NLP later)
        content_words = set(re.findall(r'\b\w{3,}\b', memory.content.lower()))
        self.keywords.update(content_words)
        # Limit keyword set size if needed
        if len(self.keywords) > 100:
            # Simple strategy: keep most frequent or randomly sample
            pass # Placeholder for keyword pruning logic

        return True

    def _update_emotion_profile(self, mem_emotion: Dict[str, Any]):
        """Update aggregated emotional profile."""
        n = len(self.memories)
        for emotion, score in mem_emotion.get("emotions", {}).items():
            current_score = self.emotion_profile.get(emotion, 0.0)
            # Weighted average (giving slightly more weight to existing profile)
            self.emotion_profile[emotion] = (current_score * (n - 1) * 0.6 + score * 0.4) / max(1, (n - 1) * 0.6 + 0.4)

    def get_similarity(self, query_embedding: np.ndarray) -> float:
        """Calculate similarity between query and assembly embedding."""
        ref_embedding = self.hyperbolic_embedding if self.geometry_manager.config['geometry_type'] == GeometryType.HYPERBOLIC and self.hyperbolic_embedding is not None else self.composite_embedding

        if ref_embedding is None:
            return 0.0

        return self.geometry_manager.calculate_similarity(query_embedding, ref_embedding)

    def activate(self, level: float):
        self.activation_level = min(1.0, max(0.0, level))
        self.last_access_time = datetime.now(timezone.utc)
        self.access_count += 1

    def decay_activation(self):
        self.activation_level = max(0.0, self.activation_level - self.activation_decay_rate)

    def to_dict(self) -> Dict[str, Any]:
        """Convert assembly to dictionary."""
        return {
            "assembly_id": self.assembly_id,
            "name": self.name,
            "description": self.description,
            "creation_time": self.creation_time.isoformat(),
            "last_access_time": self.last_access_time.isoformat(),
            "access_count": self.access_count,
            "memory_ids": list(self.memories),
            "composite_embedding": self.composite_embedding.tolist() if self.composite_embedding is not None else None,
            "hyperbolic_embedding": self.hyperbolic_embedding.tolist() if self.hyperbolic_embedding is not None else None,
            "emotion_profile": self.emotion_profile,
            "keywords": list(self.keywords),
            "activation_level": self.activation_level
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any], geometry_manager) -> 'MemoryAssembly':
        """Create assembly from dictionary."""
        assembly = cls(
            geometry_manager,
            assembly_id=data["assembly_id"],
            name=data["name"],
            description=data["description"]
        )
        assembly.creation_time = datetime.fromisoformat(data.get("creation_time"))
        assembly.last_access_time = datetime.fromisoformat(data.get("last_access_time"))
        assembly.access_count = data.get("access_count", 0)
        assembly.memories = set(data.get("memory_ids", []))
        assembly.composite_embedding = np.array(data["composite_embedding"], dtype=np.float32) if data.get("composite_embedding") else None
        assembly.hyperbolic_embedding = np.array(data["hyperbolic_embedding"], dtype=np.float32) if data.get("hyperbolic_embedding") else None
        assembly.emotion_profile = data.get("emotion_profile", {})
        assembly.keywords = set(data.get("keywords", []))
        assembly.activation_level = data.get("activation_level", 0.0)
        return assembly

```

# metadata_synthesizer.py

```py
import time
import datetime
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
import numpy as np
import json

from .custom_logger import logger

# Define the current metadata schema version
METADATA_SCHEMA_VERSION = "1.0.0"

class MetadataSynthesizer:
    """
    Enriches memory entries with synthesized metadata derived from content analysis,
    embedding characteristics, and contextual information.
    
    This class serves as a modular pipeline for extracting, computing, and assembling
    metadata fields that add semantic richness to memory entries beyond their raw content.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize the MetadataSynthesizer with configuration options.
        
        Args:
            config: Configuration dictionary for customizing metadata synthesis behavior
        """
        self.config = config or {}
        self.metadata_processors = [
            self._process_base_metadata,   # Always process base metadata first (versioning, etc)
            self._process_temporal_metadata,
            self._process_emotional_metadata,
            self._process_cognitive_metadata,
            self._process_embedding_metadata,
            self._process_identifiers_and_basic_stats  # Add identifiers and basic stats processor
        ]
        logger.info("MetadataSynthesizer", "Initialized with processors")
    
    async def synthesize(self, 
                   content: str, 
                   embedding: Optional[np.ndarray] = None,
                   base_metadata: Optional[Dict[str, Any]] = None,
                   emotion_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Synthesize rich metadata from content, embedding, and optional existing metadata.
        
        Args:
            content: The text content of the memory
            embedding: Vector representation of the content (optional)
            base_metadata: Existing metadata to build upon (optional)
            emotion_data: Pre-computed emotion analysis results (optional)
            
        Returns:
            Enriched metadata dictionary with synthesized fields
        """
        # Start with base metadata or empty dict
        metadata = base_metadata or {}
        
        # Track original fields to identify what we've added
        original_keys = set(metadata.keys())
        
        # Process through each metadata processor
        context = {
            'content': content,
            'embedding': embedding,
            'emotion_data': emotion_data,
            'original_metadata': base_metadata
        }
        
        # Run all processors
        for processor in self.metadata_processors:
            try:
                processor_result = processor(metadata, context)
                
                # Handle both synchronous and asynchronous processor results
                if processor_result and hasattr(processor_result, '__await__'):
                    metadata = await processor_result
            except Exception as e:
                logger.error("MetadataSynthesizer", f"Error in processor {processor.__name__}: {str(e)}")
        
        # Log what was added
        added_keys = set(metadata.keys()) - original_keys
        logger.info("MetadataSynthesizer", f"Added metadata fields: {list(added_keys)}")
        
        return metadata
    
    def synthesize_sync(self, 
                   content: str, 
                   embedding: Optional[np.ndarray] = None,
                   base_metadata: Optional[Dict[str, Any]] = None,
                   emotion_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Synchronous version of synthesize for contexts where async cannot be used.
        
        Args:
            content: The text content of the memory
            embedding: Vector representation of the content (optional)
            base_metadata: Existing metadata to build upon (optional)
            emotion_data: Pre-computed emotion analysis results (optional)
            
        Returns:
            Enriched metadata dictionary with synthesized fields
        """
        # Start with base metadata or empty dict
        metadata = base_metadata or {}
        
        # Track original fields to identify what we've added
        original_keys = set(metadata.keys())
        
        # Process through each metadata processor
        context = {
            'content': content,
            'embedding': embedding,
            'emotion_data': emotion_data,
            'original_metadata': base_metadata
        }
        
        # Run all processors (synchronously)
        for processor in self.metadata_processors:
            try:
                processor_result = processor(metadata, context)
                
                # Since we're in sync mode, we skip any async processors
                if processor_result and not hasattr(processor_result, '__await__'):
                    metadata = processor_result
            except Exception as e:
                logger.error("MetadataSynthesizer", f"Error in processor {processor.__name__}: {str(e)}")
        
        # Log what was added
        added_keys = set(metadata.keys()) - original_keys
        logger.info("MetadataSynthesizer", f"Added metadata fields: {list(added_keys)}")
        
        return metadata
    
    def _process_base_metadata(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add base metadata fields including:
        - metadata_schema_version
        - creation_time
        """
        # Add metadata schema version
        metadata['metadata_schema_version'] = METADATA_SCHEMA_VERSION
        
        # Add creation time
        metadata['creation_time'] = time.time()
        
        return metadata
    
    def _process_temporal_metadata(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add time-related metadata including:
        - timestamp (if not already present)
        - time_of_day (morning, afternoon, evening, night)
        - day_of_week
        - is_weekend
        """
        # Ensure timestamp exists and is a float
        if 'timestamp' not in metadata:
            metadata['timestamp'] = float(time.time())
        else:
            # Ensure timestamp is a float to avoid serialization issues
            try:
                metadata['timestamp'] = float(metadata['timestamp'])
            except (ValueError, TypeError):
                logger.warning("MetadataSynthesizer", f"Invalid timestamp format {metadata['timestamp']}, using current time")
                metadata['timestamp'] = float(time.time())
            
        # Convert timestamp to datetime
        dt = datetime.datetime.fromtimestamp(metadata['timestamp'])
        
        # Add ISO-formatted timestamp for convenience (guarantees serialization compatibility)
        metadata['timestamp_iso'] = dt.isoformat()
        
        # Add temporal markers
        hour = dt.hour
        if 5 <= hour < 12:
            time_of_day = 'morning'
        elif 12 <= hour < 17:
            time_of_day = 'afternoon'
        elif 17 <= hour < 22:
            time_of_day = 'evening'
        else:
            time_of_day = 'night'
            
        # Add temporal metadata
        metadata['time_of_day'] = time_of_day
        metadata['day_of_week'] = dt.strftime('%A').lower()
        metadata['is_weekend'] = dt.weekday() >= 5  # 5 = Saturday, 6 = Sunday
        metadata['month'] = dt.strftime('%B').lower()
        metadata['year'] = dt.year
        
        # Debug log the temporal metadata
        logger.debug("MetadataSynthesizer", "Temporal metadata processed", {
            'timestamp': metadata.get('timestamp'),
            'time_of_day': metadata.get('time_of_day'),
            'day_of_week': metadata.get('day_of_week'),
            'is_weekend': metadata.get('is_weekend')
        })
        
        return metadata
    
    def _process_emotional_metadata(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add emotion-related metadata including:
        - dominant_emotion
        - sentiment_value
        - emotional_intensity
        """
        # Use pre-computed emotion data if available
        emotion_data = context.get('emotion_data')
        
        if emotion_data and isinstance(emotion_data, dict):
            # Extract emotions from the expected location in the emotion_data
            emotions = emotion_data.get('emotions', {})
            
            if isinstance(emotions, dict) and emotions:
                # Ensure we're not duplicating emotion data
                if 'emotions' in metadata:
                    # If emotions already exists in metadata, avoid nesting
                    # Just log it and use the one from emotion_data
                    logger.debug("MetadataSynthesizer", "Emotions already present in metadata, overwriting")
                
                # Copy relevant emotion data to metadata
                if emotions.get('dominant_emotion') is not None:
                    metadata['dominant_emotion'] = emotions.get('dominant_emotion')
                elif 'dominant_emotion' in emotion_data:
                    metadata['dominant_emotion'] = emotion_data.get('dominant_emotion')
                    
                if emotions.get('sentiment_value') is not None:
                    sentiment = emotions.get('sentiment_value')
                    metadata['sentiment_value'] = float(sentiment) # Ensure it's a float
                    # Add a simple polarity label
                    if sentiment > 0.2:
                        metadata['sentiment_polarity'] = 'positive'
                    elif sentiment < -0.2:
                        metadata['sentiment_polarity'] = 'negative'
                    else:
                        metadata['sentiment_polarity'] = 'neutral'
                
                if emotions.get('intensity') is not None:
                    metadata['emotional_intensity'] = float(emotions.get('intensity', 0.5)) # Ensure it's a float
        
        # Ensure mandatory emotional fields are present with safe default values
        if 'dominant_emotion' not in metadata:
            metadata['dominant_emotion'] = 'neutral'  # Default
        
        if 'sentiment_polarity' not in metadata:
            metadata['sentiment_polarity'] = 'neutral' # Default
            
        if 'sentiment_value' not in metadata:
            metadata['sentiment_value'] = 0.0  # Default neutral sentiment
            
        if 'emotional_intensity' not in metadata:
            metadata['emotional_intensity'] = 0.5  # Default (medium intensity)
            
        # Debug log the final emotional metadata
        logger.debug("MetadataSynthesizer", "Emotional metadata processed", {
            'dominant_emotion': metadata.get('dominant_emotion'),
            'sentiment_polarity': metadata.get('sentiment_polarity'),
            'emotional_intensity': metadata.get('emotional_intensity')
        })
            
        return metadata
    
    def _process_cognitive_metadata(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add cognitive-related metadata including:
        - complexity_estimate
        - word_count
        - cognitive_load_estimate
        """
        content = context.get('content', '')
        
        # Simple metrics based on content
        word_count = len(content.split())
        metadata['word_count'] = word_count
        
        # Estimate complexity (very simple heuristic)
        avg_word_length = sum(len(word) for word in content.split()) / max(1, word_count)
        sentence_count = content.count('.') + content.count('!') + content.count('?')
        sentence_count = max(1, sentence_count)  # Avoid division by zero
        
        words_per_sentence = word_count / sentence_count
        
        # Simplified complexity score (0-1 range)
        complexity = min(1.0, ((avg_word_length / 10) + (words_per_sentence / 25)) / 2)
        metadata['complexity_estimate'] = float(complexity)
        
        # Cognitive load is a factor of complexity and length
        cognitive_load = min(1.0, (complexity * 0.7) + (min(1.0, word_count / 500) * 0.3))
        metadata['cognitive_load_estimate'] = float(cognitive_load)
        
        return metadata
    
    def _process_embedding_metadata(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract metadata from embedding characteristics:
        - embedding_norm
        - embedding_sparsity
        - embedding_dim
        - embedding_valid
        """
        embedding = context.get('embedding')
        
        if embedding is not None:
            # Extract embedding characteristics
            try:
                # First validate the embedding
                embedding, is_valid = self._validate_embedding(embedding)
                metadata['embedding_valid'] = is_valid
                
                # Calculate embedding norm (magnitude)
                embedding_norm = float(np.linalg.norm(embedding))
                metadata['embedding_norm'] = embedding_norm
                
                # Calculate sparsity (percent of near-zero values)
                near_zero = np.abs(embedding) < 0.01
                sparsity = float(np.mean(near_zero))
                metadata['embedding_sparsity'] = sparsity
                
                # Store embedding dimension
                metadata['embedding_dim'] = embedding.shape[0]
                
                # Log the embedding metadata
                logger.debug("MetadataSynthesizer", "Embedding metadata processed", {
                    'valid': metadata.get('embedding_valid'),
                    'norm': metadata.get('embedding_norm'),
                    'sparsity': metadata.get('embedding_sparsity'),
                    'dim': metadata.get('embedding_dim')
                })
            except Exception as e:
                logger.warning("MetadataSynthesizer", f"Error processing embedding metadata: {str(e)}")
                metadata['embedding_valid'] = False
        else:
            # No embedding available
            metadata['embedding_valid'] = False
        
        return metadata
        
    def _validate_embedding(self, embedding: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        Validate an embedding vector and replace with zeros if invalid.
        
        Args:
            embedding: The embedding vector to validate
            
        Returns:
            Tuple of (possibly_fixed_embedding, is_valid)
        """
        # Check for None
        if embedding is None:
            return np.zeros(768), False
            
        # Convert to numpy array if not already
        if not isinstance(embedding, np.ndarray):
            try:
                embedding = np.array(embedding, dtype=np.float32)
            except Exception:
                return np.zeros(768), False
        
        # Check for NaN or Inf values
        if np.isnan(embedding).any() or np.isinf(embedding).any():
            logger.warning("MetadataSynthesizer", "Embedding contains NaN or Inf values, replacing with zeros")
            # Create a zero vector of the same shape
            return np.zeros_like(embedding), False
            
        # Check if the vector is all zeros
        if np.all(embedding == 0):
            return embedding, False
            
        return embedding, True
        
    def _align_vectors_for_comparison(self, vec1: np.ndarray, vec2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Align two vectors to the same dimension for comparison operations.
        Will pad the smaller vector with zeros or truncate the larger one.
        
        Args:
            vec1: First vector
            vec2: Second vector
            
        Returns:
            Tuple of (aligned_vec1, aligned_vec2)
        """
        # Make sure both are numpy arrays
        if not isinstance(vec1, np.ndarray):
            vec1 = np.array(vec1, dtype=np.float32)
        if not isinstance(vec2, np.ndarray):
            vec2 = np.array(vec2, dtype=np.float32)
            
        # Get dimensions
        dim1 = vec1.shape[0]
        dim2 = vec2.shape[0]
        
        # If dimensions match, no alignment needed
        if dim1 == dim2:
            return vec1, vec2
            
        # Need to align dimensions
        if dim1 < dim2:
            # Pad vec1 with zeros
            aligned_vec1 = np.zeros(dim2, dtype=np.float32)
            aligned_vec1[:dim1] = vec1
            return aligned_vec1, vec2
        else:
            # Pad vec2 with zeros
            aligned_vec2 = np.zeros(dim1, dtype=np.float32)
            aligned_vec2[:dim2] = vec2
            return vec1, aligned_vec2

    def _process_identifiers_and_basic_stats(self, metadata: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Adds memory ID (uuid) and content length if available.
        This should run after base metadata and before final memory entry creation.
        """
        content = context.get('content', '')
        
        # Add length (raw character count)
        if 'length' not in metadata:
            metadata['length'] = len(content)
        
        # NOTE: 'uuid' (aka memory_id) must be passed externally if you want it included,
        # or you can let the core insert it *after* memory creation if needed.
        
        return metadata

```

# orchestrator\__init__.py

```py
# Orchestrator module for managing bi-hemispheric cognitive flow
# between Memory Core and Sequence Trainer

```

# orchestrator\context_cascade_engine.py

```py
import os
import json
import time
import asyncio
import logging
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
import aiohttp
from datetime import datetime
from urllib.parse import urljoin

# Import the sequence context manager
from .history import SequenceContextManager

# Import the titans variants - note we're importing the type and factory function
# but not directly importing the variant classes which would trigger TensorFlow import
from .titans_variants import TitansVariantType, create_titans_variant

logger = logging.getLogger(__name__)

class ContextCascadeEngine:
    """Orchestrates the bi-hemispheric cognitive flow between Memory Core and Neural Memory.
    
    This engine implements the Context Cascade design pattern, enabling:
    1. Storage of memory entries with embeddings in Memory Core
    2. Test-time learning in Neural Memory via associations
    3. Detection of surprise when expectations don't match reality
    4. Feedback of surprise to enhance memory retrieval
    5. Dynamic adaptation of memory importance based on narrative patterns
    """

    def __init__(self,
                 memory_core_url: str = "http://localhost:5010",  
                 neural_memory_url: str = "http://localhost:8001",  
                 geometry_manager: Optional[Any] = None,
                 metrics_enabled: bool = True,
                 sequence_context_length: int = 50):
        """Initialize the Context Cascade Engine.
        
        Args:
            memory_core_url: URL of the Memory Core service
            neural_memory_url: URL of the Neural Memory Server
            geometry_manager: Optional shared geometry manager
            metrics_enabled: Whether to enable cognitive metrics collection
            sequence_context_length: Maximum length of the sequence context buffer
        """
        self.memory_core_url = memory_core_url.rstrip('/')
        self.neural_memory_url = neural_memory_url.rstrip('/')

        if geometry_manager is None:
            raise ImportError("GeometryManager could not be imported. ContextCascadeEngine cannot function.")
        self.geometry_manager = geometry_manager  

        # Initialize metrics collection if enabled
        self.metrics_enabled = metrics_enabled
        self._current_intent_id = None
        if self.metrics_enabled:
            try:
                from synthians_memory_core.synthians_trainer_server.metrics_store import MetricsStore, get_metrics_store
                self.metrics_store = get_metrics_store()
                logger.info("Cognitive metrics collection enabled")
            except Exception as e:
                logger.warning(f"Failed to initialize metrics collection: {e}")
                self.metrics_enabled = False

        self.last_retrieved_embedding: Optional[List[float]] = None
        
        # Initialize sequence context manager for attention history
        self.sequence_context_length = sequence_context_length
        self.sequence_context_manager = SequenceContextManager(max_length=self.sequence_context_length)
        
        # Keep the legacy sequence_context list for backward compatibility
        self.sequence_context: List[Dict[str, Any]] = []
        self.processing_lock = asyncio.Lock()
        
        # Determine active Titans variant from environment
        variant_name_str = os.environ.get("TITANS_VARIANT", "NONE").upper()
        try:
            self.active_variant_type = TitansVariantType(variant_name_str)
        except ValueError:
            logger.warning(f"Invalid TITANS_VARIANT '{variant_name_str}'. Defaulting to NONE.")
            self.active_variant_type = TitansVariantType.NONE
        logger.info(f"Active Titans Variant: {self.active_variant_type.value}")
        
        # Configuration ready flag and event
        self._config_ready = False
        self._config_ready_event = asyncio.Event()
        self.variant_processor = None
        
        # Trigger dynamic configuration
        asyncio.create_task(self._configure_and_set_ready())
        
        logger.info(f"Context Cascade Engine initializing:")
        logger.info(f" - Memory Core URL: {self.memory_core_url}")
        logger.info(f" - Neural Memory URL: {self.neural_memory_url}")
        logger.info(f" - Metrics Enabled: {self.metrics_enabled}")
        logger.info(f" - Sequence Context Length: {self.sequence_context_length}")
        logger.info(f" - Active Titans Variant: {self.active_variant_type.value}")
        gm_config = getattr(self.geometry_manager, 'config', {})
        logger.info(f" - Geometry type: {gm_config.get('geometry_type', 'N/A')}")
        logger.info(f" - Dynamic configuration in progress...")
        
    async def _configure_and_set_ready(self):
        """Initialize configuration and set the ready flag when complete."""
        try:
            await self._configure_attention_and_variant()
            self._config_ready = True
            self._config_ready_event.set()
            logger.info("Dynamic configuration completed successfully.")
        except Exception as e:
            logger.error(f"Error during dynamic configuration: {e}")
            # Set ready flag even on failure to prevent blocking forever
            self._config_ready = True
            self._config_ready_event.set()
            
    async def _configure_attention_and_variant(self):
        """Retrieve configuration from Neural Memory and initialize the attention module and variant processor."""
        try:
            # Retrieve configuration from Neural Memory
            config_resp = await self._make_request(
                self.neural_memory_url,
                "/config",
                method="GET"
            )
            
            if "error" in config_resp:
                logger.warning(f"Failed to retrieve configuration from Neural Memory: {config_resp.get('error')}")
                logger.warning("Using default configuration values.")
                attention_config = {
                    'num_heads': 4,
                    'key_dim': 32,  # Per head dimension (total key_dim is 128)
                    'dropout': 0.0,
                    'use_layer_norm': True,
                    'use_residual': True,
                    'max_dim_mismatch_warnings': 10,
                }
            else:
                logger.info("Retrieved configuration from Neural Memory.")
                # Extract attention configuration from the response
                attention_config = config_resp.get("attention_config", {})
                
                # If we have neural_memory_config, extract relevant dimensions
                if "neural_memory_config" in config_resp:
                    nm_config = config_resp["neural_memory_config"]
                    if not attention_config:
                        # Create attention config from neural memory config
                        attention_config = {
                            'num_heads': 4,
                            'key_dim': nm_config.get('key_dim', 128) // 4,  # Per head dimension (typically 32)
                            'dropout': 0.0,
                            'use_layer_norm': True,
                            'use_residual': True,
                            'max_dim_mismatch_warnings': 10,
                        }
                    # Add dimensions info
                    attention_config["embedding_dimensions"] = {
                        "input_dim": nm_config.get('input_dim', 768),
                        "key_dim": nm_config.get('key_dim', 128),
                        "value_dim": nm_config.get('value_dim', 768),
                        "query_dim": nm_config.get('query_dim', 128)
                    }
                
                # Get variant support information directly from the response
                supports_external_gates = config_resp.get("supports_external_gates", False)
                supports_external_projections = config_resp.get("supports_external_projections", False)
                current_variant = config_resp.get("titans_variant", "NONE")
                
                logger.info(f"Neural Memory active variant: {current_variant}")
                logger.info(f"Neural Memory supports: external gates={supports_external_gates}, external projections={supports_external_projections}")
                
                # No need to check if our variant is supported - the Neural Memory API will handle this
            
            # Initialize the variant processor with the retrieved configuration
            if self.active_variant_type != TitansVariantType.NONE:
                self.variant_processor = create_titans_variant(
                    variant_type=self.active_variant_type,
                    config=attention_config
                )
                
                # Initialize the variant processor with context manager and neural memory URL
                self.variant_processor.set_sequence_context(self.sequence_context_manager)
                self.variant_processor.set_neural_memory_url(self.neural_memory_url)
                logger.info(f"Initialized {self.active_variant_type.value} variant processor")
            else:
                self.variant_processor = None
                logger.info("No Titans Variant active. Using standard Neural Memory flow.")
            
            return attention_config
                
        except Exception as e:
            logger.error(f"Error configuring attention and variant: {e}")
            # Return default configuration
            return {
                'num_heads': 4,
                'key_dim': 32,
                'dropout': 0.0,
                'use_layer_norm': True,
                'use_residual': True,
                'max_dim_mismatch_warnings': 10,
            }

    async def process_new_input(self,
                                content: str,
                                embedding: Optional[List[float]] = None,
                                metadata: Optional[Dict[str, Any]] = None,
                                intent_id: Optional[str] = None) -> Dict[str, Any]:
        """Orchestrates the cognitive cascade for a single input.
        
        This method implements the full cognitive flow with variant-specific processing:
        1. Store input in Memory Core
        2. Get projections from Neural Memory (k_t, v_t, q_t)
        3. Apply variant-specific pre-update processing (MAG/MAL)
        4. Update Neural Memory with appropriate modifications
        5. Update QuickRecal score based on surprise metrics
        6. Retrieve from Neural Memory
        7. Apply variant-specific post-retrieval processing (MAC)
        8. Update sequence history
        9. Return final response
        
        The processing flow differs based on the active Titans variant:
        - NONE: Standard processing without attention mechanisms
        - MAC: Standard update with post-retrieval attention enhancement
        - MAG: Pre-update calculation of gate values via attention
        - MAL: Pre-update modification of value projection via attention
        
        Args:
            content: Text content for the memory
            embedding: Optional embedding for the content (will be generated if not provided)
            metadata: Optional metadata to store with the memory
            intent_id: Optional intent ID for the cognitive operation
            
        Returns:
            Dict containing processing results and memory information
        """

        if not self._config_ready:
            logger.info("Waiting for dynamic configuration...")
            try:
                await asyncio.wait_for(self._config_ready_event.wait(), 10.0)
                logger.info("Configuration ready, proceeding.")
            except asyncio.TimeoutError:
                logger.error("Timed out waiting for configuration. Cannot process input.")
                return self._finalize_error("Configuration timeout", {})

        async with self.processing_lock:
            start_time = time.time()
            # 1. Setup Intent & Metadata
            intent_id, user_emotion = self._setup_intent_and_metadata(intent_id, metadata)
            logger.info(f"Processing input: {content[:50]}... (Intent: {intent_id})")

            # Initialize context dict for this step
            step_context = {
                "content": content,
                "input_embedding": embedding,
                "metadata": metadata,
                "user_emotion": user_emotion,
                "memory_id": None,
                "x_t": None, # Raw embedding from MemCore
                "k_t": None, # Projections
                "v_t": None,
                "q_t": None,
                "v_prime_t": None, # Potentially modified by MAL
                "external_gates": None, # Calculated by MAG
                "loss": None,
                "grad_norm": None,
                "y_t_raw": None, # Raw output from NM retrieve
                "y_t_final": None, # Final output after MAC
                "variant_metrics": {}
            }

            # 2. Store Memory
            store_resp = await self._store_memory(content, embedding, metadata)
            if not store_resp.get("success"):
                return self._finalize_error("Memory storage failed", store_resp, intent_id)
            step_context["memory_id"] = store_resp["memory_id"]
            step_context["x_t"] = store_resp["embedding"] # Store the validated embedding
            quickrecal_initial = store_resp.get("quickrecal_score")

            # 3. Get Projections
            proj_resp = await self._get_projections_from_nm(step_context["x_t"])
            if not proj_resp.get("success"):
                # Log warning but proceed, NM update/retrieve might handle it
                logger.warning(f"Failed to get explicit projections: {proj_resp.get('error')}")
            else:
                step_context["k_t"] = np.array(proj_resp["key_projection"], dtype=np.float32)
                step_context["v_t"] = np.array(proj_resp["value_projection"], dtype=np.float32)
                step_context["q_t"] = np.array(proj_resp["query_projection"], dtype=np.float32)

            # 4. Variant Pre-Update Logic (MAG/MAL)
            if self.variant_processor and self.active_variant_type in [TitansVariantType.MAG, TitansVariantType.MAL]:
                 if step_context["k_t"] is not None and step_context["v_t"] is not None and step_context["q_t"] is not None:
                     variant_pre_result = await self._apply_variant_pre_update(step_context)
                     step_context["external_gates"] = variant_pre_result.get("gates") # For MAG
                     step_context["v_prime_t"] = variant_pre_result.get("v_prime_t") # For MAL
                     step_context["variant_metrics"].update(variant_pre_result.get("metrics", {}))
                 else:
                     logger.warning(f"Skipping {self.active_variant_type.value} pre-update: Missing projections.")

            # 5. Update Neural Memory
            update_resp = await self._update_neural_memory(step_context)
            if not update_resp.get("success"):
                 # Log error but proceed if possible (e.g., maybe retrieval still works)
                 logger.error(f"Neural Memory update failed: {update_resp.get('error')}")
                 # Initialize an error response, but we'll still try to retrieve
                 response_errors = {"update_error": update_resp.get("error")}
            else:
                 step_context["loss"] = update_resp.get("loss")
                 step_context["grad_norm"] = update_resp.get("grad_norm")
                 # Update projections if returned (they should match if not MAL)
                 if update_resp.get("key_projection"): step_context["k_t"] = np.array(update_resp["key_projection"], dtype=np.float32)
                 if update_resp.get("value_projection"): step_context["v_t"] = np.array(update_resp["value_projection"], dtype=np.float32)
                 response_errors = {}

            # 6. Apply QuickRecal Boost (If update succeeded)
            feedback_resp = None
            if "loss" in step_context or "grad_norm" in step_context:
                 feedback_resp = await self._apply_quickrecal_boost(step_context, quickrecal_initial)

            # 7. Retrieve from Neural Memory
            retrieve_resp = await self._retrieve_from_neural_memory(step_context["x_t"])
            if not retrieve_resp.get("success"):
                # Log error and exit - retrieval is critical
                logger.error(f"Neural Memory retrieval failed: {retrieve_resp.get('error')}")
                return self._finalize_error("Neural Memory retrieval failed", 
                                           {"retrieve_error": retrieve_resp.get("error"), **response_errors}, 
                                           intent_id)
            else:
                 step_context["y_t_raw"] = np.array(retrieve_resp["retrieved_embedding"], dtype=np.float32)
                 step_context["y_t_final"] = step_context["y_t_raw"] # Default final to raw
                 # Use query projection returned by /retrieve for consistency
                 if retrieve_resp.get("query_projection"):
                      step_context["q_t"] = np.array(retrieve_resp["query_projection"], dtype=np.float32)


            # 8. Variant Post-Retrieval Logic (MAC)
            if self.variant_processor and self.active_variant_type == TitansVariantType.MAC:
                 if step_context["y_t_raw"] is not None and step_context["q_t"] is not None:
                     variant_post_result = await self._apply_variant_post_retrieval(step_context)
                     if variant_post_result.get("success"):
                         step_context["y_t_final"] = variant_post_result["attended_output"]
                         step_context["variant_metrics"].update(variant_post_result.get("metrics", {}))
                     else:
                         logger.warning(f"MAC post-retrieval processing failed: {variant_post_result.get('error')}")
                 else:
                     logger.warning("Skipping MAC post-retrieval: Missing raw retrieval or query projection.")

            # 9. Update History
            # Use v_t (potentially modified by MAL), raw y_t (before MAC), and final y_t
            await self._update_history(step_context)

            # 10. Finalize Response
            response = self._finalize_response({}, step_context, update_resp, retrieve_resp, feedback_resp)

            processing_time = (time.time() - start_time) * 1000
            logger.info(f"Finished processing input for memory {step_context['memory_id']} in {processing_time:.2f} ms (Variant: {self.active_variant_type.value})")

            # Finalize intent graph
            if self.metrics_enabled:
                 final_text = f"Retrieved: {len(response.get('neural_memory_retrieval',{}).get('retrieved_embedding',[]))} dims" if response.get('status') == 'completed' else f"Error: {response.get('error','Unknown')}"
                 self.metrics_store.finalize_intent(
                     intent_id=intent_id,
                     response_text=final_text,
                     confidence=1.0 if response.get('status') == 'completed' else 0.0
                 )

            return response

    # --- Private Helper Methods for Refactored Flow ---

    def _setup_intent_and_metadata(self, intent_id: Optional[str], metadata: Optional[Dict]) -> Tuple[str, Optional[str]]:
        """Handles intent ID generation and extracts user emotion."""
        metadata = metadata or {}
        user_emotion = None
        if self.metrics_enabled:
            intent_id = intent_id or self.metrics_store.begin_intent()
            self._current_intent_id = intent_id # Store current intent
            if "emotion" in metadata: user_emotion = metadata["emotion"]
            elif "emotions" in metadata:
                # Simplified extraction
                emo_data = metadata["emotions"]
                if isinstance(emo_data, dict) and emo_data: user_emotion = max(emo_data.items(), key=lambda x: x[1])[0]
                elif isinstance(emo_data, list) and emo_data: user_emotion = emo_data[0]
        else:
            intent_id = intent_id or f"intent_{int(time.time())}" # Simple ID if metrics off
        return intent_id, user_emotion

    async def _store_memory(self, content: str, embedding: Optional[List], metadata: Optional[Dict]) -> Dict:
        """Stores input in MemoryCore, returns success status, ID, and validated embedding."""
        logger.debug("Step 1: Storing memory in Memory Core...")
        mem_core_resp = await self._make_request(
            self.memory_core_url, "/process_memory", method="POST",
            payload={"content": content, "embedding": embedding, "metadata": metadata or {}}
        )
        if "error" in mem_core_resp or not mem_core_resp.get("memory_id") or not mem_core_resp.get("embedding"):
            logger.error(f"Memory Core storage failed: {mem_core_resp.get('error', 'Missing ID or embedding')}")
            return {"success": False, "error": mem_core_resp.get('error', 'Store failed'), **mem_core_resp}
        else:
             # Validate embedding received from Memory Core
             is_valid = self._validate_embedding(mem_core_resp["embedding"])
             if not is_valid:
                  logger.error("Memory Core returned an invalid embedding.")
                  return {"success": False, "error": "Invalid embedding from Memory Core", **mem_core_resp}
             logger.info(f"Memory stored successfully: ID {mem_core_resp['memory_id']}")
             return {"success": True, **mem_core_resp}

    async def _get_projections_from_nm(self, actual_embedding: List[float]) -> Dict:
        """Fetches K/V/Q projections from Neural Memory."""
        logger.debug("Step 2: Fetching projections from Neural Memory...")
        if not self._validate_embedding(actual_embedding):
            return {"success": False, "error": "Invalid embedding provided to get_projections"}

        proj_resp = await self._make_request(
            self.neural_memory_url, "/get_projections", method="POST",
            payload={"input_embedding": actual_embedding}
        )
        if "error" in proj_resp or not all(k in proj_resp for k in ["key_projection", "value_projection", "query_projection"]):
             logger.warning(f"Failed to get projections: {proj_resp.get('error', 'Missing projection keys')}")
             return {"success": False, **proj_resp}
        else:
            # Validate received projections
            valid = all(self._validate_embedding(proj_resp[k]) for k in ["key_projection", "value_projection", "query_projection"])
            if not valid:
                 logger.error("Neural Memory returned invalid projections.")
                 return {"success": False, "error": "Invalid projections from Neural Memory", **proj_resp}
            logger.info("Projections fetched successfully.")
            return {"success": True, **proj_resp}

    async def _make_request(self, base_url: str, endpoint: str, method: str = "POST", payload: Optional[Dict] = None, params: Optional[Dict] = None) -> Dict[str, Any]:
        """Shared function to make HTTP requests and handle common errors.
        
        Args:
            base_url: Base URL of the service
            endpoint: API endpoint to call
            method: HTTP method to use
            payload: JSON payload for the request
            params: URL parameters for the request
            
        Returns:
            Response from the server as a dictionary
        """
        url = f"{base_url}{endpoint}"
        log_payload = payload if payload is None or len(json.dumps(payload)) < 200 else {k: (v[:50] + '...' if isinstance(v, str) and len(v) > 50 else v) for k, v in payload.items()}  
        logger.debug(f"Making {method} request to {url}", extra={"payload": log_payload, "params": params})

        # Special debug logging for important endpoints
        debug_endpoints = ["/get_projections", "/update_memory", "/retrieve", "/config"]
        if endpoint in debug_endpoints:
            logger.info(f"DEBUG: Calling {endpoint} with payload: {log_payload if log_payload != payload else payload}")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.request(method, url, json=payload, params=params, timeout=30.0) as response:
                    status_code = response.status
                    try:
                        resp_json = await response.json()
                        
                        # Enhanced logging for specific endpoints
                        if endpoint in debug_endpoints:
                            resp_sample = {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v) 
                                          for k, v in resp_json.items()} if isinstance(resp_json, dict) else resp_json
                            logger.info(f"DEBUG: Response from {endpoint}: Status {status_code}, Content sample: {resp_sample}")
                        else:
                            logger.debug(f"Response from {url}: Status {status_code}")  
                            
                        if 200 <= status_code < 300:
                            # For specific endpoints, ensure key fields are present
                            if endpoint == "/get_projections" and isinstance(resp_json, dict):
                                expected_keys = ["key_projection", "value_projection", "query_projection"]
                                missing_keys = [k for k in expected_keys if k not in resp_json]
                                if missing_keys:
                                    logger.warning(f"WARNING: Response from {endpoint} is missing expected keys: {missing_keys}")
                                    resp_json["warning"] = f"Missing expected keys: {missing_keys}"
                            return resp_json
                        else:
                            error_detail = resp_json.get("detail", "Unknown error from server")
                            logger.error(f"Error from {url}: {status_code} - {error_detail}")
                            return {"error": error_detail, "status_code": status_code}
                    except (json.JSONDecodeError, aiohttp.ContentTypeError):
                        resp_text = await response.text()
                        logger.error(f"Non-JSON or failed response from {url}: {status_code}", extra={"response_text": resp_text[:500]})
                        return {"error": f"Server error {status_code}", "details": resp_text[:500], "status_code": status_code}
        except asyncio.TimeoutError:
            logger.error(f"Timeout connecting to {url}")
            return {"error": "Request timed out", "status_code": 408}
        except aiohttp.ClientConnectionError as e:
            logger.error(f"Connection error to {url}: {e}")
            return {"error": "Connection refused or failed", "status_code": 503}
        except Exception as e:
            logger.error(f"Unexpected error during request to {url}: {e}", exc_info=True)
            return {"error": f"Unexpected client error: {str(e)}", "status_code": 500}

    def _validate_embedding(self, embedding: Union[np.ndarray, List[float], None]) -> bool:
        """Validate that the embedding is in a usable form (valid np.ndarray or list)."""
        if embedding is None:
            return False
        
        # If it's already a list, validate its contents
        if isinstance(embedding, list):
            if not embedding or not all(isinstance(val, (int, float)) for val in embedding):
                return False
            try:
                # Convert to numpy to do further validation
                embedding = np.array(embedding, dtype=np.float32)
            except:
                return False
        
        try:
            # Convert to numpy if not already
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding, dtype=np.float32)
            
            # Check for NaN and Inf
            if np.isnan(embedding).any() or np.isinf(embedding).any():
                logger.error("Embedding contains NaN or Inf values.")
                return False
                
            # Check for zero vector
            if np.all(embedding == 0):
                logger.warning("Embedding is a zero vector.")
                # We still return True as zero vectors are technically valid
                
            return True
        except Exception as e:
            logger.error(f"Error validating embedding: {str(e)}")
            return False
            
    def _to_list(self, arr):
        """Safely convert numpy arrays or tensors to list."""
        if arr is None:
            return None
        if isinstance(arr, list):
            return arr
        if isinstance(arr, np.ndarray):
            return arr.tolist()
        
        # Try to handle tensorflow tensors with lazy loading
        try:
            # Check if this might be a TensorFlow tensor
            if hasattr(arr, 'numpy'):
                return arr.numpy().tolist()
                
            # Last attempt - import TF and try conversion
            from synthians_memory_core.orchestrator.titans_variants import _get_tf
            tf = _get_tf()
            if tf is not None and tf.is_tensor(arr):
                return tf.make_ndarray(tf.make_tensor_proto(arr)).tolist()
        except Exception as e:
            logger.debug(f"Failed to convert possible tensor to list: {e}")
        
        # Last resort, try direct conversion
        try:
            return list(arr)
        except Exception as e:
            logger.warning(f"Could not convert {type(arr)} to list: {e}")
            return None

    async def _retrieve_from_neural_memory(self, actual_embedding: np.ndarray) -> Dict:
        """Retrieves associated embedding from Neural Memory."""
        logger.debug("Step 6: Retrieving from Neural Memory...")
        if not self._validate_embedding(actual_embedding):
             return {"success": False, "error": "Invalid embedding for retrieval"}

        retrieve_payload = {"input_embedding": self._to_list(actual_embedding)}
        retrieve_resp = await self._make_request(
            self.neural_memory_url, "/retrieve", method="POST", payload=retrieve_payload
        )

        if "error" in retrieve_resp or not retrieve_resp.get("retrieved_embedding"):
             logger.error(f"Neural Memory retrieval failed: {retrieve_resp.get('error', 'Missing retrieved_embedding')}")
             return {"success": False, **retrieve_resp}
        else:
             # Validate retrieved embedding
             if not self._validate_embedding(retrieve_resp["retrieved_embedding"]):
                   logger.error("Neural Memory returned invalid retrieved_embedding.")
                   return {"success": False, "error": "Invalid retrieved_embedding", **retrieve_resp}
             # Validate query projection if returned
             if "query_projection" in retrieve_resp and not self._validate_embedding(retrieve_resp["query_projection"]):
                  logger.warning("Neural Memory returned invalid query_projection.")
                  # Don't fail the whole step, but nullify it
                  retrieve_resp["query_projection"] = None

             # Log retrieval metrics if enabled
             if self.metrics_enabled:
                 # Create synthetic memory object since we don't have full metadata
                 retrieved_memory = {
                     "memory_id": f"synthetic_associated",
                     "embedding": retrieve_resp["retrieved_embedding"],
                     "dominant_emotion": None  # We don't have this information
                 }
                 
                 self.metrics_store.log_retrieval(
                     query_embedding=self._to_list(actual_embedding),
                     retrieved_memories=[retrieved_memory],
                     user_emotion=None,
                     intent_id=self._current_intent_id,
                     metadata={
                         "embedding_dim": len(retrieve_resp["retrieved_embedding"]),
                         "timestamp": datetime.utcnow().isoformat(),
                         "variant_type": self.active_variant_type.value
                     }
                 )

             logger.info("Neural Memory retrieval successful.")
             return {"success": True, **retrieve_resp}

    async def _apply_variant_post_retrieval(self, step_context: Dict) -> Dict:
        """Apply variant-specific post-retrieval processing for MAC variant.
        
        This method handles the variant-specific processing that occurs AFTER
        Neural Memory retrieval:
        
        - MAC Variant: Enhances the retrieved embedding (y_t) by applying attention
          between the current query and historical keys, and using this to create
          a weighted combination of historical values with the retrieved embedding.
          This produces a contextually enhanced memory representation.
        
        Args:
            step_context: Current processing context containing embeddings and projections
        
        Returns:
            Dict containing variant processing results
        """
        if not self.variant_processor or self.active_variant_type != TitansVariantType.MAC:
            return {"success": True, "attended_output": step_context.get("y_t_raw")} # Return raw if not MAC

        logger.debug("Step 7: Applying MAC post-retrieval logic...")
        y_t_raw = step_context.get("y_t_raw")
        q_t = step_context.get("q_t")

        if y_t_raw is None or q_t is None:
             logger.warning("Skipping MAC: Missing raw retrieval or query projection.")
             return {"success": False, "error": "Missing y_t_raw or q_t for MAC"}

        try:
            # Call variant processor - assumes process_input can handle None for some args if needed
            variant_results = await self.variant_processor.process_input(
                memory_id=step_context["memory_id"],
                x_t=step_context["x_t"], k_t=step_context["k_t"],
                v_t=step_context["v_t"], q_t=q_t, y_t=y_t_raw # Provide raw y_t
            )
            if variant_results and "attended_output" in variant_results:
                 attended_y_t = variant_results["attended_output"]
                 if self._validate_embedding(attended_y_t):
                     logger.info("MAC variant successfully applied attention.")
                     return {"success": True, "attended_output": attended_y_t, "metrics": variant_results.get("metrics", {})}
                 else:
                      logger.error("MAC variant returned invalid attended_output.")
                      return {"success": False, "error": "Invalid attended_output from MAC", "attended_output": y_t_raw}
            else:
                 logger.warning("MAC variant did not return 'attended_output'.")
                 return {"success": False, "error": "MAC variant failed", "attended_output": y_t_raw}

        except Exception as e:
            logger.error(f"Error applying MAC variant: {e}", exc_info=True)
            return {"success": False, "error": str(e), "attended_output": y_t_raw}

    async def _update_history(self, step_context: Dict):
        """Adds the completed step context to the history manager."""
        logger.debug("Step 8: Updating sequence history...")
        
        # Early return if memory_id is missing (indicates something went wrong earlier)
        if "memory_id" not in step_context:
            logger.warning("History update skipped: Missing memory_id.")
            return
        
        # Ensure all components are valid numpy arrays before adding
        required_keys = ["x_t", "k_t", "v_t", "q_t", "y_t_final"]
        valid_context = True
        context_tuple_args = {}

        # Extract and validate required components
        for key in required_keys:
            value = step_context.get(key)
            if value is None:
                logger.warning(f"History update skipped: Missing '{key}'")
                valid_context = False
                break
            
            # Convert to numpy array if it's a list
            if isinstance(value, list):
                try:
                    value = np.array(value, dtype=np.float32)
                    step_context[key] = value  # Update in context
                except Exception as e:
                    logger.warning(f"History update skipped: Could not convert '{key}' to numpy array: {e}")
                    valid_context = False
                    break
            
            # Validate numpy array
            if not isinstance(value, np.ndarray):
                logger.warning(f"History update skipped: '{key}' is not a numpy array but {type(value)}")
                valid_context = False
                break
                
            # Further validation (NaN/Inf) - _validate_embedding does this
            if not self._validate_embedding(value):
                logger.warning(f"History update skipped: Invalid data in '{key}'")
                valid_context = False
                break
                
            context_tuple_args[key] = value

        if valid_context:
            try:
                # Log detailed shapes for debugging
                shapes_info = {
                    k: f"{v.shape} ({v.dtype})" for k, v in context_tuple_args.items()
                }
                logger.debug(f"Adding context with shapes: {shapes_info}")
                
                self.sequence_context_manager.add_context(
                    timestamp=time.time(), # Use current time for history entry
                    memory_id=step_context["memory_id"],
                    x_t=context_tuple_args["x_t"],
                    k_t=context_tuple_args["k_t"],
                    v_t=context_tuple_args["v_t"], # Use the v_t that was ACTUALLY used in update
                    q_t=context_tuple_args["q_t"],
                    y_t=context_tuple_args["y_t_final"] # Use the final output y_t
                )
                logger.info(f"Added context to SequenceContextManager. Length: {len(self.sequence_context_manager)}")
            except Exception as e:
                logger.error(f"Failed to add context to history manager: {e}", exc_info=True)
        else:
            logger.error("Failed to update history due to invalid/missing context components.")

    def _finalize_response(self, base_response: Dict, step_context: Dict,
                           update_resp: Dict, retrieve_resp: Dict, feedback_resp: Optional[Dict]) -> Dict:
        """Constructs the final response dictionary."""
        logger.debug("Step 9: Finalizing response.")
        final_response = {
            "memory_id": step_context["memory_id"],
            "intent_id": self._current_intent_id,
            "status": "completed", # Assume completion if we got this far
            "timestamp": datetime.utcnow().isoformat(),
            "neural_memory_update": update_resp,
            "neural_memory_retrieval": { # Structure this more cleanly
                 "success": retrieve_resp.get("success", False),
                 "retrieved_embedding": self._to_list(step_context.get("y_t_final")) if step_context.get("y_t_final") is not None else None,
                 "query_projection": self._to_list(step_context.get("q_t")) if step_context.get("q_t") is not None else None,
                 "error": retrieve_resp.get("error")
            },
            "surprise_metrics": {
                "loss": step_context.get("loss"),
                "grad_norm": step_context.get("grad_norm"),
                "boost_calculated": step_context.get("quickrecal_boost")
            },
            "quickrecal_feedback": feedback_resp or {"status": "N/A"},
            "variant_output": step_context.get("variant_metrics", {}) # Include variant metrics if any
        }
         # Add variant type to variant_output
        final_response["variant_output"]["variant_type"] = self.active_variant_type.value

        # Merge any errors captured earlier
        if "error" in base_response: final_response["error"] = base_response["error"]
        if not update_resp.get("success"): final_response["update_error"] = update_resp.get("error")
        if not retrieve_resp.get("success"): final_response["retrieval_error"] = retrieve_resp.get("error")

        # Update overall status if errors occurred
        if "error" in final_response or "update_error" in final_response or "retrieval_error" in final_response:
             final_response["status"] = "error_partial" if step_context["memory_id"] else "error_total"


        # Update CCE state
        if step_context.get("y_t_final") is not None:
             self.last_retrieved_embedding = self._to_list(step_context["y_t_final"])
        # Add to legacy sequence context (maybe remove later)
        self.sequence_context.append({
             "memory_id": step_context["memory_id"],
             "actual_embedding": self._to_list(step_context["x_t"]) if step_context["x_t"] is not None else None,
             "retrieved_embedding": self.last_retrieved_embedding,
             "surprise_metrics": final_response["surprise_metrics"],
             "timestamp": final_response["timestamp"],
             "intent_id": self._current_intent_id
         })
        if len(self.sequence_context) > 20: self.sequence_context.pop(0)


        return final_response

    def _finalize_error(self, message: str, context: dict, intent_id: Optional[str] = None) -> dict:
        """Constructs a standardized error response and finalizes intent."""
        intent_id = intent_id or self._current_intent_id
        logger.error(f"Finalizing with error: {message}", extra=context)
        response = {
            "status": "error",
            "error": message,
            "details": context.get("error", context.get("details", "No details")),
            "timestamp": datetime.utcnow().isoformat(),
            "intent_id": intent_id
        }
        if self.metrics_enabled:
            self.metrics_store.finalize_intent(
                intent_id=intent_id,
                response_text=f"Error: {message}",
                confidence=0.0
            )
        return response

    def _calculate_quickrecal_boost(self, surprise_value: Optional[float]) -> float:
        """Calculate quickrecal boost based on surprise value (loss or grad_norm)."""
        if surprise_value is None or surprise_value <= 0.0: return 0.0
        # Simple linear scaling for now, capped at 0.2
        # Example: loss/grad_norm of 1.0 gives 0.1 boost, 2.0 gives 0.2 boost
        max_expected_surprise = 2.0
        max_boost = 0.2
        boost = min(surprise_value / max_expected_surprise, 1.0) * max_boost
        logger.debug(f"Calculated quickrecal boost: {boost:.6f} from surprise value: {surprise_value:.6f}")
        return boost

    async def _apply_variant_pre_update(self, step_context: Dict) -> Dict:
        """Apply variant-specific pre-update processing for MAG/MAL variants.
        
        This method handles the variant-specific processing that must occur BEFORE
        the Neural Memory update:
        
        - MAG Variant: Calculates attention-based gate values (alpha_t, theta_t, eta_t)
          that control the Neural Memory update process:
          * alpha_t: Controls forgetting rate (higher = forget more)
          * theta_t: Controls learning rate (higher = learn faster)
          * eta_t: Controls momentum decay (higher = retain more momentum)
        
        - MAL Variant: Calculates a modified value projection (v_prime) by applying
          attention between the current query and historical keys/values. This enhances
          the value representation before it's stored in Neural Memory.
        
        Args:
            step_context: Current processing context containing embeddings and projections
        
        Returns:
            Dict containing variant processing results
        """
        if not self.variant_processor or self.active_variant_type not in [TitansVariantType.MAG, TitansVariantType.MAL]:
            return {"success": True} # No pre-processing needed

        logger.debug(f"Step 3: Applying {self.active_variant_type.value} pre-update logic...")
        variant_results = {}
        try:
            # MAG: Calculate Gates
            if self.active_variant_type == TitansVariantType.MAG:
                # Retrieve K_hist
                k_hist = self.sequence_context_manager.get_recent_keys()
                if not k_hist:
                    logger.info("MAG: Not enough context for gate calculation.")
                    return {"success": True, "gates": None, "metrics": {}}

                # Ensure q_t and k_hist are tensors for attention
                try:
                    from synthians_memory_core.orchestrator.titans_variants import _get_tf
                    tf = _get_tf() # Lazy load TF
                    q_tensor = tf.convert_to_tensor([step_context["q_t"]], dtype=tf.float32)
                    k_hist_tensor = tf.convert_to_tensor(k_hist, dtype=tf.float32)
                    if len(k_hist_tensor.shape) == 2: k_hist_tensor = tf.expand_dims(k_hist_tensor, 0)

                    # Calculate attention output (Query attends to historical Keys)
                    attention_output_tensor = self.attention_module(
                        query=q_tensor, key=k_hist_tensor, value=k_hist_tensor, training=False
                    )
                    attention_output_list = tf.squeeze(attention_output_tensor).numpy().tolist()
                except Exception as e:
                    logger.error(f"Error during MAG attention calculation: {e}")
                    return {"success": False, "error": str(e), "gates": None, "metrics": {}}

                # Call NM API to calculate gates
                gates_resp = await self._make_request(
                    self.neural_memory_url, "/calculate_gates", method="POST",
                    payload={"attention_output": attention_output_list}
                )
                if "error" not in gates_resp:
                     variant_results = {
                         "success": True,
                         "gates": {"alpha_t": gates_resp["alpha"], "theta_t": gates_resp["theta"], "eta_t": gates_resp["eta"]},
                         "metrics": getattr(self.attention_module, 'get_metrics', lambda: {})() # Safe access
                     }
                     logger.info(f"MAG calculated gates: {variant_results['gates']}")
                else:
                     logger.error(f"MAG failed to calculate gates via API: {gates_resp.get('error')}")
                     variant_results = {"success": False, "error": gates_resp.get('error'), "gates": None, "metrics": {}}

            # MAL: Calculate v_prime_t
            elif self.active_variant_type == TitansVariantType.MAL:
                k_hist, v_hist = self.sequence_context_manager.get_recent_kv_pairs()
                if not k_hist or not v_hist:
                     logger.info("MAL: Not enough context for value augmentation.")
                     return {"success": True, "v_prime_t": step_context["v_t"], "metrics": {}} # Return original v_t

                # Call variant processor's method (assuming it exists and handles TF conversion)
                # This requires `titans_variants.MALVariant` to have the calculation logic
                mal_output = await self.variant_processor.calculate_v_prime(
                    q_t=step_context["q_t"],
                    v_t=step_context["v_t"],
                    k_hist=k_hist,
                    v_hist=v_hist
                )
                if mal_output and mal_output.get("success"):
                     v_prime_t = mal_output["v_prime_t"]
                     if self._validate_embedding(v_prime_t):
                         variant_results = {"success": True, "v_prime_t": v_prime_t, "metrics": mal_output.get("metrics", {})}
                         logger.info("MAL calculated v_prime_t.")
                     else:
                          logger.error("MAL variant returned invalid v_prime_t.")
                          variant_results = {"success": False, "error": "Invalid v_prime_t from MAL", "v_prime_t": step_context["v_t"]}
                else:
                     logger.error(f"MAL variant processing failed: {mal_output.get('error')}")
                     variant_results = {"success": False, "error": mal_output.get('error'), "v_prime_t": step_context["v_t"]}


        except Exception as e:
            logger.error(f"Error during variant pre-update ({self.active_variant_type.value}): {e}", exc_info=True)
            return {"success": False, "error": str(e)}

        return {"success": True, **variant_results} # Default success if no relevant variant

    async def _update_neural_memory(self, step_context: Dict) -> Dict:
        """Update Neural Memory with appropriate modifications based on active variant.
        
        This method handles the Neural Memory update process with variant-specific modifications:
        
        - NONE Variant: Standard update with the input embedding only
        - MAC Variant: Standard update (variant processing occurs after retrieval)
        - MAG Variant: Update with externally calculated gate values (alpha_t, theta_t, eta_t)
        - MAL Variant: Update with modified value projection (v_prime)
        
        Args:
            step_context: Current processing context containing embeddings and projections
        
        Returns:
            Dict containing update response with loss and gradient norm
        """
        logger.debug("Step 4: Updating Neural Memory...")
        update_payload = {"input_embedding": self._to_list(step_context["x_t"])} # Base payload

        # Add MAG gates if calculated
        if step_context["external_gates"]:
             gates = step_context["external_gates"]
             # Use the specific keys expected by the updated UpdateMemoryRequest
             update_payload["external_alpha_gate"] = gates.get("alpha_t")
             update_payload["external_theta_gate"] = gates.get("theta_t")
             update_payload["external_eta_gate"] = gates.get("eta_t")
             logger.info("Using MAG external gates for update.")

        # Add MAL projections if calculated (v_prime_t overrides default v_t)
        elif step_context["v_prime_t"] is not None:
             if step_context["k_t"] is None:
                 logger.error("MAL Error: v_prime_t calculated but k_t is missing.")
                 return {"success": False, "error": "k_t missing for MAL update"}
             update_payload = { # Override payload for MAL
                 "input_embedding": self._to_list(step_context["x_t"]),
                 "key_projection": self._to_list(step_context["k_t"]),
                 "value_projection": self._to_list(step_context["v_prime_t"])
             }
             logger.info("Using MAL explicit projections (k_t, v_prime_t) for update.")

        update_resp = await self._make_request(
            self.neural_memory_url, "/update_memory", method="POST", payload=update_payload
        )

        if "error" in update_resp:
            return {"success": False, **update_resp}
        else:
            logger.info(f"Neural Memory updated: Loss={update_resp.get('loss'):.6f}, GradNorm={update_resp.get('grad_norm'):.6f}")
            # Log memory update metrics if enabled
            if self.metrics_enabled:
                self.metrics_store.log_memory_update(
                    input_embedding=self._to_list(step_context["x_t"]),
                    loss=update_resp.get("loss"),
                    grad_norm=update_resp.get("grad_norm", 0.0),
                    emotion=step_context["user_emotion"],
                    intent_id=self._current_intent_id,
                    metadata={
                        "memory_id": step_context["memory_id"],
                        "content_preview": step_context["content"][:50] if step_context["content"] else "",
                        "variant_type": self.active_variant_type.value
                    }
                )
            
        # Check for errors
        if "error" in update_resp:
             logger.error(f"Neural Memory update failed: {update_resp['error']}")
             return {"success": False, **update_resp}
             
        # Extract metrics for subsequent processing
        if "loss" in update_resp:
            step_context["loss"] = update_resp["loss"]
        if "grad_norm" in update_resp:
            step_context["grad_norm"] = update_resp["grad_norm"]
             
        logger.info("Neural Memory update successful")
        return {"success": True, **update_resp}

    async def _apply_quickrecal_boost(self, step_context: Dict, quickrecal_initial: Optional[float]) -> Optional[Dict]:
         """Calculates and applies QuickRecal boost if needed."""
         logger.debug("Step 5: Applying QuickRecal boost...")
         loss = step_context.get("loss")
         grad_norm = step_context.get("grad_norm")
         memory_id = step_context["memory_id"]
         user_emotion = step_context["user_emotion"]

         if memory_id and (loss is not None or grad_norm is not None):
             surprise_metric = grad_norm if grad_norm is not None else loss
             boost = self._calculate_quickrecal_boost(surprise_metric)
             step_context["quickrecal_boost"] = boost # Store calculated boost

             if boost > 1e-4:
                 loss_str = f"{loss:.6f}" if isinstance(loss, (float, int)) else 'N/A'
                 grad_norm_str = f"{grad_norm:.6f}" if isinstance(grad_norm, (float, int)) else 'N/A'
                 feedback_payload = {
                     "memory_id": memory_id, "delta": boost,
                     "reason": f"NM Surprise (Loss:{loss_str}, GradNorm:{grad_norm_str})"
                 }
                 feedback_resp = await self._make_request(
                     self.memory_core_url, "/api/memories/update_quickrecal_score",
                     method="POST", payload=feedback_payload
                 )
                 if "error" in feedback_resp:
                      logger.error(f"QuickRecal boost failed: {feedback_resp.get('error')}")
                      return {"status": "error", "error": feedback_resp.get('error')}
                 else:
                      logger.info(f"QuickRecal boost applied: Delta={boost:.6f}")
                      if self.metrics_enabled:
                          self.metrics_store.log_quickrecal_boost(
                              memory_id=memory_id, base_score=quickrecal_initial or 0.0,
                              boost_amount=boost, emotion=user_emotion, intent_id=self._current_intent_id,
                              metadata={"loss": loss, "grad_norm": grad_norm, "reason": "NM Surprise"}
                          )
                 return feedback_resp
             else:
                  logger.debug("QuickRecal boost skipped (too small).")
                  return {"status": "skipped", "reason": "Boost too small"}
         else:
              logger.warning("Skipping QuickRecal boost: Missing memory_id or surprise metrics.")
              return {"status": "skipped", "reason": "Missing ID or surprise metrics"}

    async def get_sequence_embeddings_for_training(self, limit: int = 100, **filters) -> Dict[str, Any]:
        """Retrieve a sequence from Memory Core for training purposes.
        
        Args:
            limit: Maximum number of embeddings to retrieve
            **filters: Additional filters like topic, user, etc.
            
        Returns:
            Sequence of embeddings with metadata
        """
        payload = {"limit": limit}
        payload.update(filters)  

        return await self._make_request(
            self.memory_core_url,
            "/api/memories/get_sequence_embeddings",
            method="POST",  
            payload=payload
        )

```

# orchestrator\history.py

```py
import time
import logging
from collections import deque
from typing import Deque, Tuple, List, Optional, Any

import numpy as np

logger = logging.getLogger(__name__)

# Define the structure of the context tuple for clarity
ContextTuple = Tuple[float, str, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
# (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)

class SequenceContextManager:
    """
    Manages a deque-based context buffer for storing attention-related embeddings and projections.

    Maintains a fixed-length history of (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t) tuples
    for attention calculations in Titans architecture variants.
    """

    def __init__(self, max_length: int = 50):
        """
        Initialize the sequence context manager.

        Args:
            max_length: Maximum number of context tuples to store.
        """
        if not isinstance(max_length, int) or max_length <= 0:
            raise ValueError("max_length must be a positive integer.")
        self.max_length = max_length
        self._context_buffer: Deque[ContextTuple] = deque(maxlen=max_length)
        logger.info(f"SequenceContextManager initialized with max_length={max_length}")

    def add_context(
        self,
        memory_id: str,
        x_t: np.ndarray,
        k_t: np.ndarray,
        v_t: np.ndarray,
        q_t: np.ndarray,
        y_t: np.ndarray, # Output from NeuralMemory.call
        timestamp: Optional[float] = None
    ) -> None:
        """
        Add a new context element (tuple) to the buffer.

        Args:
            memory_id: Identifier for the memory entry.
            x_t: Input embedding (np.ndarray).
            k_t: Key projection (np.ndarray).
            v_t: Value projection (np.ndarray).
            q_t: Query projection (np.ndarray).
            y_t: Neural memory output embedding (np.ndarray).
            timestamp: Optional timestamp (defaults to current time).
        """
        ts = timestamp if timestamp is not None else time.time()

        # Basic validation of inputs
        if not all(isinstance(arr, np.ndarray) for arr in [x_t, k_t, v_t, q_t, y_t]):
            logger.error("Invalid input type for context tuple. All embeddings/projections must be numpy arrays.")
            # Decide how to handle: raise error or skip adding? Let's skip for robustness.
            return

        context_tuple: ContextTuple = (ts, memory_id, x_t, k_t, v_t, q_t, y_t)
        self._context_buffer.append(context_tuple)
        logger.debug(f"Added context for memory {memory_id} to buffer (size: {len(self._context_buffer)})")

    def update_last_context(self, y_t: np.ndarray) -> bool:
        """Update the most recent context entry with the y_t value.
        
        This is useful when y_t is not available at the time of initial context creation,
        such as when we need to add context before Neural Memory retrieval but only get
        the y_t value after retrieval.
        
        Args:
            y_t: The retrieved embedding (output from Neural Memory)
            
        Returns:
            True if update was successful, False otherwise
        """
        if not len(self._context_buffer):
            logger.warning("Cannot update last context: buffer is empty")
            return False
            
        if not isinstance(y_t, np.ndarray):
            logger.error("Invalid y_t type for context update. Must be numpy array.")
            return False
            
        # Get the last context tuple
        last_tuple = self._context_buffer[-1]
        
        # Create a new tuple with the updated y_t
        updated_tuple = (
            last_tuple[0],  # timestamp
            last_tuple[1],  # memory_id
            last_tuple[2],  # x_t
            last_tuple[3],  # k_t
            last_tuple[4],  # v_t
            last_tuple[5],  # q_t
            y_t             # updated y_t
        )
        
        # Replace the last tuple
        self._context_buffer[-1] = updated_tuple
        logger.debug(f"Updated last context entry for memory {last_tuple[1]} with y_t")
        return True

    def get_recent_history(self, count: Optional[int] = None) -> List[ContextTuple]:
        """Get the most recent context tuples."""
        num_items = count if count is not None else len(self._context_buffer)
        num_items = min(num_items, len(self._context_buffer)) # Don't request more than available
        if num_items <= 0:
            return []
        # Return a list slice of the deque
        return list(self._context_buffer)[-num_items:]

    def get_recent_keys(self, count: Optional[int] = None) -> List[np.ndarray]:
        """Get the most recent key projections (k_t)."""
        history = self.get_recent_history(count)
        return [item[3] for item in history] # Index 3 is k_t

    def get_recent_values(self, count: Optional[int] = None) -> List[np.ndarray]:
        """Get the most recent value projections (v_t)."""
        history = self.get_recent_history(count)
        return [item[4] for item in history] # Index 4 is v_t

    def get_recent_queries(self, count: Optional[int] = None) -> List[np.ndarray]:
        """Get the most recent query projections (q_t)."""
        history = self.get_recent_history(count)
        return [item[5] for item in history] # Index 5 is q_t

    def get_recent_outputs(self, count: Optional[int] = None) -> List[np.ndarray]:
        """Get the most recent neural memory outputs (y_t)."""
        history = self.get_recent_history(count)
        return [item[6] for item in history] # Index 6 is y_t

    def get_recent_kv_pairs(self, count: Optional[int] = None) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """Convenience method to get recent (Key, Value) pairs for attention."""
        history = self.get_recent_history(count)
        keys = [item[3] for item in history]
        values = [item[4] for item in history]
        return keys, values

    def get_recent_ky_pairs(self, count: Optional[int] = None) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """Convenience method to get recent (Key, Output) pairs for MAC attention."""
        history = self.get_recent_history(count)
        keys = [item[3] for item in history]
        outputs = [item[6] for item in history]
        return keys, outputs

    def __len__(self) -> int:
        """Return the current number of items in the buffer."""
        return len(self._context_buffer)

    def clear(self) -> None:
        """Clear the context buffer."""
        self._context_buffer.clear()
        logger.info("SequenceContextManager buffer cleared.")

```

# orchestrator\server.py

```py
import os
import logging
import asyncio
from typing import Dict, List, Any, Optional

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

from synthians_memory_core.geometry_manager import GeometryManager
from synthians_memory_core.orchestrator.context_cascade_engine import ContextCascadeEngine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(title="Context Cascade Orchestrator")

# Global instance of the orchestrator
orchestrator = None

# --- Pydantic Models ---

class ProcessMemoryRequest(BaseModel):
    content: str
    embedding: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None

class SequenceEmbeddingsRequest(BaseModel):
    topic: Optional[str] = None
    limit: int = 10
    min_quickrecal_score: Optional[float] = None

class AnalyzeSurpriseRequest(BaseModel):
    predicted_embedding: List[float]
    actual_embedding: List[float]

# --- Helper Functions ---

def get_orchestrator():
    """Get or initialize the context cascade orchestrator."""
    global orchestrator
    if orchestrator is None:
        # Get URLs from environment variables with updated defaults
        memory_core_url = os.environ.get("MEMORY_CORE_URL", "http://localhost:5010")  # Default to localhost:5010
        neural_memory_url = os.environ.get("NEURAL_MEMORY_URL", "http://localhost:8001")
        
        # Initialize shared geometry manager
        geometry_manager = GeometryManager()
        
        # Initialize orchestrator
        orchestrator = ContextCascadeEngine(
            memory_core_url=memory_core_url,
            neural_memory_url=neural_memory_url,
            geometry_manager=geometry_manager,
            metrics_enabled=True
        )
        logger.info(f"Orchestrator initialized with Memory Core URL: {memory_core_url}, Neural Memory URL: {neural_memory_url}")
    
    return orchestrator

# --- Endpoints ---

@app.get("/")
async def root():
    """Root endpoint returning service information."""
    return {"service": "Context Cascade Orchestrator", "status": "running"}

@app.post("/process_memory")
async def process_memory(request: ProcessMemoryRequest):
    """Process a new memory through the full cognitive pipeline.
    
    This orchestrates:
    1. Store memory in Memory Core
    2. Compare with previous prediction if available
    3. Update quickrecal scores based on surprise
    4. Generate prediction for next memory
    """
    orchestrator = get_orchestrator()
    
    try:
        result = await orchestrator.process_new_input(
            content=request.content,
            embedding=request.embedding,
            metadata=request.metadata
        )
        return result
    except Exception as e:
        logger.error(f"Error processing memory: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing memory: {str(e)}")

@app.post("/get_sequence_embeddings")
async def get_sequence_embeddings(request: SequenceEmbeddingsRequest):
    """Retrieve a sequence of embeddings from Memory Core."""
    orchestrator = get_orchestrator()
    
    try:
        result = await orchestrator.get_sequence_embeddings(
            topic=request.topic,
            limit=request.limit,
            min_quickrecal_score=request.min_quickrecal_score
        )
        return result
    except Exception as e:
        logger.error(f"Error retrieving sequence embeddings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving sequence embeddings: {str(e)}")

@app.post("/analyze_surprise")
async def analyze_surprise(request: AnalyzeSurpriseRequest):
    """Analyze surprise between predicted and actual embeddings."""
    orchestrator = get_orchestrator()
    
    try:
        # Use the surprise detector from the orchestrator
        surprise_metrics = orchestrator.surprise_detector.calculate_surprise(
            predicted_embedding=request.predicted_embedding,
            actual_embedding=request.actual_embedding
        )
        
        # Calculate quickrecal boost
        quickrecal_boost = orchestrator.surprise_detector.calculate_quickrecal_boost(surprise_metrics)
        
        # Add boost to response
        surprise_metrics["quickrecal_boost"] = quickrecal_boost
        
        return surprise_metrics
    except Exception as e:
        logger.error(f"Error analyzing surprise: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analyzing surprise: {str(e)}")

# --- Startup and Shutdown Events ---

@app.on_event("startup")
async def startup_event():
    """Initialize the orchestrator on startup."""
    get_orchestrator()
    logger.info("Context Cascade Orchestrator is ready")

@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on shutdown."""
    logger.info("Shutting down Context Cascade Orchestrator")

```

# orchestrator\tests\test_context_cascade_engine.py

```py
# synthians_memory_core/orchestrator/tests/test_context_cascade_engine.py

import pytest
import numpy as np
import json
import asyncio
from unittest.mock import patch, MagicMock, AsyncMock
from typing import Dict, List, Any

from ..context_cascade_engine import ContextCascadeEngine
from synthians_memory_core.geometry_manager import GeometryManager


@pytest.fixture
def geometry_manager():
    """Test fixture for GeometryManager."""
    return GeometryManager({
        'embedding_dim': 768,
        'geometry_type': 'euclidean',
    })


@pytest.fixture
def engine(geometry_manager):
    """Test fixture for ContextCascadeEngine with mock URLs."""
    return ContextCascadeEngine(
        memory_core_url="http://memory-core-test",
        trainer_url="http://trainer-test",
        geometry_manager=geometry_manager
    )


@pytest.fixture
def mock_response():
    """Create a mock for aiohttp ClientResponse."""
    mock = MagicMock()
    mock.status = 200
    mock.json = AsyncMock()
    return mock


@pytest.mark.asyncio
async def test_process_new_memory(engine, mock_response):
    """Test the complete flow of processing a new memory."""
    # Mock embeddings and memory data
    test_content = "This is a test memory"
    test_embedding = np.random.randn(768).tolist()
    test_memory_id = "test-memory-123"
    
    # Mock memory core response
    memory_response = {
        "id": test_memory_id,
        "embedding": test_embedding,
        "quickrecal_score": 0.8
    }
    mock_response.json.return_value = memory_response
    
    # Mock trainer response
    trainer_response = {
        "predicted_embedding": np.random.randn(768).tolist(),
        "surprise_score": 0.3,
        "memory_state": {
            "sequence": [test_embedding],
            "surprise_history": [0.3],
            "momentum": np.random.randn(768).tolist()
        }
    }
    mock_trainer_response = MagicMock()
    mock_trainer_response.status = 200
    mock_trainer_response.json = AsyncMock(return_value=trainer_response)
    
    # Setup mock for aiohttp ClientSession
    with patch('aiohttp.ClientSession.post') as mock_post, \
         patch('aiohttp.ClientSession.get') as mock_get:
            
        # Configure mock to return different responses for different URLs
        mock_post.side_effect = lambda url, **kwargs: \
            mock_response if "memory-core-test" in url else mock_trainer_response
        
        # Call the method under test
        result = await engine.process_new_memory(
            content=test_content,
            embedding=test_embedding
        )
        
        # Verify memory core was called
        assert mock_post.call_count >= 1
        # Verify memory_id is present in result
        assert result["memory_id"] == test_memory_id
        # Verify prediction data is present
        assert "prediction" in result
        # Verify last_predicted_embedding was updated
        assert engine.last_predicted_embedding is not None


@pytest.mark.asyncio
async def test_retrieve_memories(engine, mock_response):
    """Test retrieving memories through the engine."""
    # Mock query and response
    query = "test query"
    memories = [
        {"id": "mem1", "content": "Memory 1", "similarity": 0.9},
        {"id": "mem2", "content": "Memory 2", "similarity": 0.8}
    ]
    
    mock_response.json.return_value = {"memories": memories}
    
    # Setup mock for aiohttp ClientSession
    with patch('aiohttp.ClientSession.post') as mock_post:
        mock_post.return_value.__aenter__.return_value = mock_response
        
        # Call the method under test
        result = await engine.retrieve_memories(query=query, limit=2)
        
        # Verify memory core was called
        mock_post.assert_called_once()
        # Verify results
        assert len(result["memories"]) == 2
        assert result["memories"][0]["id"] == "mem1"


@pytest.mark.asyncio
async def test_error_handling(engine):
    """Test error handling for HTTP responses."""
    # Mock error response
    error_response = MagicMock()
    error_response.status = 500
    error_response.text = AsyncMock(return_value="Internal server error")
    
    # Setup mock for aiohttp ClientSession
    with patch('aiohttp.ClientSession.post') as mock_post:
        mock_post.return_value.__aenter__.return_value = error_response
        
        # Call the method under test and expect error handling
        result = await engine.process_new_memory(content="Error test")
        
        # Verify error is captured
        assert "error" in result
        assert result["status"] == "error"


@pytest.mark.asyncio
async def test_surprise_detection(engine, mock_response):
    """Test surprise detection when actual embedding differs from predicted."""
    # Setup initial state with a predicted embedding
    engine.last_predicted_embedding = np.random.randn(768).tolist()
    
    # Create actual embedding with high difference
    actual_embedding = np.random.randn(768).tolist()  # Will be different due to randomness
    
    # Mock memory core response
    memory_response = {
        "id": "test-memory-456",
        "embedding": actual_embedding,
        "quickrecal_score": 0.7
    }
    mock_response.json.return_value = memory_response
    
    # Mock trainer response with high surprise
    trainer_response = {
        "predicted_embedding": np.random.randn(768).tolist(),
        "surprise_score": 0.8,  # High surprise
        "memory_state": {
            "sequence": [actual_embedding],
            "surprise_history": [0.8],
            "momentum": np.random.randn(768).tolist()
        }
    }
    mock_trainer_response = MagicMock()
    mock_trainer_response.status = 200
    mock_trainer_response.json = AsyncMock(return_value=trainer_response)
    
    # Setup mock for aiohttp ClientSession
    with patch('aiohttp.ClientSession.post') as mock_post:
        # Configure mock to return different responses for different URLs
        mock_post.side_effect = lambda url, **kwargs: \
            mock_response if "memory-core-test" in url else mock_trainer_response
        
        # Call the method under test
        result = await engine.process_new_memory(
            content="Surprise test",
            embedding=actual_embedding
        )
        
        # Verify surprise was detected
        assert "surprise" in result
        assert result["surprise"]["score"] > 0.7  # High surprise threshold

```

# orchestrator\titans_variants.py

```py
#!/usr/bin/env python

from enum import Enum
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple, Union, TYPE_CHECKING
import threading

# Use TYPE_CHECKING for type hints that won't be evaluated at runtime
if TYPE_CHECKING:
    import tensorflow as tf

# Lazy-load TensorFlow to avoid NumPy incompatibility issues during startup
_tf = None
_tf_lock = threading.Lock()

def _get_tf():
    """Lazy-load TensorFlow only when needed to avoid early NumPy conflicts"""
    global _tf
    if _tf is None:
        with _tf_lock:
            # Double-check after acquiring lock (thread-safe singleton pattern)
            if _tf is None:
                import tensorflow as tf
                _tf = tf
    return _tf

logger = logging.getLogger(__name__)

class TitansVariantType(str, Enum):
    """Enumeration of Titans architecture variants."""
    NONE = "NONE"  # No attention mechanism, base Neural Memory
    MAC = "MAC"    # Memory-Attended Computation
    MAG = "MAG"    # Memory-Attended Gates
    MAL = "MAL"    # Memory-Augmented Learning


class TitansVariantConfig(dict):
    """Configuration for Titans architecture variants."""
    def __init__(self, *args, **kwargs):
        defaults = {
            "variant": TitansVariantType.NONE.value,
            "attention_num_heads": 4,
            "attention_key_dim": 32,  # per head
            "attention_dropout": 0.0,
            "attention_use_layer_norm": True,
            "attention_use_residual": True,
            "max_context_length": 50,
            "max_dim_mismatch_warnings": 10,
        }
        
        config = defaults.copy()
        # Apply kwargs first
        config.update(kwargs)
        # Then apply dict from args if provided
        if args and isinstance(args[0], dict):
            config.update(args[0])
            
        super().__init__(config)
        
    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(f"'TitansVariantConfig' object has no attribute '{key}'")

    def __setattr__(self, key, value):
        self[key] = value


class TitansVariantBase:
    """Base class for all Titans architecture variants."""
    
    def __init__(self, config: Optional[Union[TitansVariantConfig, Dict]] = None, **kwargs):
        """Initialize the base Titans variant.
        
        Args:
            config: Optional configuration dictionary for attention parameters.
        """
        if isinstance(config, dict) or config is None: 
            self.config = TitansVariantConfig(**(config or {}))
        elif isinstance(config, TitansVariantConfig): 
            self.config = config
        else: 
            raise TypeError("config must be a dict or TitansVariantConfig")
            
        self.variant_type = TitansVariantType.NONE
        self.sequence_context = None
        self.neural_memory_url = None
        self.api_client = None
    
    def set_sequence_context(self, sequence_context) -> None:
        """Set the sequence context manager for historical attention context.
        
        Args:
            sequence_context: SequenceContextManager instance to use for context history.
        """
        self.sequence_context = sequence_context
    
    def set_neural_memory_url(self, neural_memory_url: str) -> None:
        """Set the Neural Memory server URL and initialize API client.
        
        Args:
            neural_memory_url: URL to the Neural Memory server
        """
        self.neural_memory_url = neural_memory_url
        # Initialize the API client for making requests to Neural Memory server
        from ..api_client import NeuralMemoryClient
        self.api_client = NeuralMemoryClient(base_url=neural_memory_url)
        logger.info(f"Initialized API client for {self.name} variant with Neural Memory URL: {neural_memory_url}")
    
    def process_input(self, memory_id: str, x_t: np.ndarray, k_t: np.ndarray, 
                      v_t: np.ndarray, q_t: np.ndarray, y_t: np.ndarray) -> Dict[str, Any]:
        """Process input through the variant's logic.
        
        Args:
            memory_id: ID of the current memory being processed
            x_t: Original input embedding
            k_t: Key projection
            v_t: Value projection
            q_t: Query projection
            y_t: Retrieved embedding from Neural Memory
            
        Returns:
            Dict containing variant-specific outputs and metrics
        """
        # Base implementation just returns empty dict
        return {}


class MACVariant(TitansVariantBase):
    """Memory-Attended Computation (MAC) variant.
    
    Enhances memory retrieval by attending over historical memory outputs.
    Flow: q_t -> M -> y_t -> Attend(q_t, K_hist, Y_hist) -> attended_y_t
    """
    
    def __init__(
        self, 
        config: Optional[Union[TitansVariantConfig, Dict]] = None,
        **kwargs
    ):
        super().__init__(config, **kwargs)
        self.name = "MAC"
        self.variant_type = TitansVariantType.MAC
        
        # Initialize attention module for this variant
        attention_config = {
            "num_heads": self.config.get("attention_num_heads", 4),
            "key_dim": self.config.get("attention_key_dim", 32),
            "dropout": self.config.get("attention_dropout", 0.0),
            # The following parameters are not supported in this TF version
            # "use_layer_norm": self.config.get("attention_use_layer_norm", True),
            # "use_residual": self.config.get("attention_use_residual", True),
            "max_dim_mismatch_warnings": self.config.get("max_dim_mismatch_warnings", 10),
        }
        self.attention_module = _get_tf().keras.layers.MultiHeadAttention(
            num_heads=attention_config["num_heads"],
            key_dim=attention_config["key_dim"],
            dropout=attention_config["dropout"],
            # Removed unsupported parameters
            # use_layer_norm=attention_config["use_layer_norm"],
            # use_residual=attention_config["use_residual"],
            name="MAC_Attention"
        )
        
        logger.info(f"Initialized MAC variant with {attention_config['num_heads']} attention heads")

    def process_input(
        self,
        memory_id: str,
        x_t: np.ndarray, 
        k_t: np.ndarray,
        v_t: np.ndarray,
        q_t: np.ndarray,
        y_t: np.ndarray,
    ) -> Dict[str, Any]:
        """Implement MAC variant logic.
        
        1. Store context tuple (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)
        2. Retrieve recent history pairs (k_i, y_i) from sequence_context
        3. Calculate attended output using attention module: attended_y_t = AttentionModule(q_t, K_hist, Y_hist)
        4. Return attended_y_t for use by downstream components
        """
        # First store the context
        self.store_context(memory_id, x_t, k_t, v_t, q_t, y_t)
        
        # Get historical contexts for attention
        if len(self.sequence_context) < 2:
            # Not enough context for attention, return original output
            logger.info("MAC: Not enough context for attention, using original output")
            return {
                "memory_id": memory_id,
                "attended_output": y_t,  # No change
                "metrics": self.attention_module.get_metrics() if self.attention_module else {},
            }
        
        # Get historical keys and memory outputs
        k_hist, y_hist = self.sequence_context.get_recent_ky_pairs(count=len(self.sequence_context) - 1)  # Exclude current
        
        # If using TensorFlow backend for attention:
        # Convert numpy arrays to tensors for TF operations
        q_tensor = _get_tf().convert_to_tensor(q_t, dtype='float32')
        if len(q_tensor.shape) == 1:
            q_tensor = _get_tf().expand_dims(q_tensor, 0)  # Add batch dimension
            
        k_hist_tensor = _get_tf().convert_to_tensor(k_hist, dtype='float32')
        if len(k_hist_tensor.shape) == 2:  # [seq_len, key_dim]
            k_hist_tensor = _get_tf().expand_dims(k_hist_tensor, 0)  # Add batch dimension [1, seq_len, key_dim]
            
        y_hist_tensor = _get_tf().convert_to_tensor(y_hist, dtype='float32')
        if len(y_hist_tensor.shape) == 2:  # [seq_len, value_dim]
            y_hist_tensor = _get_tf().expand_dims(y_hist_tensor, 0)  # Add batch dimension [1, seq_len, value_dim]
        
        # Apply attention mechanism
        try:
            attended_y_tensor = self.attention_module(
                query=q_tensor,
                key=k_hist_tensor,
                value=y_hist_tensor,
                training=False,
            )
            
            # Convert back to numpy for consistency
            attended_y = attended_y_tensor.numpy()
            if len(attended_y.shape) > 1:
                attended_y = attended_y[0]  # Remove batch dimension
                
            logger.info(f"MAC: Applied attention to {len(k_hist)} historical memories")
            
            return {
                "memory_id": memory_id,
                "attended_output": attended_y,
                "original_output": y_t,  # Keep original for comparison
                "metrics": self.attention_module.get_metrics(),
            }
            
        except Exception as e:
            logger.error(f"MAC attention failed: {e}")
            # Fallback to original output
            return {
                "memory_id": memory_id,
                "attended_output": y_t,  # Fallback to original
                "error": str(e),
                "metrics": {},
            }


class MAGVariant(TitansVariantBase):
    """Memory-Attended Gates (MAG) variant.
    
    Modifies gate values (alpha, theta, eta) for the neural memory update
    by attending over historical key projections.
    
    Flow: 
    1. q_t -> Attend(q_t, K_hist, K_hist) -> attention_output
    2. Call Neural Memory's /calculate_gates endpoint with attention output
    3. Update memory with calculated gates
    """
    
    def __init__(
        self, 
        config: Optional[Union[TitansVariantConfig, Dict]] = None,
        **kwargs
    ):
        super().__init__(config, **kwargs)
        self.name = "MAG"
        self.variant_type = TitansVariantType.MAG
        
        # Initialize attention module for this variant
        attention_config = {
            "num_heads": self.config.get("attention_num_heads", 4),
            "key_dim": self.config.get("attention_key_dim", 32),
            "dropout": self.config.get("attention_dropout", 0.0),
            "max_dim_mismatch_warnings": self.config.get("max_dim_mismatch_warnings", 10),
        }
        self.attention_module = _get_tf().keras.layers.MultiHeadAttention(
            num_heads=attention_config["num_heads"],
            key_dim=attention_config["key_dim"],
            dropout=attention_config["dropout"],
            name="MAG_Attention"
        )
        
        logger.info(f"MAG: Initialized attention module with config {attention_config}")

    def process_input(
        self,
        memory_id: str,
        x_t: np.ndarray, 
        k_t: np.ndarray,
        v_t: np.ndarray,
        q_t: np.ndarray,
        y_t: np.ndarray,
    ) -> Dict[str, Any]:
        """Implement MAG variant logic.
        
        1. Store context tuple (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)
        2. Retrieve recent history keys from sequence_context
        3. Calculate attention output using K_hist
        4. Call Neural Memory's /calculate_gates endpoint with attention output
        5. Return calculated gates for use in neural memory update
        """
        # First store the context
        self.store_context(memory_id, x_t, k_t, v_t, q_t, y_t)
        
        # Get historical contexts for attention
        if len(self.sequence_context) < 2:
            # Not enough context for attention, return default gates
            logger.info("MAG: Not enough context for attention, using default gates")
            return {
                "memory_id": memory_id,
                "attended_output": y_t,  # No change to y_t
                "alpha": None,  # Let neural memory use default
                "theta": None,  # Let neural memory use default
                "eta": None,    # Let neural memory use default
                "metrics": {},
            }
        
        # Get historical keys
        k_hist = self.sequence_context.get_recent_keys(count=len(self.sequence_context) - 1)  # Exclude current
        
        # Convert numpy arrays to tensors for TF operations
        q_tensor = _get_tf().convert_to_tensor(q_t, dtype='float32')
        if len(q_tensor.shape) == 1:
            q_tensor = _get_tf().expand_dims(q_tensor, 0)  # Add batch dimension
            
        k_hist_tensor = _get_tf().convert_to_tensor(k_hist, dtype='float32')
        if len(k_hist_tensor.shape) == 2:  # [seq_len, key_dim]
            k_hist_tensor = _get_tf().expand_dims(k_hist_tensor, 0)  # Add batch dimension [1, seq_len, key_dim]
        
        # Apply attention mechanism
        try:
            attention_output = self.attention_module(
                query=q_tensor,
                key=k_hist_tensor,
                value=k_hist_tensor,  # Self-attention on historical keys
                training=False,
            )
            
            # Call the Neural Memory server's /calculate_gates endpoint
            try:
                # Prepare the request payload
                attention_output_np = attention_output.numpy().squeeze()  # Remove batch dimension
                
                # Make the API request
                response = self.api_client.calculate_gates(
                    attention_output=attention_output_np.tolist()
                )
                
                # Extract gate values from response
                alpha = response.get("alpha")
                theta = response.get("theta")
                eta = response.get("eta")
                
                logger.info(f"MAG: Calculated gates from Neural Memory server: alpha={alpha:.4f}, theta={theta:.4f}, eta={eta:.4f}")
                
                return {
                    "memory_id": memory_id,
                    "attended_output": y_t,  # No change to y_t
                    "alpha": alpha,
                    "theta": theta,
                    "eta": eta,
                    "metrics": self.attention_module.get_metrics(),
                }
            except Exception as api_err:
                logger.error(f"MAG: Failed to call /calculate_gates API: {api_err}")
                # Fall back to default gates
                return {
                    "memory_id": memory_id,
                    "attended_output": y_t,
                    "alpha": None,
                    "theta": None,
                    "eta": None,
                    "error": str(api_err),
                    "metrics": self.attention_module.get_metrics(),
                }
            
        except Exception as e:
            logger.error(f"MAG attention failed: {e}")
            # Fallback to default gates
            return {
                "memory_id": memory_id,
                "attended_output": y_t,
                "alpha": None,
                "theta": None,
                "eta": None,
                "error": str(e),
                "metrics": {},
            }


class MALVariant(TitansVariantBase):
    """Memory-Augmented Learning (MAL) variant.
    
    Modifies value projection for neural memory update by attending over
    historical value projections.
    
    Flow: 
    1. q_t, K_hist, V_hist -> Attend(q_t, K_hist, V_hist) -> attended_v_t
    2. Combine attended_v_t with v_t -> v_prime_t
    3. Update memory with k_t and v_prime_t
    """
    
    def __init__(
        self, 
        config: Optional[Union[TitansVariantConfig, Dict]] = None,
        **kwargs
    ):
        super().__init__(config, **kwargs)
        self.name = "MAL"
        self.variant_type = TitansVariantType.MAL
        
        # Initialize attention module for this variant
        attention_config = {
            "num_heads": self.config.get("attention_num_heads", 4),
            "key_dim": self.config.get("attention_key_dim", 32),
            "dropout": self.config.get("attention_dropout", 0.0),
            "max_dim_mismatch_warnings": self.config.get("max_dim_mismatch_warnings", 10),
        }
        self.attention_module = _get_tf().keras.layers.MultiHeadAttention(
            num_heads=attention_config["num_heads"],
            key_dim=attention_config["key_dim"],
            dropout=attention_config["dropout"],
            name="MAL_Attention"
        )
        
        # Gating layers for combining attended and current values (initialized when dimensions are known)
        self.v_prime_gate = None
        self.v_prime_projector = None
        
        logger.info(f"Initialized MAL variant with {attention_config['num_heads']} attention heads")
    
    def init_value_projection_layers(self, value_dim: int):
        """Initialize value projection and gating layers.
        
        Args:
            value_dim: Dimension of the value vectors
        """
        self.v_prime_gate = _get_tf().keras.layers.Dense(1, activation='sigmoid', name="v_prime_gate")
        self.v_prime_projector = _get_tf().keras.layers.Dense(value_dim, activation='tanh', name="v_prime_projector")
        
        # Build the layers with dummy inputs to ensure variables are created
        dummy_input = _get_tf().zeros([1, value_dim * 2], dtype='float32')  # Concatenated dimension
        self.v_prime_gate(dummy_input)
        
        dummy_input2 = _get_tf().zeros([1, value_dim], dtype='float32')
        self.v_prime_projector(dummy_input2)
        
        logger.info(f"MAL: Initialized value projection layers with value_dim={value_dim}")

    async def calculate_v_prime(self, q_t: np.ndarray, v_t: np.ndarray) -> Dict[str, Any]:
        """Calculate modified value projection using attention over historical values.
        
        This method is specifically called by the ContextCascadeEngine._apply_variant_pre_update
        method to get a modified value projection for use in the Neural Memory update.
        
        Args:
            q_t: Query projection for the current input
            v_t: Original value projection for the current input
            
        Returns:
            Dict containing v_prime (modified value projection) and metrics
        """
        # Get historical contexts for attention
        if not self.sequence_context or len(self.sequence_context) < 2:
            # Not enough context for attention, return original values
            logger.info("MAL: Not enough context for calculate_v_prime, using original value projection")
            return {
                "v_prime": v_t,  # No change
                "metrics": {}
            }
        
        # Get historical keys and values
        k_hist, v_hist = self.sequence_context.get_recent_kv_pairs(count=len(self.sequence_context) - 1)  # Exclude current
        
        # Convert numpy arrays to tensors for TF operations
        q_tensor = _get_tf().convert_to_tensor(q_t, dtype='float32')
        if len(q_tensor.shape) == 1:
            q_tensor = _get_tf().expand_dims(q_tensor, 0)  # Add batch dimension
            
        k_hist_tensor = _get_tf().convert_to_tensor(k_hist, dtype='float32')
        if len(k_hist_tensor.shape) == 2:  # [seq_len, key_dim]
            k_hist_tensor = _get_tf().expand_dims(k_hist_tensor, 0)  # Add batch dimension [1, seq_len, key_dim]
            
        v_hist_tensor = _get_tf().convert_to_tensor(v_hist, dtype='float32')
        if len(v_hist_tensor.shape) == 2:  # [seq_len, value_dim]
            v_hist_tensor = _get_tf().expand_dims(v_hist_tensor, 0)  # Add batch dimension [1, seq_len, value_dim]
        
        v_tensor = _get_tf().convert_to_tensor(v_t, dtype='float32')
        if len(v_tensor.shape) == 1:
            v_tensor = _get_tf().expand_dims(v_tensor, 0)  # Add batch dimension
        
        # Apply attention mechanism
        try:
            attended_v_tensor = self.attention_module(
                query=q_tensor,
                key=k_hist_tensor,
                value=v_hist_tensor,
                training=False,
            )
            
            # Combine attended and current values
            if self.v_prime_gate is None:
                # Initialize projection layers if not already done
                self.init_value_projection_layers(v_tensor.shape[-1])
            
            # Concatenate vectors for gating
            concat_v = _get_tf().concat([v_tensor, attended_v_tensor], axis=-1)
            
            # Calculate gate value
            gate = self.v_prime_gate(concat_v)
            
            # Combine original and attended values
            v_prime_tensor = gate * v_tensor + (1 - gate) * attended_v_tensor
            
            # Final projection
            v_prime_tensor = self.v_prime_projector(v_prime_tensor)
            
            # Convert back to numpy
            v_prime = v_prime_tensor.numpy()
            if len(v_prime.shape) > 1:
                v_prime = v_prime[0]  # Remove batch dimension
            
            logger.info(f"MAL: Generated augmented value projection from {len(k_hist)} historical values")
            
            return {
                "v_prime": v_prime,  # Augmented value projection
                "original_v": v_t,  # Original for comparison
                "metrics": self.attention_module.get_metrics(),
            }
            
        except Exception as e:
            logger.error(f"MAL calculate_v_prime failed: {e}")
            # Fallback to original value projection
            return {
                "v_prime": v_t,  # Fallback to original
                "error": str(e),
                "metrics": {},
            }

    def process_input(
        self,
        memory_id: str,
        x_t: np.ndarray, 
        k_t: np.ndarray,
        v_t: np.ndarray,
        q_t: np.ndarray,
        y_t: np.ndarray,
    ) -> Dict[str, Any]:
        """Implement MAL variant logic.
        
        1. Store context tuple (timestamp, memory_id, x_t, k_t, v_t, q_t, y_t)
        2. Retrieve recent history pairs (k_i, v_i) from sequence_context
        3. Calculate attended value using attention module: attended_v_t = AttentionModule(q_t, K_hist, V_hist)
        4. Combine attended_v_t with current v_t using gating mechanism
        5. Return v_prime_t for neural memory update
        """
        # First store the context
        self.store_context(memory_id, x_t, k_t, v_t, q_t, y_t)
        
        # Get historical contexts for attention
        if len(self.sequence_context) < 2:
            # Not enough context for attention, return original values
            logger.info("MAL: Not enough context for attention, using original value projection")
            return {
                "memory_id": memory_id,
                "attended_output": y_t,  # No change
                "v_prime": v_t,  # No change
                "metrics": {},
            }
        
        # Get historical keys and values
        k_hist, v_hist = self.sequence_context.get_recent_kv_pairs(count=len(self.sequence_context) - 1)  # Exclude current
        
        # Convert numpy arrays to tensors for TF operations
        q_tensor = _get_tf().convert_to_tensor(q_t, dtype='float32')
        if len(q_tensor.shape) == 1:
            q_tensor = _get_tf().expand_dims(q_tensor, 0)  # Add batch dimension
            
        k_hist_tensor = _get_tf().convert_to_tensor(k_hist, dtype='float32')
        if len(k_hist_tensor.shape) == 2:  # [seq_len, key_dim]
            k_hist_tensor = _get_tf().expand_dims(k_hist_tensor, 0)  # Add batch dimension [1, seq_len, key_dim]
            
        v_hist_tensor = _get_tf().convert_to_tensor(v_hist, dtype='float32')
        if len(v_hist_tensor.shape) == 2:  # [seq_len, value_dim]
            v_hist_tensor = _get_tf().expand_dims(v_hist_tensor, 0)  # Add batch dimension [1, seq_len, value_dim]
        
        v_tensor = _get_tf().convert_to_tensor(v_t, dtype='float32')
        if len(v_tensor.shape) == 1:
            v_tensor = _get_tf().expand_dims(v_tensor, 0)  # Add batch dimension
        
        # Apply attention mechanism
        try:
            attended_v_tensor = self.attention_module(
                query=q_tensor,
                key=k_hist_tensor,
                value=v_hist_tensor,
                training=False,
            )
            
            # Combine attended and current values
            if self.v_prime_gate is None:
                # Initialize projection layers if not already done
                self.init_value_projection_layers(v_tensor.shape[-1])
            
            # Concatenate vectors for gating
            concat_v = _get_tf().concat([v_tensor, attended_v_tensor], axis=-1)
            
            # Calculate gate value
            gate = self.v_prime_gate(concat_v)
            
            # Combine original and attended values
            v_prime_tensor = gate * v_tensor + (1 - gate) * attended_v_tensor
            
            # Final projection
            v_prime_tensor = self.v_prime_projector(v_prime_tensor)
            
            # Convert back to numpy
            v_prime = v_prime_tensor.numpy()
            if len(v_prime.shape) > 1:
                v_prime = v_prime[0]  # Remove batch dimension
            
            logger.info(f"MAL: Generated augmented value projection from {len(k_hist)} historical values")
            
            return {
                "memory_id": memory_id,
                "attended_output": y_t,  # No change in output
                "v_prime": v_prime,  # Augmented value projection
                "original_v": v_t,  # Original for comparison
                "metrics": self.attention_module.get_metrics(),
            }
            
        except Exception as e:
            logger.error(f"MAL attention failed: {e}")
            # Fallback to original value projection
            return {
                "memory_id": memory_id,
                "attended_output": y_t,
                "v_prime": v_t,  # Fallback to original
                "error": str(e),
                "metrics": {},
            }


def create_titans_variant(variant_type: TitansVariantType, attention_config: Optional[Dict[str, Any]] = None) -> TitansVariantBase:
    """Factory function to create a Titans variant instance based on type.
    
    Args:
        variant_type: Type of variant to create (MAC, MAG, MAL, or NONE)
        attention_config: Configuration dictionary for attention parameters
        
    Returns:
        An instance of the requested variant type
    """
    if variant_type == TitansVariantType.NONE:
        return TitansVariantBase(attention_config)
    elif variant_type == TitansVariantType.MAC:
        return MACVariant(attention_config)
    elif variant_type == TitansVariantType.MAG:
        return MAGVariant(attention_config)
    elif variant_type == TitansVariantType.MAL:
        return MALVariant(attention_config)
    else:
        raise ValueError(f"Unknown variant type: {variant_type}")

```

# README.md

```md
# synthians_memory_core/README.md

# Synthians Memory Core

This directory contains the unified and optimized memory system for the Synthians AI architecture, integrating the best features from the Lucid Recall system.

## Overview

The Synthians Memory Core provides a lean, efficient, yet powerful memory system incorporating:

-   **Advanced Relevance Scoring:** Uses the `UnifiedQuickRecallCalculator` (HPC-QR) for multi-factor memory importance assessment.
-   **Flexible Geometry:** Supports Euclidean, Hyperbolic, Spherical, and Mixed geometries for embedding representation via the `GeometryManager`.
-   **Emotional Intelligence:** Integrates emotional analysis and gating (`EmotionalAnalyzer`, `EmotionalGatingService`) for nuanced retrieval.
-   **Memory Assemblies:** Groups related memories (`MemoryAssembly`) for complex concept representation.
-   **Robust Persistence:** Handles disk storage, backups, and atomic writes asynchronously (`MemoryPersistence`).
-   **Adaptive Thresholds:** Dynamically adjusts retrieval thresholds based on feedback (`ThresholdCalibrator`).
-   **Unified Interface:** Provides a cohesive API through `SynthiansMemoryCore`.
-   **Vector Search:** Fast retrieval using FAISS vector indexing with GPU acceleration support.

## Recent Improvements (March 2025)

The Synthians Memory Core has received significant enhancements in the `Synthience_memory_remaster` branch:

-   **Fixed Vector Index Persistence:** The FAISS vector index and ID mappings are now properly saved during the persistence cycle, ensuring memories can be retrieved after system restarts.
-   **Enhanced API Observability:** Added comprehensive vector index information to the `/stats` endpoint for better monitoring and debugging.
-   **Improved Embedding Handling:** Robust dimension handling to ensure vector operations work correctly regardless of embedding dimensions (384 vs 768).
-   **Retrieval Threshold Adjustments:** Lowered pre-filter threshold from 0.5 to 0.2 for improved recall while maintaining precision.
-   **Validation Tools:** Added comprehensive test scripts to validate the full memory lifecycle.

See `docs/memory_system_remaster.md` for detailed documentation on these improvements.

## Components

-   `synthians_memory_core.py`: The main orchestrator class.
-   `hpc_quickrecal.py`: Contains the `UnifiedQuickRecallCalculator`.
-   `geometry_manager.py`: Centralizes embedding and geometry operations.
-   `emotional_intelligence.py`: Provides emotion analysis and gating.
-   `memory_structures.py`: Defines `MemoryEntry` and `MemoryAssembly`.
-   `memory_persistence.py`: Manages disk storage and backups.
-   `adaptive_components.py`: Includes `ThresholdCalibrator`.
-   `vector_index.py`: Handles FAISS vector indexing with GPU support.
-   `custom_logger.py`: Simple logging utility.

## Usage

\`\`\`python
import asyncio
from synthians_memory_core import SynthiansMemoryCore
import numpy as np

async def main():
    # Configuration (adjust paths and dimensions as needed)
    config = {
        'embedding_dim': 768,
        'geometry': 'hyperbolic',
        'storage_path': './synthians_memory_data'
    }

    # Initialize
    memory_core = SynthiansMemoryCore(config)
    await memory_core.initialize()

    # --- Example Operations ---

    # Generate a sample embedding (replace with your actual embedding generation)
    sample_embedding = np.random.rand(config['embedding_dim']).astype(np.float32)

    # 1. Store a new memory
    memory_entry = await memory_core.process_new_memory(
        content="Learned about hyperbolic embeddings today.",
        embedding=sample_embedding,
        metadata={"source": "learning_session"}
    )
    if memory_entry:
        print(f"Stored memory: {memory_entry.id}")

    # 2. Retrieve memories
    query_embedding = np.random.rand(config['embedding_dim']).astype(np.float32) # Use actual query embedding
    retrieved = await memory_core.retrieve_memories(
        query="hyperbolic geometry",
        query_embedding=query_embedding,
        top_k=3
    )
    print(f"\nRetrieved {len(retrieved)} memories:")
    for mem_dict in retrieved:
        print(f"- ID: {mem_dict.get('id')}, Score: {mem_dict.get('final_score', mem_dict.get('relevance_score')):.3f}, Content: {mem_dict.get('content', '')[:50]}...")


    # 3. Provide feedback (if adaptive thresholding enabled)
    if memory_entry and memory_core.threshold_calibrator:
         await memory_core.provide_feedback(
              memory_id=memory_entry.id,
              similarity_score=0.85, # Example score from retrieval
              was_relevant=True
         )
         print(f"\nProvided feedback. New threshold: {memory_core.threshold_calibrator.get_current_threshold():.3f}")

    # 4. Detect Contradictions
    # (Add potentially contradictory memories first)
    await memory_core.process_new_memory(content="A causes B", embedding=np.random.rand(config['embedding_dim']))
    await memory_core.process_new_memory(content="A prevents B", embedding=np.random.rand(config['embedding_dim']))
    contradictions = await memory_core.detect_contradictions()
    print(f"\nDetected {len(contradictions)} potential contradictions.")

    # Shutdown
    await memory_core.shutdown()

if __name__ == "__main__":
    asyncio.run(main())
\`\`\`

## Key Improvements

-   **Unified Structure:** Consolidates core logic into fewer files.
-   **Centralized Geometry:** `GeometryManager` handles all geometric operations consistently.
-   **Direct Integration:** HPC-QR, Emotion, Assemblies are integral parts, not separate layers added via mixins.
-   **Improved Efficiency:** Leverages `asyncio` and dedicated persistence class.
-   **Clearer Interfaces:** Simplified API focused on core memory operations.
-   **Hyperbolic First-Class:** Hyperbolic geometry is treated as a core configuration option.

This new structure provides a more streamlined, maintainable, and potentially more efficient implementation while capturing the core value propositions (HPC-QR, Hyperbolic, Emotion, Assemblies) of the original system.
```

# run_server.py

```py
# synthians_memory_core/run_server.py

import os
import sys
import logging
import uvicorn
from pathlib import Path

# Ensure we can import from the synthians_memory_core package
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(
    level=logging.getLevelName(os.environ.get("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

def main():
    """Run the Synthians Memory Core API server"""
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "5010"))
    
    print(f"Starting Synthians Memory Core API server at {host}:{port}")
    
    # Use Uvicorn to run the FastAPI application
    uvicorn.run(
        "synthians_memory_core.api.server:app",
        host=host,
        port=port,
        reload=False,  # Disable reload in production
        workers=1      # Single worker for memory consistency
    )

if __name__ == "__main__":
    main()

```

# synthians_memory_core.py

```py
# synthians_memory_core/synthians_memory_core.py

import time
import asyncio
import numpy as np
from typing import Dict, Any, Optional, List, Tuple, Set, Union
from pathlib import Path
import random
import uuid
import json
import os
import datetime as dt
from datetime import timezone, datetime # Ensure datetime is imported directly
import copy
import traceback # Import traceback for detailed error logging
import math

# Import core components from this package
from .custom_logger import logger
from .memory_structures import MemoryEntry, MemoryAssembly
from .hpc_quickrecal import UnifiedQuickRecallCalculator, QuickRecallMode, QuickRecallFactor
from .geometry_manager import GeometryManager, GeometryType
from .emotional_intelligence import EmotionalGatingService
from .memory_persistence import MemoryPersistence
from .adaptive_components import ThresholdCalibrator
from .metadata_synthesizer import MetadataSynthesizer
from .emotion_analyzer import EmotionAnalyzer
from .vector_index import MemoryVectorIndex

# --- Add Deep Update Utility Function ---
# (Can be placed inside the class or outside)
def deep_update(source, overrides):
    """
    Update a nested dictionary or similar mapping.
    Modifies source in place.
    """
    for key, value in overrides.items():
        if isinstance(value, dict) and value:
            # Ensure source[key] exists and is a dict before recursing
            current_value = source.get(key)
            if isinstance(current_value, dict):
                returned = deep_update(current_value, value)
                source[key] = returned
            else:
                # If source[key] is not a dict or doesn't exist, just overwrite
                source[key] = value
    return source


class SynthiansMemoryCore:
    """
    Unified Synthians Memory Core.

    Integrates HPC-QuickRecal, Hyperbolic Geometry, Emotional Intelligence,
    Memory Assemblies, Adaptive Thresholds, and Robust Persistence
    into a lean and efficient memory system.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = {
            'embedding_dim': 768,
            'geometry': 'hyperbolic', # 'euclidean', 'hyperbolic', 'spherical', 'mixed'
            'hyperbolic_curvature': -1.0,
            'storage_path': '/app/memory/stored/synthians', # Unified path
            'persistence_interval': 60.0, # Persist every minute
            'decay_interval': 3600.0, # Check decay every hour
            'prune_check_interval': 600.0, # Check if pruning needed every 10 mins
            'max_memory_entries': 50000,
            'prune_threshold_percent': 0.9, # Prune when 90% full
            'min_quickrecal_for_ltm': 0.2, # Min score to keep after decay
            'assembly_threshold': 0.75,
            'max_assemblies_per_memory': 3,
            'adaptive_threshold_enabled': True,
            'initial_retrieval_threshold': 0.75,
            'vector_index_type': 'Cosine',  # 'L2', 'IP', 'Cosine'
            'persistence_batch_size': 100, # Batch size for persistence loop
            'check_index_on_retrieval': False, # New config option
            'index_check_interval': 3600, # New config option
            'migrate_to_idmap': True, # New config option
            **(config or {})
        }

        logger.info("SynthiansMemoryCore", "Initializing...", self.config)

        # --- Core Components ---
        self.geometry_manager = GeometryManager({
            'embedding_dim': self.config['embedding_dim'],
            'geometry_type': self.config['geometry'],
            'curvature': self.config['hyperbolic_curvature']
        })

        self.quick_recal = UnifiedQuickRecallCalculator({
            'embedding_dim': self.config['embedding_dim'],
            'mode': QuickRecallMode.HPC_QR, # Default to HPC-QR mode
            'geometry_type': self.config['geometry'],
            'curvature': self.config['hyperbolic_curvature']
        }, geometry_manager=self.geometry_manager) # Pass geometry manager

        # Provide the analyzer instance directly to the gating service
        self.emotional_analyzer = EmotionAnalyzer()  # Use our new robust emotion analyzer
        self.emotional_gating = EmotionalGatingService(
            emotion_analyzer=self.emotional_analyzer, # Pass the instance
            config={'emotional_weight': 0.3} # Example config
        )

        self.persistence = MemoryPersistence({'storage_path': self.config['storage_path']})

        self.threshold_calibrator = ThresholdCalibrator(
            initial_threshold=self.config['initial_retrieval_threshold']
        ) if self.config['adaptive_threshold_enabled'] else None

        self.metadata_synthesizer = MetadataSynthesizer()  # Initialize metadata synthesizer

        # Initialize vector index for fast retrieval
        # If we're using IndexIDMap (which is the default and recommended), we need to use CPU
        # as FAISS GPU indexes don't support add_with_ids
        use_index_id_map = self.config.get('migrate_to_idmap', True)
        self.vector_index = MemoryVectorIndex({
            'embedding_dim': self.config['embedding_dim'],
            'storage_path': self.config['storage_path'],
            'index_type': self.config['vector_index_type'],
            'use_gpu': not use_index_id_map  # Use GPU only if not using IndexIDMap
        })
        
        # Check if we should migrate the index to the new IndexIDMap format
        if use_index_id_map:
            is_index_id_map = hasattr(self.vector_index.index, 'id_map')
            if not is_index_id_map:
                logger.info("Migrating vector index to use IndexIDMap for improved ID management")
                success = self.vector_index.migrate_to_idmap()
                if success:
                    logger.info("Successfully migrated vector index to IndexIDMap")
                else:
                    logger.warning("Failed to migrate vector index to IndexIDMap. Some features may not work correctly.")
        
        # --- Memory State ---
        self._memories: Dict[str, MemoryEntry] = {} # In-memory cache/working set
        self.assemblies: Dict[str, MemoryAssembly] = {}
        self.memory_to_assemblies: Dict[str, Set[str]] = {}
        self._dirty_memories: Set[str] = set() # Track modified memory IDs for persistence

        # --- Concurrency & Tasks ---
        self._lock = asyncio.Lock()
        self._background_tasks: List[asyncio.Task] = []
        self._initialized = False
        self._shutdown_signal = asyncio.Event()

        logger.info("SynthiansMemoryCore", "Core components initialized.")

    async def initialize(self):
        """Load persisted state and start background tasks."""
        if self._initialized: return True
        logger.info("SynthiansMemoryCore", "Starting initialization...")
        async with self._lock:
            # Initialize persistence first to ensure memory index is loaded
            await self.persistence.initialize()
            logger.info("SynthiansMemoryCore", "Persistence layer initialized.")

            # REMOVED: Don't load all memories immediately, just get the IDs from the index
            # loaded_memories = await self.persistence.load_all()
            # for mem in loaded_memories:
            #     self._memories[mem.id] = mem
            memory_ids = self.persistence.memory_index.keys()
            logger.info("SynthiansMemoryCore", f"Found {len(memory_ids)} memory IDs in index. Deferring full memory load.")
            
            # Initialize memory_to_assemblies mapping with empty sets for all known memory IDs
            self.memory_to_assemblies = {memory_id: set() for memory_id in memory_ids}

            # Load assemblies and memory_to_assemblies mapping
            assembly_list = await self.persistence.list_assemblies()
            loaded_assemblies_count = 0

            # Load each assembly
            for assembly_info in assembly_list:
                assembly_id = assembly_info.get("id")
                if assembly_id:
                    assembly = await self.persistence.load_assembly(assembly_id, self.geometry_manager)
                    if assembly:
                        self.assemblies[assembly_id] = assembly
                        loaded_assemblies_count += 1

                        # Update memory_to_assemblies mapping
                        for memory_id in assembly.memories:
                            if memory_id in self.memory_to_assemblies:
                                self.memory_to_assemblies[memory_id].add(assembly_id)
                            else:
                                # Create mapping entry if memory not in cache
                                self.memory_to_assemblies[memory_id] = {assembly_id}

            logger.info("SynthiansMemoryCore", f"Loaded {loaded_assemblies_count} assemblies from persistence.")

            # Load the vector index
            index_loaded = self.vector_index.load()

            # If index wasn't found, we'll need to build it as memories get loaded
            if not index_loaded:
                logger.info("SynthiansMemoryCore", "No vector index found. Will build incrementally as memories are loaded.")
            else:
                logger.info("SynthiansMemoryCore", f"Loaded vector index with {self.vector_index.count()} entries")

            # Verify index integrity
            is_consistent, diagnostics = self.vector_index.verify_index_integrity()
            
            # Log the diagnostics
            logger.info(f"Vector index integrity check: {is_consistent}")
            logger.info(f"Vector index diagnostics: {diagnostics}")
            
            # Automatically repair common issues
            need_repair = False
            
            # Case 1: Index inconsistency detected (FAISS count > 0, Mapping count = 0)
            if diagnostics.get('faiss_count', 0) > 0 and diagnostics.get('id_mapping_count', 0) == 0:
                logger.warning("Critical inconsistency detected: FAISS count > 0 but Mapping count = 0")
                logger.warning("Initiating automatic repair...")
                need_repair = True
            
            # Case 2: Index is not using IndexIDMap but config requests it
            if self.config.get('migrate_to_idmap', True) and not diagnostics.get('is_index_id_map', False):
                logger.info("Migrating to IndexIDMap as requested by configuration")
                need_repair = True
            
            # Perform repair if needed
            if need_repair:
                logger.info("Performing automatic index repair...")
                
                # First make sure we're using IndexIDMap
                if not diagnostics.get('is_index_id_map', False):
                    success = self.vector_index.migrate_to_idmap(force_cpu=True)
                    if success:
                        logger.info("Successfully migrated vector index to IndexIDMap")
                    else:
                        logger.warning("Failed to migrate vector index to IndexIDMap. Will try repair_index next.")
                
                # If we still have inconsistency, run the repair
                is_consistent, diagnostics = self.vector_index.verify_index_integrity()
                if not is_consistent:
                    asyncio.create_task(self.repair_index("recreate_mapping"))
                    logger.info("Scheduled automatic repair_index to fix inconsistencies")
            
            # Final index stats after initialization
            logger.info(f"Vector index initialized with {self.vector_index.count()} vectors and "
                        f"{len(self.vector_index.id_to_index)} ID mappings")

            # Start background tasks only if intervals are > 0
            if self.config['persistence_interval'] > 0:
                 self._background_tasks.append(asyncio.create_task(self._persistence_loop(), name=f"PersistenceLoop_{id(self)}"))
            else:
                logger.warning("SynthiansMemoryCore", "Persistence loop disabled (interval <= 0)")

            if self.config['decay_interval'] > 0 or self.config['prune_check_interval'] > 0:
                self._background_tasks.append(asyncio.create_task(self._decay_and_pruning_loop(), name=f"DecayPruningLoop_{id(self)}"))
            else:
                logger.warning("SynthiansMemoryCore", "Decay/Pruning loop disabled (intervals <= 0)")

            self._initialized = True
            logger.info("SynthiansMemoryCore", "Initialization complete. Background tasks started.")
        return True

    async def shutdown(self):
        """Gracefully shut down the memory core."""
        if not self._initialized:
            logger.info("SynthiansMemoryCore", "Shutdown called but not initialized.")
            return

        logger.info("SynthiansMemoryCore", "Shutting down...")
        # Signal loops to stop checking/sleeping first
        self._shutdown_signal.set()
        # Give loops a brief moment to recognize the signal
        await asyncio.sleep(0.05)

        # Cancel active tasks
        tasks_to_cancel = []
        for task in self._background_tasks:
            if task and not task.done():
                # Don't cancel if already cancelling
                if not task.cancelling():
                    task.cancel()
                    tasks_to_cancel.append(task)

        # Wait for tasks to complete cancellation
        if tasks_to_cancel:
            logger.info(f"Waiting for {len(tasks_to_cancel)} background tasks to cancel...")
            # Use return_exceptions=True so one failed task doesn't stop others
            # Wait for a reasonable time (e.g., 5 seconds) for tasks to finish cancelling
            results = await asyncio.gather(*tasks_to_cancel, return_exceptions=True)
            logger.info("Background tasks cancellation completed.")
            # Check for exceptions during cancellation
            for i, result in enumerate(results):
                task_name = tasks_to_cancel[i].get_name() if hasattr(tasks_to_cancel[i], 'get_name') else f"Task-{i}"
                if isinstance(result, asyncio.CancelledError):
                    logger.debug(f"{task_name} was cancelled successfully.")
                elif isinstance(result, Exception):
                    logger.error(f"Error during cancellation of {task_name}: {result}", exc_info=result)
        else:
            logger.info("No active background tasks found to cancel.")

        # Clear the list of tasks *after* attempting cancellation
        self._background_tasks = []

        # --- Critical: Call persistence shutdown *before* resetting state ---
        # This allows persistence to do its final save using the current state
        logger.info("SynthiansMemoryCore", "Calling persistence shutdown...")
        if hasattr(self, 'persistence') and self.persistence:
            try:
                # Add a timeout for safety
                await asyncio.wait_for(self.persistence.shutdown(), timeout=5.0)
                logger.info("SynthiansMemoryCore", "Persistence shutdown completed.")
            except asyncio.TimeoutError:
                logger.warning("SynthiansMemoryCore", "Timeout waiting for persistence shutdown")
            except Exception as e:
                logger.error("SynthiansMemoryCore", f"Error during persistence shutdown: {str(e)}", exc_info=True)
        else:
             logger.warning("SynthiansMemoryCore", "Persistence object not available during shutdown.")

        # Reset state
        self._initialized = False
        # Reset shutdown signal for potential re-initialization
        self._shutdown_signal = asyncio.Event()
        logger.info("SynthiansMemoryCore", "Shutdown sequence complete.")

    # --- Core Memory Operations ---

    async def process_memory(self,
                           content: Optional[str] = None,
                           embedding: Optional[Union[np.ndarray, List[float]]] = None,
                           metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """API-compatible wrapper for process_new_memory."""
        if not self._initialized: await self.initialize()

        # Call the underlying implementation
        memory = await self.process_new_memory(content=content, embedding=embedding, metadata=metadata)

        if memory:
            return {
                "success": True, # Add success flag
                "memory_id": memory.id,
                "quickrecal_score": memory.quickrecal_score,
                "embedding": memory.embedding.tolist() if memory.embedding is not None else None, # Include embedding
                "metadata": memory.metadata
            }
        else:
            return {
                "success": False, # Add success flag
                "memory_id": None,
                "quickrecal_score": None,
                "error": "Failed to process memory"
            }

    async def process_new_memory(self,
                                 content: str,
                                 embedding: Optional[Union[np.ndarray, List[float]]] = None,
                                 metadata: Optional[Dict[str, Any]] = None) -> Optional[MemoryEntry]:
        """Process and store a new memory entry."""
        if not self._initialized: await self.initialize()
        start_time = time.time()
        metadata = metadata or {}

        # 1. Validate/Generate Embedding
        if embedding is None:
            logger.info("SynthiansMemoryCore", "Generating embedding for new memory...")
            embedding = await self.generate_embedding(content) # Generate if not provided
            if embedding is None:
                logger.error("SynthiansMemoryCore", "Failed to generate embedding, cannot process memory.")
                return None

        # Handle common case where embedding is wrongly passed as a dict
        if isinstance(embedding, dict):
            logger.warning("SynthiansMemoryCore", f"Received embedding as dict type, attempting to extract vector")
            try:
                if 'embedding' in embedding and isinstance(embedding['embedding'], (list, np.ndarray)): embedding = embedding['embedding']
                elif 'vector' in embedding and isinstance(embedding['vector'], (list, np.ndarray)): embedding = embedding['vector']
                elif 'value' in embedding and isinstance(embedding['value'], (list, np.ndarray)): embedding = embedding['value']
                else: raise ValueError(f"Could not extract embedding from dict keys: {list(embedding.keys())[:5]}")
            except Exception as e:
                logger.error("SynthiansMemoryCore", f"Failed to extract embedding from dict: {str(e)}")
                return None

        validated_embedding = self.geometry_manager._validate_vector(embedding, "Input Embedding")
        if validated_embedding is None:
             logger.error("SynthiansMemoryCore", "Invalid embedding provided, cannot process memory.")
             return None
        aligned_embedding, _ = self.geometry_manager._align_vectors(validated_embedding, np.zeros(self.config['embedding_dim']))
        normalized_embedding = self.geometry_manager._normalize(aligned_embedding)

        # 2. Calculate QuickRecal Score
        context = {'timestamp': time.time(), 'metadata': metadata}
        # Include momentum buffer if available/needed by the mode
        # context['external_momentum'] = ...
        quickrecal_score = await self.quick_recal.calculate(normalized_embedding, text=content, context=context)

        # 3. Analyze Emotion only if not already provided
        emotional_context = metadata.get("emotional_context")
        if not emotional_context:
            logger.info("SynthiansMemoryCore", "Analyzing emotional context for memory")
            emotional_context = await self.emotional_analyzer.analyze(content)
            # Do not add to metadata here, let synthesizer handle it
        else:
            logger.debug("SynthiansMemoryCore", "Using precomputed emotional context from metadata")

        # 4. Generate Hyperbolic Embedding (if enabled)
        hyperbolic_embedding = None
        if self.geometry_manager.config['geometry_type'] == GeometryType.HYPERBOLIC:
            hyperbolic_embedding = self.geometry_manager._to_hyperbolic(normalized_embedding)

        # 5. Run Metadata Synthesizer
        # Pass the analyzed emotion data directly to the synthesizer
        metadata = await self.metadata_synthesizer.synthesize(
            content=content,
            embedding=normalized_embedding,
            base_metadata=metadata,
            emotion_data=emotional_context # Pass pre-analyzed data
        )

        # 6. Create Memory Entry
        memory = MemoryEntry(
            content=content,
            embedding=normalized_embedding,
            quickrecal_score=quickrecal_score,
            metadata=metadata,
            hyperbolic_embedding=hyperbolic_embedding
        )

        # Add memory ID to metadata for easier access
        memory.metadata["uuid"] = memory.id

        # 7. Store in memory and mark as dirty
        async with self._lock:
            self._memories[memory.id] = memory
            self._dirty_memories.add(memory.id) # Mark for persistence
            logger.info("SynthiansMemoryCore", f"Stored new memory {memory.id}", {"quickrecal": quickrecal_score})

        # 8. Update Assemblies
        await self._update_assemblies(memory)

        # 9. Add to vector index for fast retrieval
        self.vector_index.add(memory.id, normalized_embedding)
        logger.debug("SynthiansMemoryCore", f"Added memory {memory.id} to vector index")

        proc_time = (time.time() - start_time) * 1000
        logger.debug("SynthiansMemoryCore", f"Processed new memory {memory.id}", {"time_ms": proc_time})
        return memory

    async def retrieve_memories(
        self,
        query: str,
        top_k: int = 5,
        threshold: Optional[float] = None,
        user_emotion: Optional[str] = None, # Changed to Optional[str] to match server endpoint
        metadata_filter: Optional[Dict[str, Any]] = None,
        search_strategy: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Retrieve memories based on query relevance.
        Handles potential query_embedding generation internally.
        """
        if not self._initialized: await self.initialize()
        start_time = time.time()
        
        # Add diagnostic logging for parameter passing
        logger.debug(f"[retrieve_memories] START retrieve_memories: Received threshold argument = {threshold} (type: {type(threshold)})")
        
        query_embedding = None
        try:
            # Generate embedding for the query if necessary
            if query:
                query_embedding = await self.generate_embedding(query)
                if query_embedding is None:
                     logger.error("SynthiansMemoryCore", "Failed to generate query embedding.")
                     return {"success": False, "memories": [], "error": "Failed to generate query embedding"}
                logger.debug("SynthiansMemoryCore", "Query embedding generated")
                
                # Validate and normalize query embedding first
                query_embedding = self.geometry_manager._validate_vector(query_embedding, "Query Embedding")
                if query_embedding is None:
                    logger.error("SynthiansMemoryCore", "Query embedding validation failed")
                    return {"success": False, "memories": [], "error": "Invalid query embedding"}
                logger.debug(f"Validated query embedding - shape: {query_embedding.shape}")

            # Get the current threshold
            current_threshold = threshold
            if current_threshold is None and self.threshold_calibrator is not None:
                current_threshold = self.threshold_calibrator.get_current_threshold()
                logger.debug(f"Using calibrated threshold: {current_threshold:.4f}")
            elif current_threshold is None:
                # TEMPORARILY set threshold to 0.0 for debugging the '0 memories' issue
                # Will revert to self.config['initial_retrieval_threshold'] once issue is resolved
                current_threshold = 0.0  # DEBUG: Lowered to 0.0 to see if any memories pass
                logger.warning(f"[DEBUG MODE] Using debug threshold of {current_threshold} to diagnose '0 memories' issue")
            else:
                logger.debug(f"Using explicit threshold from request: {current_threshold:.4f}")

            # Make vector index integrity check configurable and periodic
            check_index = self.config.get('check_index_on_retrieval', False)
            current_time = time.time()
            last_check_time = getattr(self, '_last_index_check_time', 0)
            check_interval = self.config.get('index_check_interval', 3600)  # Default: check once per hour
            
            if check_index or (current_time - last_check_time > check_interval):
                is_consistent, diagnostics = self.vector_index.verify_index_integrity()
                self._last_index_check_time = current_time
                logger.debug(f"Vector index status - Consistent: {is_consistent}, FAISS: {diagnostics.get('faiss_count')}, Mapping: {diagnostics.get('mapping_count')}")
                
                # Warn if inconsistency detected
                if not is_consistent:
                    logger.warning(f"Vector index inconsistency detected! FAISS count: {diagnostics.get('faiss_count')}, Mapping count: {diagnostics.get('mapping_count')}")

            # Perform the retrieval using candidate generation
            candidates = await self._get_candidate_memories(query_embedding, top_k * 2) # Get more candidates for filtering
            
            # ENHANCED: Log the raw candidates with more detail
            logger.info(f"[FAISS Results] Raw candidates count: {len(candidates)}")
            candidate_ids = [c.get('id') for c in candidates[:10]]
            logger.debug(f"First 10 candidate IDs: {candidate_ids}")
            
            # If no candidates found, return empty results
            if not candidates:
                logger.debug(f"No candidate memories found.")
                return {"success": True, "memories": [], "error": None}

            # Score candidates based on similarity to query
            scored_candidates = []
            if query_embedding is not None:
                logger.debug(f"Query embedding dimension: {query_embedding.shape}")
                logger.warning(f"CRITICAL DEBUG: Found {len(candidates)} raw candidates - first ID: {candidates[0].get('id') if candidates else 'None'}")
                
            for memory_dict in candidates:
                memory_embedding_list = memory_dict.get("embedding")
                if memory_embedding_list is not None and query_embedding is not None:
                    try:
                        # Re-convert list to numpy array
                        memory_embedding_np = np.array(memory_embedding_list, dtype=np.float32)
                        
                        # ENHANCED: Add detailed validation logging
                        mem_id = memory_dict.get('id')
                        logger.debug(f"Processing memory {mem_id} for similarity calculation")
                        
                        # ADDED: Explicit validation of memory embedding
                        memory_embedding_np = self.geometry_manager._validate_vector(memory_embedding_np, f"Memory {mem_id}")
                        if memory_embedding_np is None:
                            logger.warning(f"Memory {mem_id} embedding validation failed. Using zero vector.")
                            memory_embedding_np = np.zeros(self.config['embedding_dim'], dtype=np.float32)
                        
                        # ADDED: Explicit alignment of vectors before similarity calculation
                        before_shapes = f"Before alignment - Query: {query_embedding.shape}, Memory: {memory_embedding_np.shape}"
                        logger.debug(before_shapes)
                        
                        aligned_query, aligned_memory = self.geometry_manager._align_vectors(query_embedding, memory_embedding_np)
                        
                        after_shapes = f"After alignment - Query: {aligned_query.shape}, Memory: {aligned_memory.shape}"
                        logger.debug(after_shapes)
                        
                        # Check for NaN or Inf values in aligned vectors
                        if np.isnan(aligned_memory).any() or np.isinf(aligned_memory).any():
                            logger.warning(f"Memory {mem_id} aligned embedding contains NaN/Inf values. Replacing with zeros.")
                            aligned_memory = np.nan_to_num(aligned_memory, nan=0.0, posinf=0.0, neginf=0.0)
                        
                        # Use GeometryManager to calculate similarity with aligned vectors
                        similarity = self.geometry_manager.calculate_similarity(aligned_query, aligned_memory)
                        logger.debug(f"  Calculated similarity: {similarity:.4f}")
                        
                        memory_dict["similarity"] = similarity
                        memory_dict["relevance_score"] = similarity
                        scored_candidates.append(memory_dict)
                        logger.debug(f"Memory {mem_id}: similarity={similarity:.4f}")
                    except Exception as e:
                        # Log the specific exception
                        logger.warning(f"Error calculating similarity for memory {memory_dict.get('id')}: {str(e)}")
                        logger.debug(traceback.format_exc())  # ADDED: Include stack trace for debugging
                        # Fallback: Include the memory with zero similarity rather than skipping it
                        memory_dict["similarity"] = 0.0
                        memory_dict["relevance_score"] = 0.0
                        scored_candidates.append(memory_dict)
                else:
                    # Log which specific condition failed
                    if memory_embedding_list is None:
                        logger.warning(f"Memory {memory_dict.get('id')} is missing embedding")
                    if query_embedding is None:
                        logger.warning("Query embedding is None")
                    
                    # Even if embedding is missing, include in results with zero similarity
                    memory_dict["similarity"] = 0.0
                    memory_dict["relevance_score"] = 0.0
                    scored_candidates.append(memory_dict)
            
            # Sort by similarity score (descending)
            scored_candidates.sort(key=lambda x: x.get("similarity", 0.0), reverse=True)

            # ENHANCED: Log all candidates with their scores before filtering
            logger.info(f"[Similarity Results] Found {len(scored_candidates)} scored candidates before threshold filtering")
            logger.debug(f"Threshold filtering: Using threshold {current_threshold:.4f}")
            
            similarities = [(c.get('id'), c.get('similarity', 0.0)) for c in scored_candidates[:10]]
            logger.debug(f"Top 10 similarities: {similarities}")
            
            # Apply threshold filtering
            logger.info(f"[Threshold Filtering] Starting threshold filtering with {len(scored_candidates)} candidates")
            candidates_passing_threshold = []
            candidates_filtered_out = []
            
            for c in scored_candidates:
                similarity = c.get("similarity", 0.0)
                mem_id = c.get("id", "unknown")
                if similarity >= current_threshold:
                    candidates_passing_threshold.append(c)
                    logger.debug(f"Memory {mem_id} PASSED threshold with similarity {similarity:.4f} >= {current_threshold:.4f}")
                else:
                    candidates_filtered_out.append((mem_id, similarity))
                    logger.debug(f"Memory {mem_id} FILTERED OUT with similarity {similarity:.4f} < {current_threshold:.4f}")
            
            # Log summary of threshold filtering results
            filtered_candidates = candidates_passing_threshold
            logger.info(f"[Threshold Filtering] Kept {len(filtered_candidates)} candidates, filtered out {len(candidates_filtered_out)} candidates")
            
            # Log the first few filtered out candidates for debugging
            if candidates_filtered_out:
                logger.debug(f"First 5 filtered out (ID, similarity): {candidates_filtered_out[:5]}")

            # *** ADDED PRE-GATING LOG ***
            if filtered_candidates:
                top_passing = [(c.get('id'), c.get('similarity', 0.0)) for c in filtered_candidates[:5]]
                logger.info(f"[Pre-Emotional Gating] Top 5 candidates: {top_passing}")
            else:
                logger.warning(f"[Pre-Emotional Gating] No candidates passed threshold filtering. Consider lowering threshold.")

            # Apply emotional gating if requested
            if user_emotion and self.emotional_gating:
                logger.info(f"[Emotional Gating] Applying with user_emotion: {user_emotion}, candidates: {len(filtered_candidates)}") 
                # Construct user_emotion dict for gating service
                user_emotion_dict = {"dominant_emotion": user_emotion} # Simulate expected input for gating service
                
                # Store candidate count before gating for comparison
                pre_gating_count = len(filtered_candidates)
                
                filtered_candidates = await self.emotional_gating.gate_memories(
                    filtered_candidates, user_emotion_dict
                )
                
                # Calculate and log difference in candidate count
                post_gating_count = len(filtered_candidates)
                diff_count = pre_gating_count - post_gating_count
                logger.info(f"[Emotional Gating] Result: {post_gating_count} candidates remain ({diff_count} removed)") 
                
                # Re-sort based on 'final_score' if gating was applied
                filtered_candidates.sort(key=lambda x: x.get("final_score", x.get("similarity", 0.0)), reverse=True)
                
                # Log the new ordering after emotional gating
                if filtered_candidates:
                    top_emotional = [(c.get('id'), c.get('final_score', c.get('similarity', 0.0))) 
                                    for c in filtered_candidates[:5]]
                    logger.debug(f"[Post-Emotional Gating] Top 5 candidates with scores: {top_emotional}")
            else:
                logger.debug("[Emotional Gating] Skipped (user_emotion is None or no gating service)") 
            
            # Apply metadata filtering if requested (basic implementation)
            if metadata_filter:
                logger.info(f"[Metadata Filtering] Applying filter: {metadata_filter}") 
                pre_filter_count = len(filtered_candidates)
                
                filtered_candidates = self._filter_by_metadata(filtered_candidates, metadata_filter)
                
                post_filter_count = len(filtered_candidates)
                filter_diff = pre_filter_count - post_filter_count
                logger.info(f"[Metadata Filtering] Result: {post_filter_count} candidates remain ({filter_diff} removed)") 
                
                # Log the metadata of the remaining candidates
                if filtered_candidates:
                    # Get the first candidate's metadata keys for reference
                    first_meta_keys = list(filtered_candidates[0].get("metadata", {}).keys())[:5]  # First 5 keys
                    logger.debug(f"[Post-Metadata Filtering] First candidate metadata keys: {first_meta_keys}")
            else:
                logger.debug("[Metadata Filtering] Skipped (no metadata filter provided)")

            # *** ENHANCED POST-FILTERING LOG ***
            logger.info(f"[Final Filtering] Total filtered candidates: {len(filtered_candidates)}")
            if filtered_candidates:
                final_top_ids = [c.get('id') for c in filtered_candidates[:5]]
                logger.info(f"[Final Filtering] Top 5 candidate IDs after all filtering: {final_top_ids}")
            else:
                logger.warning("[Final Filtering] No candidates remain after all filtering steps")

            # Return top_k results (simplify slicing)
            if len(filtered_candidates) >= top_k:
                final_memories = filtered_candidates[:top_k]
                logger.info(f"[Results] Returning {top_k} memories out of {len(filtered_candidates)} filtered candidates")
            else:
                final_memories = filtered_candidates.copy() # Take all if fewer than top_k, and make a copy to be safe
                logger.info(f"[Results] Returning all {len(final_memories)} filtered candidates (fewer than requested {top_k})")

            # *** ENHANCED FINAL CHECK ***
            if final_memories:
                final_ids = [mem.get('id') for mem in final_memories]
                final_scores = [mem.get('similarity', 0.0) for mem in final_memories]
                logger.info(f"[Results] Final memory IDs: {final_ids}")
                logger.info(f"[Results] Final similarity scores: {final_scores}")
            else:
                logger.warning("[Results] No memories to return!")

            retrieval_time = (time.time() - start_time) * 1000
            # Log the length again, just before returning
            logger.info("SynthiansMemoryCore", f"Retrieved {len(final_memories)} memories", {
                "top_k": top_k, "threshold": current_threshold, "user_emotion": user_emotion, "time_ms": retrieval_time
            })
            
            # DIRECT DEBUG: Log full response payload length
            response = {"success": True, "memories": final_memories, "error": None}
            logger.info(f"[Response] Payload stats: success={response['success']}, memories_count={len(response['memories'])}")
            
            return response

        except Exception as e:
            logger.error("SynthiansMemoryCore", f"Error in retrieve_memories: {str(e)}")
            logger.error(traceback.format_exc())
            return {"success": False, "memories": [], "error": str(e)}

    async def _get_candidate_memories(self, query_embedding: Optional[np.ndarray], limit: int) -> List[Dict[str, Any]]:
        """Retrieve candidate memories using assembly activation and direct vector search."""
        if query_embedding is None:
            logger.warning("SynthiansMemoryCore", "_get_candidate_memories called with no query embedding.")
            return []

        # Log the query embedding stats for debugging
        if query_embedding is not None:
            logger.debug(f"[Candidate Gen] Query embedding shape: {query_embedding.shape}, sum: {np.sum(query_embedding):.4f}, mean: {np.mean(query_embedding):.4f}")
            if np.isnan(query_embedding).any() or np.isinf(query_embedding).any():
                logger.warning(f"[Candidate Gen] WARNING: Query embedding contains NaN/Inf values!")

        assembly_candidates = set()
        direct_candidates = set()

        # 1. Assembly Activation
        activated_assemblies = await self._activate_assemblies(query_embedding)
        for assembly, activation_score in activated_assemblies[:5]: # Consider top 5 assemblies
            if activation_score > 0.2: # Lower activation threshold
                assembly_candidates.update(assembly.memories)
        
        logger.info(f"[Candidate Gen] Found {len(assembly_candidates)} candidates from assembly activation")

        # 2. Direct Vector Search using FAISS Index
        search_threshold = 0.0  # Set to zero to get all candidates regardless of similarity
        faiss_count = self.vector_index.count()
        id_mapping_count = len(self.vector_index.id_to_index) if hasattr(self.vector_index, 'id_to_index') else 0
        
        logger.info(f"[Candidate Gen] Vector index stats: FAISS count={faiss_count}, ID mapping count={id_mapping_count}")
        
        # Check if index is empty
        if faiss_count == 0:
            logger.warning(f"[Candidate Gen] FAISS index is empty! Check memory creation and indexing.")
        
        search_results = self.vector_index.search(query_embedding, k=min(limit, max(faiss_count, 1)))
        
        logger.info(f"[Candidate Gen] FAISS search returned {len(search_results)} results")
        
        # Log detailed search results
        if search_results:
            top_results = search_results[:5] if len(search_results) > 5 else search_results
            result_details = [f"({mem_id}, {sim:.4f})" for mem_id, sim in top_results]
            logger.info(f"[Candidate Gen] Top FAISS results: {', '.join(result_details)}")
        else:
            logger.warning(f"[Candidate Gen] FAISS search returned ZERO results! Check indexing.")
            
        for memory_id, similarity in search_results:
            direct_candidates.add(memory_id)

        # 3. Get the most recently added memories as fallback
        # This ensures we always have candidates even if similarity search fails
        async with self._lock:
            # Get IDs of memories in our persistence index
            memory_ids = list(self.persistence.memory_index.keys())
            logger.info(f"[Candidate Gen] Persistence index has {len(memory_ids)} memories total")
            
        # Take the most recent ones if we have any
        if memory_ids and len(direct_candidates) == 0:
            # Sort by creation time if available, otherwise just take the last few
            recent_candidates = set(memory_ids[-min(5, len(memory_ids)):])  # Get the last 5 memories
            logger.info(f"[Candidate Gen] Added {len(recent_candidates)} recent memories as fallback candidates: {list(recent_candidates)}")
            direct_candidates.update(recent_candidates)
        elif len(memory_ids) == 0:
            logger.warning(f"[Candidate Gen] Persistence index is EMPTY! No memories have been created.")

        # Combine candidates
        all_candidate_ids = assembly_candidates.union(direct_candidates)
        logger.info(f"[Candidate Gen] Found {len(all_candidate_ids)} total candidate IDs: {list(all_candidate_ids)[:10]}")

        # Fetch MemoryEntry objects as dictionaries
        final_candidates = []
        for mem_id in all_candidate_ids:
            # Log before attempting to load
            logger.debug(f"[Candidate Gen] Attempting to load memory with ID: {mem_id}")
            # Use our new async method to get the memory from disk if not in cache
            memory = await self.get_memory_by_id_async(mem_id)
            if memory:
                # Make sure to convert memory to dict before returning
                mem_dict = memory.to_dict()
                final_candidates.append(mem_dict)
                logger.debug(f"[Candidate Gen] Successfully loaded memory {mem_id}: content_len={len(mem_dict.get('content', ''))}, embedding_shape={memory.embedding.shape if memory.embedding is not None else 'None'}")
            else:
                logger.warning(f"[Candidate Gen] Failed to load memory {mem_id}! Check persistence storage.")

        # Always ensure we return at least some candidates for scoring/filtering
        if len(final_candidates) == 0:
            logger.warning("[Candidate Gen] No candidates found after loading! This will result in empty retrieval results.")
            # Log vector index statistics to help debug
            is_consistent, diagnostics = self.vector_index.verify_index_integrity()
            logger.warning(f"[Candidate Gen] Vector index diagnostics: consistent={is_consistent}, {diagnostics}")
            
            # Check storage files
            import os
            if hasattr(self.persistence, 'storage_path'):
                storage_files = os.listdir(self.persistence.storage_path) if os.path.exists(self.persistence.storage_path) else []
                logger.warning(f"[Candidate Gen] Storage directory contents: {storage_files[:10]}")
                
                # Check for FAISS index file
                faiss_path = os.path.join(self.persistence.storage_path, 'faiss_index.bin')
                mapping_path = os.path.join(self.persistence.storage_path, 'id_to_index_mapping.json')
                logger.warning(f"[Candidate Gen] FAISS index file exists: {os.path.exists(faiss_path)}")
                logger.warning(f"[Candidate Gen] ID mapping file exists: {os.path.exists(mapping_path)}")

        logger.info(f"[Candidate Gen] Returning {len(final_candidates)} final candidates for scoring/filtering")
        return final_candidates[:limit * 2] # Return more initially for scoring/filtering

    async def _activate_assemblies(self, query_embedding: np.ndarray) -> List[Tuple[MemoryAssembly, float]]:
        """Find and activate assemblies based on query similarity."""
        activated = []
        async with self._lock: # Accessing shared self.assemblies
            for assembly_id, assembly in self.assemblies.items():
                 similarity = assembly.get_similarity(query_embedding)
                 if similarity >= self.config['assembly_threshold'] * 0.8: # Lower threshold for activation
                      assembly.activate(similarity)
                      activated.append((assembly, similarity))
        # Sort by activation score
        activated.sort(key=lambda x: x[1], reverse=True)
        return activated

    async def _update_assemblies(self, memory: MemoryEntry):
        """Find or create assemblies for a new memory."""
        if memory.embedding is None: return

        suitable_assemblies = []
        best_similarity = 0.0
        best_assembly_id = None

        async with self._lock: # Accessing shared self.assemblies
             for assembly_id, assembly in self.assemblies.items():
                  similarity = assembly.get_similarity(memory.embedding)
                  if similarity >= self.config['assembly_threshold']:
                       suitable_assemblies.append((assembly_id, similarity))
                  if similarity > best_similarity:
                       best_similarity = similarity
                       best_assembly_id = assembly_id

        # Sort suitable assemblies by similarity
        suitable_assemblies.sort(key=lambda x: x[1], reverse=True)

        # Add memory to best matching assemblies (up to max limit)
        added_count = 0
        assemblies_updated = set()
        for assembly_id, _ in suitable_assemblies[:self.config['max_assemblies_per_memory']]:
            async with self._lock: # Lock for modifying assembly
                 if assembly_id in self.assemblies:
                     assembly = self.assemblies[assembly_id]
                     if assembly.add_memory(memory):
                          added_count += 1
                          assemblies_updated.add(assembly_id)
                          # Update memory_to_assemblies mapping
                          if memory.id not in self.memory_to_assemblies:
                               self.memory_to_assemblies[memory.id] = set()
                          self.memory_to_assemblies[memory.id].add(assembly_id)
                          self._dirty_memories.add(assembly.assembly_id) # Mark assembly as dirty

        # If no suitable assembly found, consider creating a new one
        if added_count == 0 and best_similarity > self.config['assembly_threshold'] * 0.5: # Threshold to create new
             async with self._lock: # Lock for creating new assembly
                 # Double check if a suitable assembly was created concurrently
                 assembly_exists = False
                 for asm_id in self.memory_to_assemblies.get(memory.id, set()):
                      if asm_id in self.assemblies: assembly_exists = True; break

                 if not assembly_exists:
                     logger.info("SynthiansMemoryCore", f"Creating new assembly seeded by memory {memory.id[:8]}")
                     new_assembly = MemoryAssembly(geometry_manager=self.geometry_manager, name=f"Assembly around {memory.id[:8]}")
                     if new_assembly.add_memory(memory):
                          self.assemblies[new_assembly.assembly_id] = new_assembly
                          assemblies_updated.add(new_assembly.assembly_id)
                          # Update mapping
                          if memory.id not in self.memory_to_assemblies:
                               self.memory_to_assemblies[memory.id] = set()
                          self.memory_to_assemblies[memory.id].add(new_assembly.assembly_id)
                          added_count += 1
                          self._dirty_memories.add(new_assembly.assembly_id) # Mark assembly as dirty

        if added_count > 0:
             logger.debug("SynthiansMemoryCore", f"Updated {added_count} assemblies for memory {memory.id}", {"assemblies": list(assemblies_updated)})


    async def provide_feedback(self, memory_id: str, similarity_score: float, was_relevant: bool):
        """Provide feedback to the threshold calibrator."""
        if self.threshold_calibrator:
            self.threshold_calibrator.record_feedback(similarity_score, was_relevant)
            logger.debug("SynthiansMemoryCore", "Recorded feedback", {"memory_id": memory_id, "score": similarity_score, "relevant": was_relevant})

    async def detect_contradictions(self, threshold: float = 0.75) -> List[Dict[str, Any]]:
        """Detect potential causal contradictions using embeddings."""
        contradictions = []
        async with self._lock: # Access shared _memories
            memories_list = list(self._memories.values())

        # Basic Keyword Filtering for Causal Statements (Can be improved with NLP)
        causal_keywords = ["causes", "caused", "leads to", "results in", "effect of", "affects"]
        causal_memories = [m for m in memories_list if m.embedding is not None and any(k in m.content.lower() for k in causal_keywords)]

        if len(causal_memories) < 2: return []

        logger.info("SynthiansMemoryCore", f"Checking {len(causal_memories)} causal memories for contradictions.")

        # Compare pairs (simplified N^2 comparison, can be optimized)
        compared_pairs = set()
        for i in range(len(causal_memories)):
            for j in range(i + 1, len(causal_memories)):
                mem_a = causal_memories[i]
                mem_b = causal_memories[j]

                # Calculate similarity
                similarity = self.geometry_manager.calculate_similarity(mem_a.embedding, mem_b.embedding)

                # Basic Topic Overlap Check (can be improved)
                words_a = set(mem_a.content.lower().split())
                words_b = set(mem_b.content.lower().split())
                common_words = words_a.intersection(words_b)
                overlap_ratio = len(common_words) / min(len(words_a), len(words_b)) if min(len(words_a), len(words_b)) > 0 else 0

                # Check for potential semantic opposition (basic keyword check)
                opposites = [("increase", "decrease"), ("up", "down"), ("positive", "negative"), ("high", "low")]
                has_opposite = False
                content_a_lower = mem_a.content.lower()
                content_b_lower = mem_b.content.lower()
                for w1, w2 in opposites:
                    if (w1 in content_a_lower and w2 in content_b_lower) or \
                       (w2 in content_a_lower and w1 in content_b_lower):
                        has_opposite = True
                        break

                # If high similarity, sufficient topic overlap, and potential opposition -> contradiction
                if similarity >= threshold and overlap_ratio > 0.3 and has_opposite:
                     contradictions.append({
                          "memory_a_id": mem_a.id,
                          "memory_a_content": mem_a.content,
                          "memory_b_id": mem_b.id,
                          "memory_b_content": mem_b.content,
                          "similarity": similarity,
                          "overlap_ratio": overlap_ratio
                     })

        logger.info("SynthiansMemoryCore", f"Detected {len(contradictions)} potential contradictions.")
        return contradictions


    # --- Background Tasks ---

    async def _persistence_loop(self):
        """Periodically persist changed memories."""
        logger.info("SynthiansMemoryCore","Persistence loop started.")
        persist_interval = self.config.get('persistence_interval', 60.0)
        try:
            while not self._shutdown_signal.is_set():
                try:
                    # Wait for the configured interval OR the shutdown signal
                    await asyncio.wait_for(
                        self._shutdown_signal.wait(),
                        timeout=persist_interval
                    )
                    # If wait() finished without timeout, it means signal was set
                    logger.info("SynthiansMemoryCore","Persistence loop: Shutdown signal received during wait.")
                    break # Exit loop if shutdown signal is set
                except asyncio.TimeoutError:
                    # Timeout occurred, time to persist
                    if not self._shutdown_signal.is_set(): # Double-check signal
                        logger.debug("SynthiansMemoryCore", "Running periodic persistence.")
                        await self._persist_dirty_items() # Persist dirty items
                except asyncio.CancelledError:
                    logger.info("SynthiansMemoryCore","Persistence loop cancelled during wait.")
                    break # Exit loop if cancelled
        except asyncio.CancelledError:
            logger.info("SynthiansMemoryCore","Persistence loop received cancel signal.")
        except Exception as e:
            logger.error("SynthiansMemoryCore","Persistence loop error", {"error": str(e)}, exc_info=True)
        finally:
            # Remove final save attempt to avoid 'no running event loop' errors
            # The main shutdown method should handle any critical final saves
            logger.info("SynthiansMemoryCore","Persistence loop stopped.")

    async def _decay_and_pruning_loop(self):
        """Periodically decay memory scores and prune old/irrelevant memories."""
        logger.info("SynthiansMemoryCore","Decay/Pruning loop started.")
        decay_interval = self.config.get('decay_interval', 3600.0)
        prune_interval = self.config.get('prune_check_interval', 600.0)
        # Determine the shortest interval to check the shutdown signal more frequently
        check_interval = min(decay_interval, prune_interval, 5.0) # Check at least every 5s
        last_decay_time = time.monotonic()
        last_prune_time = time.monotonic()
        try:
            while not self._shutdown_signal.is_set():
                try:
                    # Wait for the check interval OR the shutdown signal
                    await asyncio.wait_for(
                        self._shutdown_signal.wait(),
                        timeout=check_interval
                    )
                    # If wait() finished without timeout, it means signal was set
                    logger.info("SynthiansMemoryCore","Decay/Pruning loop: Shutdown signal received during wait.")
                    break # Exit loop if shutdown signal is set
                except asyncio.TimeoutError:
                    # Timeout occurred, check if it's time for decay or pruning
                    now = time.monotonic()
                    if not self._shutdown_signal.is_set():
                        # Decay Check
                        if now - last_decay_time >= decay_interval:
                            logger.info("SynthiansMemoryCore","Running memory decay check.")
                            try:
                                await self._apply_decay()
                                last_decay_time = now
                            except Exception as decay_e:
                                logger.error("SynthiansMemoryCore","Error during decay application", {"error": str(decay_e)})
                        # Pruning Check (can happen more often than decay)
                        if now - last_prune_time >= prune_interval:
                             logger.debug("SynthiansMemoryCore","Running pruning check.")
                             try:
                                 await self._prune_if_needed()
                                 last_prune_time = now
                             except Exception as prune_e:
                                 logger.error("SynthiansMemoryCore","Error during pruning check", {"error": str(prune_e)})
                except asyncio.CancelledError:
                    logger.info("SynthiansMemoryCore","Decay/Pruning loop cancelled during wait.")
                    break # Exit loop if cancelled
        except asyncio.CancelledError:
            logger.info("SynthiansMemoryCore","Decay/Pruning loop received cancel signal.")
        except Exception as e:
            logger.error("SynthiansMemoryCore","Decay/Pruning loop error", {"error": str(e)}, exc_info=True)
        finally:
            logger.info("SynthiansMemoryCore","Decay/Pruning loop stopped.")

    async def _persist_dirty_items(self):
        """Persist all items marked as dirty."""
        # Get a copy of dirty IDs and clear the set under the lock
        async with self._lock:
            if not self._dirty_memories:
                logger.debug("SynthiansMemoryCore", "No dirty items to persist.")
                # Save index periodically even if no memories changed? Maybe not needed if index saved on add/delete.
                # await self.persistence._save_index_no_lock() # Save index under lock
                # vector_index_save_needed = ... # Logic to check if vector index needs saving
                # if vector_index_save_needed: self.vector_index.save() # Save outside lock?
                return

            logger.info(f"Persisting {len(self._dirty_memories)} dirty items...")
            items_to_save = list(self._dirty_memories)
            self._dirty_memories.clear() # Clear the set *after* copying

        # --- Perform saving outside the main core lock ---
        persist_count = 0
        persist_errors = 0
        batch_size = self.config.get('persistence_batch_size', 100)

        for i in range(0, len(items_to_save), batch_size):
            batch_ids = items_to_save[i:i+batch_size]
            save_tasks = []
            items_in_batch = {}

            # Get copies of items under lock first
            async with self._lock:
                 for item_id in batch_ids:
                     item = None
                     if item_id.startswith("mem_") and item_id in self._memories:
                         item = copy.deepcopy(self._memories[item_id])
                         item_type = "memory"
                     elif item_id.startswith("asm_") and item_id in self.assemblies:
                         item = copy.deepcopy(self.assemblies[item_id])
                         item_type = "assembly"

                     if item:
                         items_in_batch[item_id] = (item, item_type)
                     else:
                          logger.warning(f"Dirty item {item_id} not found in cache for persistence.")

            # Now create save tasks outside the lock
            for item_id, (item, item_type) in items_in_batch.items():
                 if item_type == "memory":
                      save_tasks.append(self.persistence.save_memory(item))
                 elif item_type == "assembly":
                      save_tasks.append(self.persistence.save_assembly(item))

            # Run tasks for the batch
            if save_tasks:
                 results = await asyncio.gather(*save_tasks, return_exceptions=True)
                 for result, item_id in zip(results, items_in_batch.keys()):
                      if isinstance(result, Exception) or result is False:
                           logger.error(f"Error persisting dirty item {item_id}", {"error": str(result)})
                           persist_errors += 1
                           # Optionally re-add to dirty set for next attempt?
                           # async with self._lock: self._dirty_memories.add(item_id)
                      else:
                           persist_count += 1

            # Check for shutdown signal between batches
            if self._shutdown_signal.is_set():
                logger.info("Persistence interrupted by shutdown signal.")
                break

        logger.info(f"Periodic persistence completed: Saved {persist_count} dirty items with {persist_errors} errors.")

        # Save index and vector index after processing dirty items
        await self.persistence._save_index() # Use the method with the lock
        if hasattr(self, 'vector_index') and self.vector_index:
            try:
                self.vector_index.save() # This might block briefly
            except Exception as e:
                 logger.error("Failed to save vector index during periodic persistence.", {"error": str(e)})

    async def _apply_decay(self):
        """Apply decay to QuickRecal scores."""
        async with self._lock:
             # No actual score modification needed, just update metadata if desired
             logger.info("SynthiansMemoryCore", f"Decay check completed for {len(self._memories)} memories (no scores changed).")


    async def _prune_if_needed(self):
        """Prune memories if storage limit is exceeded."""
        async with self._lock:
             current_size = len(self._memories)
             max_size = self.config['max_memory_entries']
             prune_threshold = int(max_size * self.config['prune_threshold_percent'])

             if current_size <= prune_threshold:
                  return # No pruning needed

             logger.info("SynthiansMemoryCore", f"Memory usage ({current_size}/{max_size}) exceeds threshold ({prune_threshold}). Starting pruning.")
             num_to_prune = current_size - int(max_size * 0.85) # Prune down to 85%

             # Get memories sorted by effective QuickRecal score (lowest first)
             scored_memories = [(mem.id, mem.get_effective_quickrecal(self.config['time_decay_rate'])) for mem in self._memories.values()]
             scored_memories.sort(key=lambda x: x[1])

             pruned_ids = []
             for mem_id, score in scored_memories[:num_to_prune]:
                 if score < self.config['min_quickrecal_for_ltm']:
                      pruned_ids.append(mem_id)

             if not pruned_ids:
                  logger.info("SynthiansMemoryCore", "No memories met pruning criteria.")
                  return

             logger.info(f"Identified {len(pruned_ids)} memories for pruning.")
             # Perform deletion outside the main iteration
             pruned_count = 0
             ids_to_remove_from_index = []
             for mem_id in pruned_ids:
                 if mem_id in self._memories:
                      del self._memories[mem_id]
                      ids_to_remove_from_index.append(mem_id) # Mark for vector index removal

                      # Also remove from assemblies mapping
                      if mem_id in self.memory_to_assemblies:
                           for asm_id in list(self.memory_to_assemblies[mem_id]): # Iterate over copy
                                if asm_id in self.assemblies:
                                     self.assemblies[asm_id].memories.discard(mem_id)
                                     self._dirty_memories.add(asm_id) # Mark assembly dirty
                           del self.memory_to_assemblies[mem_id]

                      # Delete from persistence - call async method
                      deleted_persistence = await self.persistence.delete_memory(mem_id)
                      if deleted_persistence:
                          pruned_count += 1
                      else:
                           logger.warning(f"Failed to delete memory {mem_id} from persistence.")

             # Remove from vector index if needed (Requires remove method)
             # if ids_to_remove_from_index:
             #     # removed_count = self.vector_index.remove(ids_to_remove_from_index)
             #     logger.warning(f"Vector index remove not implemented. Cannot remove {len(ids_to_remove_from_index)} pruned IDs.")


             logger.info("SynthiansMemoryCore", f"Pruned {pruned_count} memories.")

    # --- Tool Interface ---

    def get_tools(self) -> List[Dict[str, Any]]:
        """Return descriptions of available tools for LLM integration."""
        return [
            {
                "type": "function",
                "function": {
                    "name": "retrieve_memories_tool",
                    "description": "Retrieve relevant memories based on a query text.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "The search query."},
                            "top_k": {"type": "integer", "description": "Max number of results.", "default": 5},
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "process_new_memory_tool",
                    "description": "Process and store a new piece of information or experience.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "content": {"type": "string", "description": "The content of the memory."},
                            "metadata": {"type": "object", "description": "Optional metadata (source, type, etc.)."}
                        },
                        "required": ["content"]
                    }
                }
            },
             {
                "type": "function",
                "function": {
                    "name": "provide_retrieval_feedback_tool",
                    "description": "Provide feedback on the relevance of retrieved memories.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "memory_id": {"type": "string", "description": "The ID of the memory being rated."},
                             "similarity_score": {"type": "number", "description": "The similarity score assigned during retrieval."},
                            "was_relevant": {"type": "boolean", "description": "True if the memory was relevant, False otherwise."}
                        },
                        "required": ["memory_id", "similarity_score", "was_relevant"]
                    }
                }
            },
             {
                "type": "function",
                "function": {
                    "name": "detect_contradictions_tool",
                    "description": "Check for potential contradictions within recent memory.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                             "threshold": {"type": "number", "description": "Similarity threshold for contradiction.", "default": 0.75}
                        }
                    }
                }
            }
            # TODO: Add tools for assemblies, emotional state, etc.
        ]

    async def handle_tool_call(self, tool_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Handle a tool call from an external agent (e.g., LLM)."""
        logger.info("SynthiansMemoryCore", f"Handling tool call: {tool_name}", {"args": args})
        try:
            if tool_name == "retrieve_memories_tool":
                 query = args.get("query")
                 top_k = args.get("top_k", 5)
                 # Retrieve memories method handles embedding generation
                 response_data = await self.retrieve_memories(query=query, top_k=top_k)
                 # Return simplified dicts for LLM
                 if response_data["success"]:
                      return {"memories": [{"id": m.get("id"), "content": m.get("content"), "score": m.get("final_score", m.get("relevance_score", m.get("similarity"))) } for m in response_data["memories"]]}
                 else:
                      return {"success": False, "error": response_data.get("error", "Retrieval failed")}

            elif tool_name == "process_new_memory_tool":
                 content = args.get("content")
                 metadata = args.get("metadata")
                 # Embedding generation happens in process_new_memory if needed
                 entry = await self.process_new_memory(content=content, metadata=metadata)
                 return {"success": entry is not None, "memory_id": entry.id if entry else None}

            elif tool_name == "provide_retrieval_feedback_tool":
                 memory_id = args.get("memory_id")
                 similarity_score = args.get("similarity_score")
                 was_relevant = args.get("was_relevant")
                 if self.threshold_calibrator:
                      await self.provide_feedback(memory_id, similarity_score, was_relevant)
                      return {"success": True, "message": "Feedback recorded."}
                 else:
                      return {"success": False, "error": "Adaptive thresholding not enabled."}

            elif tool_name == "detect_contradictions_tool":
                 threshold = args.get("threshold", 0.75)
                 contradictions = await self.detect_contradictions(threshold)
                 return {"success": True, "contradictions_found": len(contradictions), "contradictions": contradictions}

            else:
                 logger.warning("SynthiansMemoryCore", f"Unknown tool called: {tool_name}")
                 return {"success": False, "error": f"Unknown tool: {tool_name}"}

        except Exception as e:
            logger.error("SynthiansMemoryCore", f"Error handling tool call {tool_name}", {"error": str(e)})
            return {"success": False, "error": str(e)}

    # --- Helper & Placeholder Methods ---

    def get_memory_by_id(self, memory_id: str) -> Optional[MemoryEntry]:
        """Retrieve a specific memory entry by its ID from the cache.
           NOTE: This is synchronous and operates on the current in-memory cache.
           It does NOT acquire the async lock. Caller must manage concurrency.

        Args:
            memory_id: The unique identifier of the memory to retrieve

        Returns:
            The MemoryEntry if found in cache, None otherwise
        """
        # No lock needed here - caller (e.g., update_memory) holds the lock
        memory = self._memories.get(memory_id)
        if memory:
            logger.debug("SynthiansMemoryCore", f"Retrieved memory {memory_id} directly from cache (sync).")
        else:
            logger.warning("SynthiansMemoryCore", f"Memory {memory_id} not found in cache (sync).")
        return memory

    async def get_memory_by_id_async(self, memory_id: str) -> Optional[MemoryEntry]:
        """Asynchronously retrieve a specific memory entry by its ID, loading from disk if needed.
        
        Unlike the synchronous get_memory_by_id which only checks the cache, this method
        will attempt to load the memory from disk if it's not found in the cache but exists
        in the index.
        
        Args:
            memory_id: The unique identifier of the memory to retrieve
            
        Returns:
            The MemoryEntry if found in cache or successfully loaded, None otherwise
        """
        async with self._lock:
            # First check if it's already in the memory cache
            memory = self._memories.get(memory_id)
            if memory:
                logger.debug("SynthiansMemoryCore", f"Retrieved memory {memory_id} from cache.")
                memory.access_count += 1
                memory.last_access_time = datetime.now(timezone.utc) # Convert to datetime
                return memory
                
            # Not in cache, check if it's in the index and try to load it
            if memory_id in self.persistence.memory_index:
                logger.debug("SynthiansMemoryCore", f"Memory {memory_id} not in cache, loading from persistence...")
                memory = await self.persistence.load_memory(memory_id)
                if memory:
                    # Add to cache
                    self._memories[memory_id] = memory
                    memory.access_count += 1
                    memory.last_access_time = datetime.now(timezone.utc) # Convert to datetime
                    
                    # If this is our first time seeing this memory and we have a vector index,
                    # add it to the index if it has a valid embedding
                    if memory.embedding is not None and self.vector_index is not None:
                        self.vector_index.add(memory_id, memory.embedding)
                        logger.debug("SynthiansMemoryCore", f"Added memory {memory_id} to vector index on first load.")
                    
                    logger.debug("SynthiansMemoryCore", f"Successfully loaded memory {memory_id} from persistence.")
                    return memory
                else:
                    logger.warning("SynthiansMemoryCore", f"Failed to load memory {memory_id} from persistence despite being in the index.")
                    return None
            else:
                logger.warning("SynthiansMemoryCore", f"Memory {memory_id} not found in cache or index.")
                return None

    async def update_memory(self, memory_id: str, updates: Dict[str, Any]) -> bool:
        """
        Update a memory entry with provided updates.
        Performs a deep merge for metadata updates.

        Args:
            memory_id: ID of the memory to update
            updates: Dictionary of fields to update and their new values

        Returns:
            True if the update succeeded, False otherwise
        """
        try:
            logger.debug(f"Updating memory {memory_id} - acquiring lock")

            # Step 1: Get and update the memory while holding the lock
            try:
                async with asyncio.timeout(5):  # 5 second timeout
                    async with self._lock: # Use the actual async lock
                        logger.debug(f"Lock acquired for memory {memory_id}")
                        # Get the memory (use synchronous version since we already hold the lock)
                        memory = self.get_memory_by_id(memory_id)
                        if not memory:
                            logger.warning(f"Cannot update memory {memory_id}: Not found")
                            return False

                        # Store metadata update separately to apply after all direct attributes
                        metadata_to_update = None
                        score_updated = False

                        # Update the memory fields
                        for key, value in updates.items():
                            if key == "metadata" and isinstance(value, dict):
                                # Store metadata updates to apply them after direct attribute updates
                                metadata_to_update = value
                                continue # Process metadata last

                            if key == "quickrecal_score":
                                try:
                                    new_score_val = float(value)
                                    new_score_val = max(0.0, min(1.0, new_score_val))
                                    if abs(memory.quickrecal_score - new_score_val) > 1e-6:
                                         memory.quickrecal_score = new_score_val
                                         score_updated = True # Mark score as updated
                                except (ValueError, TypeError):
                                    logger.warning("SynthiansMemoryCore", f"Invalid quickrecal_score value: {value}")
                                    continue
                            elif hasattr(memory, key):
                                 setattr(memory, key, value) # Update other direct attributes
                            else:
                                logger.warning(f"Unknown/invalid field '{key}' in memory update")

                        # Apply metadata updates after other fields have been processed
                        if metadata_to_update:
                            if memory.metadata is None:
                                memory.metadata = {}
                            # Use deep update to properly handle nested dictionaries
                            deep_update(memory.metadata, metadata_to_update)

                        # Update quickrecal timestamp ONLY if the score actually changed in THIS update call
                        if score_updated:
                            if memory.metadata is None: memory.metadata = {}
                            memory.metadata['quickrecal_updated_at'] = datetime.now(timezone.utc).isoformat()
                            logger.debug(f"quickrecal_updated_at set for memory {memory_id}")

                        # Mark as dirty for persistence
                        self._dirty_memories.add(memory_id) # Mark for persistence
                        logger.debug(f"Memory {memory_id} updated in memory (marked dirty), releasing lock")
            except asyncio.TimeoutError:
                logger.error(f"Timeout while acquiring or using lock for memory {memory_id}")
                return False

            # The memory is marked as dirty, the persistence loop will handle saving it.
            logger.info(f"Updated memory {memory_id} with {len(updates)} fields (marked dirty for persistence)")
            return True

        except Exception as e:
            logger.error("SynthiansMemoryCore", f"Error updating memory {memory_id}: {str(e)}", exc_info=True)
            return False


    def _filter_by_metadata(self, candidates: List[Dict], metadata_filter: Dict) -> List[Dict]:
        """
        Filter candidates based on metadata key-value pairs.
        
        Args:
            candidates: List of candidate memory dictionaries to filter
            metadata_filter: Dictionary of key-value pairs that must be present in memory metadata
            
        Returns:
            Filtered list of candidates that match all metadata criteria
        """
        if not metadata_filter:
            return candidates
            
        logger.debug(f"[_filter_by_metadata] Filtering {len(candidates)} candidates with filter: {metadata_filter}")
        filtered_results = []
        
        for candidate in candidates:
            metadata = candidate.get("metadata", {})
            # Skip if candidate has no metadata
            if not metadata:
                logger.debug(f"Skipping candidate {candidate.get('id')} - no metadata")
                continue
                
            # Check each filter criterion
            matches_all = True
            for key, value in metadata_filter.items():
                # Support for nested paths with dots (e.g., 'details.source')
                if '.' in key:
                    path_parts = key.split('.')
                    current_obj = metadata
                    # Navigate through the nested structure
                    for part in path_parts[:-1]:
                        if part not in current_obj or not isinstance(current_obj[part], dict):
                            matches_all = False
                            break
                        current_obj = current_obj[part]
                    
                    # Check the final value
                    if matches_all and (path_parts[-1] not in current_obj or current_obj[path_parts[-1]] != value):
                        matches_all = False
                # Simple direct key match        
                elif key not in metadata or metadata[key] != value:
                    matches_all = False
                    break
                    
            if matches_all:
                filtered_results.append(candidate)
                logger.debug(f"Candidate {candidate.get('id')} matched all metadata criteria")
            else:
                logger.debug(f"Candidate {candidate.get('id')} failed metadata criteria")
                
        logger.debug(f"[_filter_by_metadata] Found {len(filtered_results)} candidates matching metadata criteria")
        return filtered_results

    async def generate_embedding(self, text: str) -> Optional[np.ndarray]:
        """Generate embeddings using a consistent method for all text processing."""
        # Use SentenceTransformer directly without importing server.py
        try:
            from sentence_transformers import SentenceTransformer
            # Use the same model name as server.py
            import os
            model_name = os.environ.get("EMBEDDING_MODEL", "all-mpnet-base-v2")
            model = SentenceTransformer(model_name)

            logger.info("SynthiansMemoryCore", f"Using embedding model {model_name}")
            # Run encode in executor to avoid blocking event loop
            loop = asyncio.get_running_loop()
            embedding_list = await loop.run_in_executor(None, lambda: model.encode([text], convert_to_tensor=False))
            if embedding_list is None or len(embedding_list) == 0:
                raise ValueError("Embedding model returned empty result")

            embedding = embedding_list[0]
            return self.geometry_manager._normalize(np.array(embedding, dtype=np.float32))
        except Exception as e:
            logger.error("SynthiansMemoryCore", f"Error generating embedding: {str(e)}")

            # Fallback to a deterministic embedding based on text hash
            import hashlib

            # Create a deterministic embedding based on the hash of the text
            text_bytes = text.encode('utf-8')
            hash_obj = hashlib.md5(text_bytes)
            hash_digest = hash_obj.digest()

            # Convert the 16-byte digest to a list of floats
            byte_values = list(hash_digest) * (self.config['embedding_dim'] // 16 + 1)
            embedding = np.array([float(byte) / 255.0 for byte in byte_values[:self.config['embedding_dim']]], dtype=np.float32)

            logger.warning("SynthiansMemoryCore", "Using deterministic hash-based embedding generation")
            return self.geometry_manager._normalize(embedding)

    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics."""
        # Run get_stats synchronously as it doesn't involve async operations directly
        persistence_stats = self.persistence.get_stats()
        quick_recal_stats = self.quick_recal.get_stats()
        threshold_stats = self.threshold_calibrator.get_statistics() if self.threshold_calibrator else {}
        vector_index_stats = self.vector_index.get_stats() if hasattr(self.vector_index, 'get_stats') else {"count": self.vector_index.count(), "id_mappings": len(self.vector_index.id_to_index)}


        return {
            "core_stats": {
                "total_memories": len(self._memories),
                "total_assemblies": len(self.assemblies),
                "dirty_memories": len(self._dirty_memories),
                "initialized": self._initialized,
            },
            "persistence_stats": persistence_stats,
            "quick_recal_stats": quick_recal_stats,
            "threshold_stats": threshold_stats,
            "vector_index_stats": vector_index_stats
        }

    def process_memory_sync(self, content: str, embedding: Optional[np.ndarray] = None,
                           metadata: Optional[Dict[str, Any]] = None,
                           emotion_data: Optional[Dict[str, Any]] = None) -> Union[Dict[str, Any], None]:
        """
Process a new memory synchronously without using asyncio.run().

This is a synchronous version of process_new_memory that avoids potential asyncio.run() issues.

Args:
    content: The text content of the memory
    embedding: Vector representation of the content (optional)
    metadata: Base metadata for the memory entry (optional)
    emotion_data: Pre-computed emotion analysis results (optional)
        """
        try:
            logger.info("SynthiansMemoryCore", "Processing memory synchronously")

            # Create a new memory entry
            memory_id = f"mem_{uuid.uuid4().hex[:12]}"  # More consistent ID format
            timestamp = metadata.get('timestamp', time.time()) if metadata else time.time()

            # Ensure metadata is a dictionary
            metadata = metadata or {}
            metadata['timestamp'] = timestamp

            # Use provided embedding or generate from content
            if embedding is None:
                logger.warning("SynthiansMemoryCore", "Sync processing requires embedding, using zeros")
                embedding = np.zeros(self.config.get('embedding_dim', 768), dtype=np.float32)

            # Validate/normalize embedding
            validated_embedding = self.geometry_manager._validate_vector(embedding, "Input Embedding")
            if validated_embedding is None:
                 logger.error("SynthiansMemoryCore", "Invalid embedding provided (sync).")
                 return None
            aligned_embedding, _ = self.geometry_manager._align_vectors(validated_embedding, np.zeros(self.config['embedding_dim']))
            normalized_embedding = self.geometry_manager._normalize(aligned_embedding)

            # If emotion_data is not provided but we have an emotion analyzer, try to generate it
            if emotion_data is None and self.emotional_analyzer is not None:
                # Needs a sync version of analyze
                logger.warning("SynthiansMemoryCore", "Sync emotion analysis not implemented")

            # Enhance metadata using the MetadataSynthesizer
            enhanced_metadata = metadata
            if self.metadata_synthesizer is not None:
                try:
                    enhanced_metadata = self.metadata_synthesizer.synthesize_sync(
                        content=content,
                        embedding=normalized_embedding,
                        base_metadata=metadata,
                        emotion_data=emotion_data
                    )
                    logger.info("SynthiansMemoryCore", f"Enhanced metadata for memory {memory_id} (sync)")
                except Exception as e:
                    logger.error("SynthiansMemoryCore", f"Error enhancing metadata (sync): {str(e)}")

            # Calculate QuickRecal score
            quickrecal_score = 0.5  # Default value
            if self.quick_recal is not None:
                try:
                    context = {'text': content, 'timestamp': timestamp}
                    if enhanced_metadata: context.update(enhanced_metadata)
                    quickrecal_score = self.quick_recal.calculate_sync(normalized_embedding, context=context)
                    logger.info("SynthiansMemoryCore", f"Calculated QuickRecal score (sync): {quickrecal_score}")
                except Exception as e:
                    logger.error("SynthiansMemoryCore", f"Error calculating QuickRecal score (sync): {str(e)}")

            # Create memory object (using MemoryEntry directly)
            memory_entry_obj = MemoryEntry(
                id=memory_id,
                content=content,
                embedding=normalized_embedding,
                metadata=enhanced_metadata,
                quickrecal_score=quickrecal_score,
                timestamp=datetime.fromtimestamp(timestamp, timezone.utc) # Convert to datetime
            )

            # Add memory ID to metadata for easier access
            memory_entry_obj.metadata["uuid"] = memory_entry_obj.id

            # Store memory directly
            self._memories[memory_id] = memory_entry_obj
            self._dirty_memories.add(memory_id) # Mark as dirty for next persistence cycle
            logger.info("SynthiansMemoryCore", f"Memory {memory_id} stored in memory (sync)")

            # Persistence is handled by the background loop

            # Return a dictionary representation
            return memory_entry_obj.to_dict()
        except Exception as e:
            logger.error("SynthiansMemoryCore", f"Error processing memory synchronously: {str(e)}")
            return None

    async def check_index_integrity(self) -> Dict[str, Any]:
        """Check the integrity of the vector index and return diagnostic information.
        
        This method checks if the FAISS index and ID-to-index mapping are consistent.
        
        Returns:
            Dict with diagnostic information about the index integrity
        """
        if not self._initialized: await self.initialize()
        
        async with self._lock: # We need the lock to ensure thread safety
            is_consistent, diagnostics = self.vector_index.verify_index_integrity()
            
            return {
                "success": True,
                "is_consistent": is_consistent,
                "diagnostics": diagnostics,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    async def repair_index(self, repair_type: str = "auto") -> Dict[str, Any]:
        """Attempt to repair integrity issues with the vector index.
        
        Args:
            repair_type: The type of repair to perform.
                - "auto": Automatically determine the best repair strategy
                - "recreate_mapping": Recreate the ID-to-index mapping from scratch
                - "rebuild": Completely rebuild the index (not fully implemented)
                
        Returns:
            Dict with repair status and diagnostics
        """
        if not self._initialized: await self.initialize()
        
        async with self._lock:
            logger.info("SynthiansMemoryCore", f"Starting index repair of type: {repair_type}")
            
            # Check initial integrity state
            is_consistent_before, diagnostics_before = self.vector_index.verify_index_integrity()
            
            # If already consistent and not a forced rebuild, we can consider this a success
            if is_consistent_before and repair_type != "rebuild":
                logger.info("SynthiansMemoryCore", "Index is already consistent, no repair needed.")
                return {
                    "success": True,
                    "message": "Index is already consistent, no repair needed.",
                    "diagnostics_before": diagnostics_before,
                    "diagnostics_after": diagnostics_before,
                    "is_consistent": True
                }
            
            # Check current implementation and migrate if needed
            is_index_id_map = hasattr(self.vector_index.index, 'id_map')
            if not is_index_id_map:
                logger.info("Migrating vector index to use IndexIDMap for improved ID management")
                success = self.vector_index.migrate_to_idmap()
                if success:
                    logger.info("Successfully migrated vector index to IndexIDMap")
                else:
                    logger.warning("Failed to migrate vector index to IndexIDMap. Some features may not work correctly.")
            else:
                logger.info("Vector index is already using IndexIDMap")
            
            # Determine repair strategy
            if repair_type == "auto":
                # Choose the best repair strategy based on diagnostics
                faiss_count = self.vector_index.count()
                id_mapping_count = len(self.vector_index.id_to_index)
                
                if id_mapping_count == 0 and faiss_count > 0:
                    repair_type = "recreate_mapping"
                    logger.info("SynthiansMemoryCore", "Auto-selected 'recreate_mapping' repair strategy")
                elif id_mapping_count > faiss_count:
                    # Prune excess mappings
                    repair_type = "recreate_mapping"
                    logger.info("SynthiansMemoryCore", "Auto-selected 'recreate_mapping' to handle excess mappings")
                else:
                    # In other cases, we don't have a good automated solution yet
                    repair_type = "recreate_mapping"  # Default to recreate_mapping for now
                    logger.warning("SynthiansMemoryCore", "No optimal repair strategy determined, defaulting to 'recreate_mapping'")
            
            # Execute repair
            if repair_type == "recreate_mapping":
                success = self.vector_index.recreate_mapping()
            elif repair_type == "rebuild":
                logger.warning("SynthiansMemoryCore", "Full rebuild requires original embeddings which aren't stored. Falling back to recreate_mapping.")
                success = self.vector_index.recreate_mapping()
            else:
                logger.error("SynthiansMemoryCore", f"Unsupported repair_type: {repair_type}")
                success = False
            
            # Check integrity after repair
            is_consistent_after, diagnostics_after = self.vector_index.verify_index_integrity()
            
            # Determine overall success: either repair succeeded or the index is now consistent
            overall_success = success or is_consistent_after
            
            if overall_success:
                logger.info("SynthiansMemoryCore", f"Index repair of type '{repair_type}' completed successfully. Consistency: {is_consistent_after}")
            else:
                logger.error("SynthiansMemoryCore", f"Index repair of type '{repair_type}' failed. Consistency: {is_consistent_after}")
                
            return {
                "success": overall_success,
                "repair_type": repair_type,
                "diagnostics_before": diagnostics_before,
                "diagnostics_after": diagnostics_after,
                "is_consistent": is_consistent_after
            }
```

# synthians_trainer_server\__init__.py

```py

```

# synthians_trainer_server\http_server.py

```py
# synthians_trainer_server/http_server.py

import os
import tensorflow as tf
import numpy as np
import aiohttp
import asyncio
import json
from fastapi import FastAPI, HTTPException, Body, Request, status, Response
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Tuple, Literal
import logging
import traceback # Import traceback
import datetime  # Add datetime module for timestamps
import inspect
# Import the new Neural Memory module and config
from .neural_memory import NeuralMemoryModule, NeuralMemoryConfig

# Import the new MetricsStore for cognitive flow instrumentation
from .metrics_store import MetricsStore, get_metrics_store

# Keep SurpriseDetector if needed for outer loop analysis
from .surprise_detector import SurpriseDetector
# Assume GeometryManager might be needed if surprise calculation uses it
try:
    from ..geometry_manager import GeometryManager
except ImportError:
    logger.warning("Could not import GeometryManager from synthians_memory_core. Using basic numpy ops.")
    class GeometryManager: # Dummy version
        def __init__(self, config=None): pass
        def normalize_embedding(self, vec):
            vec = np.array(vec, dtype=np.float32)
            norm = np.linalg.norm(vec)
            return vec / norm if norm > 0 else vec
        def calculate_similarity(self, v1, v2):
             v1 = self.normalize_embedding(v1)
             v2 = self.normalize_embedding(v2)
             return np.dot(v1, v2)
        def align_vectors(self, v1, v2):
             v1, v2 = np.array(v1), np.array(v2)
             if v1.shape == v2.shape: return v1, v2
             logger.warning("Dummy GeometryManager cannot align vectors.")
             return v1, v2 # Assume they match or fail later


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Synthians Neural Memory API (Titans)")

# --- Global State ---
neural_memory: Optional[NeuralMemoryModule] = None
surprise_detector: Optional[SurpriseDetector] = None
geometry_manager: Optional[GeometryManager] = None
memory_core_url: Optional[str] = None # URL for potential outer loop callbacks

# --- Pydantic Models ---

class InitRequest(BaseModel):
    config: Optional[dict] = Field(default_factory=dict, description="Neural Memory config overrides")
    memory_core_url: Optional[str] = None
    load_path: Optional[str] = None

class InitResponse(BaseModel):
    message: str
    config: dict # Return as dict for JSON

class RetrieveRequest(BaseModel):
    input_embedding: List[float]

class RetrieveResponse(BaseModel):
    retrieved_embedding: List[float]
    query_projection: Optional[List[float]] = None

class UpdateMemoryRequest(BaseModel):
    input_embedding: List[float]
    # Add external projections and gates for MAG/MAL variants
    external_key_projection: Optional[List[float]] = None
    external_value_projection: Optional[List[float]] = None
    external_alpha_gate: Optional[float] = None
    external_theta_gate: Optional[float] = None
    external_eta_gate: Optional[float] = None

class UpdateMemoryResponse(BaseModel):
    status: str
    loss: Optional[float] = None
    grad_norm: Optional[float] = None
    key_projection: Optional[List[float]] = None
    value_projection: Optional[List[float]] = None
    # Add applied gates to response for debugging
    applied_alpha: Optional[float] = None
    applied_theta: Optional[float] = None
    applied_eta: Optional[float] = None

class TrainOuterRequest(BaseModel):
    input_sequence: List[List[float]]
    target_sequence: List[List[float]]

class TrainOuterResponse(BaseModel):
    average_loss: float

class SaveLoadRequest(BaseModel):
    path: str

class StatusResponse(BaseModel):
     status: str
     config: Optional[dict] = None # Return as dict

class AnalyzeSurpriseRequest(BaseModel):
    predicted_embedding: List[float]
    actual_embedding: List[float]

class GetProjectionsRequest(BaseModel):
    input_embedding: List[float] = Field(..., description="The raw input embedding vector")
    embedding_model: str = Field(default="unknown", example="sentence-transformers/all-mpnet-base-v2")
    projection_adapter: Optional[str] = Field(default="identity")

class GetProjectionsResponse(BaseModel):
    input_embedding_norm: float
    projection_adapter_used: str
    key_projection: List[float]
    value_projection: List[float]
    query_projection: List[float]
    projection_metadata: dict

class CalculateGatesRequest(BaseModel):
    attention_output: List[float] = Field(..., description="Output from the attention mechanism")
    current_alpha: Optional[float] = None
    current_theta: Optional[float] = None
    current_eta: Optional[float] = None

class CalculateGatesResponse(BaseModel):
    alpha: float
    theta: float
    eta: float
    metadata: dict = Field(default_factory=dict)

class ConfigRequest(BaseModel):
    variant: Optional[str] = Field(None, description="Titans variant to use (MAC, MAG, MAL)")

class ConfigResponse(BaseModel):
    neural_memory_config: dict
    attention_config: Optional[dict] = None
    titans_variant: str
    supports_external_gates: bool
    supports_external_projections: bool

class ClusterHotspot(BaseModel):
    cluster_id: str
    updates: int

class DiagnoseEmoLoopResponse(BaseModel):
    diagnostic_window: str
    avg_loss: float
    avg_grad_norm: float
    avg_quickrecal_boost: float
    dominant_emotions_boosted: List[str]
    emotional_entropy: float
    emotion_bias_index: float
    user_emotion_match_rate: float
    cluster_update_hotspots: List[ClusterHotspot]
    alerts: List[str]
    recommendations: List[str]

# --- Helper Functions ---

def get_neural_memory() -> NeuralMemoryModule:
    if neural_memory is None:
        logger.error("Neural Memory module not initialized. Call /init first.")
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                            detail="Neural Memory module not initialized.")
    return neural_memory

def get_surprise_detector() -> SurpriseDetector:
     global surprise_detector, geometry_manager
     if surprise_detector is None:
          if geometry_manager is None:
               nm_conf = neural_memory.config if neural_memory else NeuralMemoryConfig()
               # Use get with default for safety
               gm_dim = nm_conf.get('input_dim', 768)
               geometry_manager = GeometryManager({'embedding_dim': gm_dim})
          surprise_detector = SurpriseDetector(geometry_manager=geometry_manager)
          logger.info("Initialized SurpriseDetector.")
     return surprise_detector


def _validate_vector(vec: Optional[List[float]], expected_dim: int, name: str, allow_none=False):
    """Validates vector type, length, and content."""
    if vec is None:
        if allow_none: return
        else: raise HTTPException(status_code=400, detail=f"'{name}' cannot be null.")

    if not isinstance(vec, list):
         raise HTTPException(status_code=400, detail=f"'{name}' must be a list of floats.")

    # <<< MODIFIED: Explicitly handle expected_dim == -1 >>>
    if expected_dim != -1 and len(vec) != expected_dim:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid vector length for '{name}'. Expected {expected_dim}, got {len(vec)}.")
    # Add NaN/Inf check
    try:
         # Using np.isfinite is more efficient for checking both NaN and Inf
         if not np.all(np.isfinite(vec)):
             raise HTTPException(
                  status_code=400,
                  detail=f"Invalid values (NaN/Inf) found in '{name}'.")
    except TypeError:
          # This might happen if vec contains non-numeric types
          raise HTTPException(
               status_code=400,
               detail=f"Invalid value types in '{name}', expected floats.")


# --- API Endpoints ---

@app.post("/init", response_model=InitResponse, status_code=status.HTTP_200_OK)
async def init_neural_memory(req: InitRequest):
    """Initialize the Neural Memory Module."""
    global neural_memory, memory_core_url, surprise_detector, geometry_manager
    logger.info(f"Received /init request. Config overrides: {req.config}, Load path: {req.load_path}")
    try:
        # Use .get() for safer access to potentially missing keys in Pydantic model
        mc_url = req.memory_core_url
        if mc_url:
            memory_core_url = mc_url
            logger.info(f"Memory Core URL set to: {memory_core_url}")

        # Create config, overriding defaults with request body config
        # req.config should be a dict here from Pydantic parsing
        config_data = req.config if req.config is not None else {}
        config = NeuralMemoryConfig(**config_data)
        logger.info(f"Parsed config: {dict(config)}")


        # Initialize or re-initialize
        logger.info("Creating NeuralMemoryModule instance...")
        neural_memory = NeuralMemoryModule(config=config)
        logger.info("NeuralMemoryModule instance created.")

        # Initialize shared geometry manager and surprise detector based on module's config
        # Use dictionary access here too
        geometry_manager = GeometryManager({'embedding_dim': neural_memory.config['input_dim']})
        # Reset surprise detector to use new geometry manager if re-initializing
        surprise_detector = None
        get_surprise_detector() # Initialize if not already

        loaded_ok = True
        if req.load_path:
            logger.info(f"Attempting to load state from: {req.load_path}")
            # Build model before loading
            try:
                 logger.info("Building model before loading state...")
                 _ = neural_memory(tf.zeros((1, neural_memory.config['query_dim'])))
                 logger.info("Model built successfully.")
            except Exception as build_err:
                 logger.error(f"Error explicitly building model before load: {build_err}. Load might still succeed.")

            loaded_ok = neural_memory.load_state(req.load_path)
            if not loaded_ok:
                # Fail init if loading was requested but failed
                raise HTTPException(status_code=500, detail=f"Failed to load state from {req.load_path}")

        effective_config = neural_memory.get_config_dict()
        logger.info(f"Neural Memory module initialized. Effective Config: {effective_config}")
        return InitResponse(message="Neural Memory module initialized successfully.", config=effective_config)

    except AttributeError as ae:
         # Catch the specific AttributeError related to config access during init
         logger.error(f"AttributeError during initialization: {ae}. Config object: {config}", exc_info=True)
         neural_memory = None
         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                             detail=f"Initialization failed due to config access error: {ae}")
    except Exception as e:
        logger.error(f"Failed to initialize Neural Memory module: {e}", exc_info=True)
        neural_memory = None # Ensure it's None on failure
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                            detail=f"Initialization failed: {str(e)}")

@app.post("/retrieve", response_model=RetrieveResponse)
async def retrieve(req: RetrieveRequest):
    nm = get_neural_memory()
    try:
        _validate_vector(req.input_embedding, nm.config['input_dim'], "input_embedding")
        
        # Create tensor with proper batch dimension as expected by TensorFlow
        input_tensor = tf.convert_to_tensor([req.input_embedding], dtype=tf.float32)
        
        # Get the query projection
        k_t, v_t, q_t = nm.get_projections(input_tensor)
        
        # Log shapes for debugging
        logger.debug(f"DEBUG /retrieve: Shape of input_tensor: {tf.shape(input_tensor).numpy()}, Shape of q_t: {tf.shape(q_t).numpy()}")
        logger.debug(f"DEBUG /retrieve: Config - query_dim={nm.config['query_dim']}, key_dim={nm.config['key_dim']}")
        
        # Pass the QUERY projection to the model, not the raw input tensor
        retrieved_embedding = nm(q_t)
        
        # Convert to Python list for JSON serialization
        retrieved_embedding_list = retrieved_embedding[0].numpy().tolist() if len(tf.shape(retrieved_embedding)) > 1 else retrieved_embedding.numpy().tolist()
        
        # Convert query projection to list for response
        query_projection_list = q_t[0].numpy().tolist() if len(tf.shape(q_t)) > 1 else q_t.numpy().tolist()
        
        return RetrieveResponse(
            retrieved_embedding=retrieved_embedding_list,
            query_projection=query_projection_list
        )
    except HTTPException as http_exc: raise http_exc
    except Exception as e:
        logger.error(f"Retrieve failed: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Retrieve failed: {str(e)}")

@app.post("/update_memory", response_model=UpdateMemoryResponse)
async def update_memory(req: UpdateMemoryRequest):
    nm = get_neural_memory()
    try:
        _validate_vector(req.input_embedding, nm.config['input_dim'], "input_embedding")
        
        # Validate optional external projections if provided
        if req.external_key_projection is not None:
            _validate_vector(req.external_key_projection, nm.config['key_dim'], "external_key_projection")
        if req.external_value_projection is not None:
            _validate_vector(req.external_value_projection, nm.config['value_dim'], "external_value_projection")
        
        # Create tensor with proper batch dimension as expected by TensorFlow
        input_tensor = tf.convert_to_tensor([req.input_embedding], dtype=tf.float32)

        # Prepare external projections if provided (for MAL variant)
        external_k_t = None
        external_v_t = None
        if req.external_key_projection is not None:
            external_k_t = tf.convert_to_tensor([req.external_key_projection], dtype=tf.float32)
        if req.external_value_projection is not None:
            external_v_t = tf.convert_to_tensor([req.external_value_projection], dtype=tf.float32)

        # Get the key and value projections if not provided externally
        if external_k_t is None or external_v_t is None:
            k_t, v_t, _ = nm.get_projections(input_tensor)
            # Use externally provided projections if available
            if external_k_t is not None:
                k_t = external_k_t
            if external_v_t is not None:
                v_t = external_v_t
        else:
            # Both projections provided externally
            k_t, v_t = external_k_t, external_v_t

        # Prepare external gates if provided (for MAG variant)
        external_gates = {}
        if req.external_alpha_gate is not None:
            external_gates["alpha_t"] = req.external_alpha_gate
        if req.external_theta_gate is not None:
            external_gates["theta_t"] = req.external_theta_gate
        if req.external_eta_gate is not None:
            external_gates["eta_t"] = req.external_eta_gate
        
        # Log the gate values we're using
        if any([req.external_alpha_gate, req.external_theta_gate, req.external_eta_gate]):
            logger.info(f"MAG variant: Using external gates - alpha:{req.external_alpha_gate}, theta:{req.external_theta_gate}, eta:{req.external_eta_gate}")
        
        # Call update_step with the correct named parameters
        loss_tensor, grads = nm.update_step(
            x_t=input_tensor,
            external_k_t=k_t,  # Pass the determined key projection
            external_v_t=v_t,  # Pass the determined value projection
            external_alpha_t=req.external_alpha_gate,  # Pass individual gate values
            external_theta_t=req.external_theta_gate,
            external_eta_t=req.external_eta_gate
        )

        # Get the actual gates used (if available from the method)
        applied_gates = {}
        if hasattr(nm, "last_applied_gates") and nm.last_applied_gates:
            applied_gates = nm.last_applied_gates

        grad_norm = 0.0
        if grads:
             valid_grads = [g for g in grads if g is not None]
             if valid_grads:
                 # Calculate L2 norm for each valid gradient tensor and sum them
                 norms = [tf.norm(g) for g in valid_grads]
                 grad_norm = tf.reduce_sum(norms).numpy().item()

        loss_value = loss_tensor.numpy().item() if loss_tensor is not None else 0.0

        # Include timestamp in response for tracking
        timestamp = datetime.datetime.now().isoformat()
        
        # Log metrics to MetricsStore for cognitive flow monitoring
        metrics = get_metrics_store()
        metrics.log_memory_update(
            input_embedding=req.input_embedding,
            loss=loss_value,
            grad_norm=grad_norm,
            # Extract emotion if available in metadata
            emotion=req.metadata.get("emotion") if hasattr(req, "metadata") and req.metadata else None,
            metadata={
                "timestamp": timestamp,
                "input_dim": len(req.input_embedding),
                "external_projections_used": external_k_t is not None or external_v_t is not None,
                "external_gates_used": bool(external_gates)
            }
        )

        # Convert projections to lists for response
        key_projection_list = k_t[0].numpy().tolist() if len(tf.shape(k_t)) > 1 else k_t.numpy().tolist()
        value_projection_list = v_t[0].numpy().tolist() if len(tf.shape(v_t)) > 1 else v_t.numpy().tolist()

        return UpdateMemoryResponse(
            status="success",
            loss=loss_value,
            grad_norm=grad_norm,
            key_projection=key_projection_list,
            value_projection=value_projection_list,
            applied_alpha=applied_gates.get("alpha_t"),
            applied_theta=applied_gates.get("theta_t"),
            applied_eta=applied_gates.get("eta_t")
        )
    except HTTPException as http_exc: raise http_exc
    except Exception as e:
        logger.error(f"Memory update failed: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Update error: {str(e)}")

@app.post("/train_outer", response_model=TrainOuterResponse)
async def train_outer(req: TrainOuterRequest):
    nm = get_neural_memory()
    if not hasattr(nm, 'compiled') or not nm.compiled:
        try:
             # Make sure the optimizer is properly set
             if not hasattr(nm, 'optimizer') or nm.optimizer is None:
                 nm.optimizer = nm.outer_optimizer
             nm.compile(optimizer=nm.optimizer, loss='mse')
             logger.info("NeuralMemoryModule compiled for outer training.")
        except Exception as compile_err:
             logger.error(f"Error compiling NeuralMemoryModule: {compile_err}")
             raise HTTPException(status_code=500, detail=f"Model compilation error: {compile_err}")

    try:
        if not req.input_sequence or not req.target_sequence: raise HTTPException(status_code=400, detail="Sequences empty.")
        seq_len = len(req.input_sequence)
        if seq_len != len(req.target_sequence): raise HTTPException(status_code=400, detail="Sequence lengths mismatch.")
        if seq_len == 0: raise HTTPException(status_code=400, detail="Sequences length 0.")

        # Validate dimensions for first item in sequences
        _validate_vector(req.input_sequence[0], nm.config['input_dim'], "input_sequence[0]")
        _validate_vector(req.target_sequence[0], nm.config['value_dim'], "target_sequence[0]")

        # Convert to tensors with proper shape: [batch_size=1, seq_len, dim]
        input_seq_tensor = tf.convert_to_tensor([req.input_sequence], dtype=tf.float32)
        target_seq_tensor = tf.convert_to_tensor([req.target_sequence], dtype=tf.float32)

        # Log tensor shapes for debugging
        logger.info(f"Input sequence tensor shape: {input_seq_tensor.shape}, Target sequence tensor shape: {target_seq_tensor.shape}")
        
        # Directly call train_step with the properly shaped tensors
        metrics = nm.train_step((input_seq_tensor, target_seq_tensor))
        avg_loss = metrics.get('loss', 0.0)
        
        # Ensure we return a Python native float
        return TrainOuterResponse(average_loss=float(avg_loss))

    except HTTPException as http_exc: raise http_exc
    except tf.errors.InvalidArgumentError as tf_err:
         logger.error(f"TensorFlow argument error during outer training: {tf_err}", exc_info=True)
         raise HTTPException(status_code=400, detail=f"TF Argument Error: {tf_err}")
    except Exception as e:
        logger.error(f"Outer training failed: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Outer training error: {str(e)}")

@app.post("/save", status_code=status.HTTP_200_OK)
async def save_neural_memory_state(req: SaveLoadRequest):
    nm = get_neural_memory()
    try:
        nm.save_state(req.path)
        return {"message": f"Neural Memory state saved to {req.path}"}
    except Exception as e:
        logger.error(f"Failed to save neural memory state: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to save state: {str(e)}")

@app.post("/load", status_code=status.HTTP_200_OK)
async def load_neural_memory_state(req: SaveLoadRequest):
    global neural_memory, surprise_detector, geometry_manager
    try:
        # First, read the state file to examine the config without loading
        if not os.path.exists(req.path):
            raise FileNotFoundError(f"State file not found: {req.path}")
            
        with open(req.path, 'r') as f: 
            state_data = json.load(f)
            
        # Extract config from saved state
        saved_config = state_data.get("config")
        if not saved_config:
            raise ValueError("State file is missing 'config' section")
        
        # Create a properly initialized model with the saved config
        temp_nm = NeuralMemoryModule(config=saved_config)

        # Initialize geometry manager and surprise detector based on config
        geometry_manager = GeometryManager({'embedding_dim': temp_nm.config['input_dim']})
        # Reset surprise detector to use new geometry manager if re-initializing
        surprise_detector = None
        get_surprise_detector() # Initialize if not already

        # Attempt to load state into the fully initialized model with matching config
        loaded_ok = temp_nm.load_state(req.path)

        if loaded_ok:
            # Replace the global instance with our successfully loaded one
            neural_memory = temp_nm
            logger.info(f"Neural Memory state loaded from {req.path} and components re-initialized.")
            return {"message": f"Neural Memory state loaded from {req.path}"}
        else:
             raise HTTPException(status_code=500, detail=f"Failed to load state from {req.path}. Check logs.")

    except FileNotFoundError:
        raise HTTPException(status_code=404, detail=f"State file not found: {req.path}")
    except Exception as e:
        logger.error(f"Failed to load neural memory state: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to load state: {str(e)}")

@app.get("/status", response_model=StatusResponse)
async def get_neural_memory_status():
    if neural_memory is None:
        return StatusResponse(status="Neural Memory module not initialized.")
    try:
        config_dict = neural_memory.get_config_dict()
        return StatusResponse(status="Initialized", config=config_dict)
    except Exception as e:
        logger.error(f"Failed to get status: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")

@app.post("/analyze_surprise", response_model=Dict[str, Any])
async def analyze_surprise(request: AnalyzeSurpriseRequest):
    detector = get_surprise_detector()
    nm = get_neural_memory() # Need this for dimension info
    try:
        # Validate embeddings using input_dim from the initialized model
        _validate_vector(request.predicted_embedding, nm.config['input_dim'], "predicted_embedding")
        _validate_vector(request.actual_embedding, nm.config['input_dim'], "actual_embedding")

        surprise_metrics = detector.calculate_surprise(
            predicted_embedding=request.predicted_embedding,
            actual_embedding=request.actual_embedding
        )
        quickrecal_boost = detector.calculate_quickrecal_boost(surprise_metrics)

        response_data = surprise_metrics.copy()
        if 'delta' in response_data and isinstance(response_data['delta'], np.ndarray):
             response_data['delta'] = response_data['delta'].tolist()
        response_data["quickrecal_boost"] = quickrecal_boost

        return response_data

    except HTTPException as http_exc: raise http_exc
    except Exception as e:
        logger.error(f"Error analyzing surprise: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error analyzing surprise: {str(e)}")

# --- Health Check ---
@app.get("/health", status_code=status.HTTP_200_OK)
async def health_check():
    """Basic health check."""
    logger.info("Health check requested.")
    try:
         tf_version = tf.__version__
         # Perform a minimal TF computation
         tensor_sum = tf.reduce_sum(tf.constant([1.0, 2.0])).numpy()
         can_compute = abs(tensor_sum - 3.0) < 1e-6
         status_msg = "ok" if can_compute else "error_tf_compute"
    except Exception as e:
         logger.error(f"TensorFlow health check failed: {e}", exc_info=True)
         tf_version = "error"
         status_msg = f"error_tf_init: {str(e)}"

    return {
         "status": status_msg,
         "tensorflow_version": tf_version,
         "neural_memory_initialized": neural_memory is not None,
         "timestamp": datetime.datetime.utcnow().isoformat() 
     }

# --- Introspection and Diagnostic Endpoints ---

@app.post("/get_projections", response_model=GetProjectionsResponse, summary="Get K/V/Q Projections")
async def get_projections_endpoint(request: GetProjectionsRequest):
    """Exposes internal K, V, Q projections for a given input embedding."""
    nm = get_neural_memory()
    try:
        _validate_vector(request.input_embedding, nm.config['input_dim'], "input_embedding")
        
        # Convert to tensor format expected by NeuralMemoryModule
        input_tensor = tf.convert_to_tensor([request.input_embedding], dtype=tf.float32)  # Add batch dim
        
        # Get projections (k_t, v_t, q_t tensors)
        k_t, v_t, q_t = nm.get_projections(input_tensor)
        
        # Ensure tensors are squeezed and converted to Python lists
        k_list = tf.squeeze(k_t).numpy().tolist()
        v_list = tf.squeeze(v_t).numpy().tolist()
        q_list = tf.squeeze(q_t).numpy().tolist()
        
        # Calculate input embedding L2 norm
        input_norm = float(np.linalg.norm(np.array(request.input_embedding, dtype=np.float32)))
        
        # Get projection matrix hash (placeholder implementation)
        proj_hash = "hash_placeholder_v1"
        if hasattr(nm, 'get_projection_hash'):
            proj_hash = nm.get_projection_hash()
        else:
            # Basic placeholder hash since the method doesn't exist yet
            # In the future, implement get_projection_hash in NeuralMemoryModule
            logger.warning("get_projection_hash not implemented, using placeholder")
            
        # Prepare the response
        response = GetProjectionsResponse(
            input_embedding_norm=input_norm,
            projection_adapter_used=request.projection_adapter or "identity",
            key_projection=k_list,
            value_projection=v_list,
            query_projection=q_list,
            projection_metadata={
                "dim_key": nm.config['key_dim'],
                "dim_value": nm.config['value_dim'],
                "dim_query": nm.config['query_dim'],
                "projection_matrix_hash": proj_hash,
                "input_dim": nm.config['input_dim'],
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
        )
        return response
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"/get_projections failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error getting projections: {str(e)}")


@app.get("/diagnose_emoloop", response_model=DiagnoseEmoLoopResponse, summary="Diagnose Emotional Feedback Loop Health")
async def diagnose_emoloop(window: str = "last_100", emotion_filter: Optional[str] = "all", format: Optional[str] = None):
    """Returns diagnostic metrics for the surprise->QuickRecal feedback loop.
    
    Args:
        window: Time/count window to analyze ("last_100", "last_hour", "session")
        emotion_filter: Optional emotion to filter by ("all" or specific emotion)
        format: Output format ("json" or "table" for CLI-friendly ASCII table)
    """
    # Log the parameters for future reference
    logger.info(f"Received /diagnose_emoloop request: window={window}, filter={emotion_filter}, format={format}")
    
    # Get metrics from the MetricsStore instead of using placeholder data
    metrics_store = get_metrics_store()
    diagnostics = metrics_store.get_diagnostic_metrics(window=window, emotion_filter=emotion_filter)
    
    # Create response using the real metrics data
    response = DiagnoseEmoLoopResponse(
        diagnostic_window=diagnostics["diagnostic_window"],
        avg_loss=diagnostics["avg_loss"],
        avg_grad_norm=diagnostics["avg_grad_norm"],
        avg_quickrecal_boost=diagnostics["avg_quickrecal_boost"],
        dominant_emotions_boosted=diagnostics["dominant_emotions_boosted"],
        emotional_entropy=diagnostics["emotional_entropy"],
        emotion_bias_index=diagnostics["emotion_bias_index"],
        user_emotion_match_rate=diagnostics["user_emotion_match_rate"],
        cluster_update_hotspots=[ClusterHotspot(**hotspot) for hotspot in diagnostics["cluster_update_hotspots"]],
        alerts=diagnostics["alerts"],
        recommendations=diagnostics["recommendations"]
    )
    
    # Handle table format for CLI-friendly output
    if format == "table":
        return Response(
            content=metrics_store.format_diagnostics_as_table(diagnostics),
            media_type="text/plain"
        )
    
    return response

@app.post("/calculate_gates", response_model=CalculateGatesResponse)
async def calculate_gates(request: CalculateGatesRequest):
    """Calculate gate values (alpha, theta, eta) from attention output for MAG variant.
    
    Args:
        request: The request containing attention output and optional current gate values
    
    Returns:
        CalculateGatesResponse containing the calculated gate values
    """
    nm = get_neural_memory()
    try:
        # Convert attention output to tensor
        attention_output = tf.convert_to_tensor([request.attention_output], dtype=tf.float32)
        
        # Call the calculate_gates method of the Neural Memory Module
        alpha_t, theta_t, eta_t = nm.calculate_gates(attention_output)
        
        # Convert to Python scalars for response
        alpha_value = float(alpha_t.numpy()) if hasattr(alpha_t, 'numpy') else float(alpha_t)
        theta_value = float(theta_t.numpy()) if hasattr(theta_t, 'numpy') else float(theta_t)
        eta_value = float(eta_t.numpy()) if hasattr(eta_t, 'numpy') else float(eta_t)
        
        # Create response with metadata
        return CalculateGatesResponse(
            alpha=alpha_value,
            theta=theta_value,
            eta=eta_value,
            metadata={
                "timestamp": datetime.datetime.now().isoformat(),
                "attention_output_dim": len(request.attention_output),
                "current_alpha": request.current_alpha,
                "current_theta": request.current_theta,
                "current_eta": request.current_eta
            }
        )
    except Exception as e:
        logger.error(f"Calculate gates failed: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Calculate gates error: {str(e)}")

@app.get("/config", response_model=ConfigResponse)
@app.post("/config", response_model=ConfigResponse)
async def get_config(request: Optional[ConfigRequest] = None):
    """Get or update the Neural Memory configuration, including Titans variant support.
    
    Args:
        request: Optional request to update the Titans variant
    
    Returns:
        ConfigResponse containing the current configuration
    """
    nm = get_neural_memory()
    try:
        # Update variant if requested
        if request and request.variant:
            # Validate variant
            valid_variants = ["MAC", "MAG", "MAL"]
            if request.variant.upper() not in valid_variants:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Invalid Titans variant '{request.variant}'. Must be one of {valid_variants}"
                )
            
            # Set environment variable for variant
            os.environ["TITANS_VARIANT"] = request.variant.upper()
            logger.info(f"Updated TITANS_VARIANT to {request.variant.upper()}")
        
        # Get current variant from environment or default to MAC
        current_variant = os.environ.get("TITANS_VARIANT", "MAC").upper()
        
        # Dynamically determine capabilities based on implemented method signatures
        # Check if update_step supports external gates and projections using inspect
        update_step_sig = inspect.signature(nm.update_step)
        supports_external_gates = any(param in update_step_sig.parameters 
                                   for param in ["external_alpha_t", "external_theta_t", "external_eta_t"])
        supports_external_projections = any(param in update_step_sig.parameters 
                                        for param in ["external_k_t", "external_v_t"])
        
        logger.info(f"Detected capabilities: supports_external_gates={supports_external_gates}, "
                   f"supports_external_projections={supports_external_projections}")
        
        # Get neural memory config
        neural_memory_config = nm.get_config_dict()
        
        # Get attention config if available
        attention_config = None
        if hasattr(nm, "attention_config"):
            attention_config = nm.attention_config
        
        return ConfigResponse(
            neural_memory_config=neural_memory_config,
            attention_config=attention_config,
            titans_variant=current_variant,
            supports_external_gates=supports_external_gates,
            supports_external_projections=supports_external_projections
        )
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(f"Config endpoint failed: {e}\n{traceback.format_exc()}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Config error: {str(e)}")

# --- App startup/shutdown ---
@app.on_event("startup")
async def startup_event():
    global neural_memory, memory_core_url, surprise_detector, geometry_manager
    logger.info("Synthians Neural Memory API starting up...")

    # --- ADD AUTO-INITIALIZATION LOGIC ---
    try:
        logger.info("Attempting auto-initialization of Neural Memory module...")
        # Use environment variables for default config or load path if needed
        default_config_dict = {
            # Set input_dim to match Memory Core's embedding dimension (768)
            'input_dim': 768,
            # Key and query dimensions should match for proper attention computation
            'key_dim': 128,
            'query_dim': 128,  
            'value_dim': 768,  
            'hidden_dim': 512   
        }
        load_path = os.environ.get("NM_DEFAULT_STATE_PATH", None)
        mc_url = os.environ.get("MEMORY_CORE_URL", "http://localhost:5010") 

        # Create default config
        config = NeuralMemoryConfig(**default_config_dict)

        # Create the module instance
        neural_memory = NeuralMemoryModule(config=config)

        # Initialize geometry manager and surprise detector based on config
        geometry_manager = GeometryManager({'embedding_dim': neural_memory.config['input_dim']})
        # Reset surprise detector to use new geometry manager if re-initializing
        surprise_detector = None
        get_surprise_detector() 

        # Attempt to load state if path specified
        if load_path:
            logger.info(f"Attempting to load default state from: {load_path}")
            # Build model before loading
            try:
                logger.info("Building model before loading state...")
                _ = neural_memory(tf.zeros((1, neural_memory.config['query_dim'])))
                logger.info("Model built successfully for auto-load.")
            except Exception as build_err:
                logger.error(f"Error building model during auto-load: {build_err}")
            loaded = neural_memory.load_state(load_path)
            if loaded:
                logger.info(f"Successfully auto-loaded state from {load_path}")
            else:
                logger.warning(f"Failed to auto-load state from {load_path}. Starting with fresh state.")

        # Set Memory Core URL if available
        if mc_url:
            memory_core_url = mc_url

        logger.info("Neural Memory module auto-initialized successfully on startup.")
        logger.info(f"Effective Config: {neural_memory.get_config_dict()}")

    except Exception as e:
        logger.error(f"CRITICAL: Auto-initialization of Neural Memory failed: {e}", exc_info=True)
        # Ensure neural_memory is None if init fails
        neural_memory = None
    # --- END AUTO-INITIALIZATION LOGIC ---

    # Original message still useful as a fallback indication
    logger.info("Synthians Neural Memory API started. Send POST to /init to reinitialize if needed.")


@app.on_event("shutdown")
async def shutdown():
    logger.info("Shutting down neural memory server.")
    # if neural_memory:
    #     try:
    #         save_path = os.environ.get("SHUTDOWN_SAVE_PATH", "/app/memory/shutdown_state.json")
    #         logger.info(f"Attempting final state save to {save_path}")
    #         neural_memory.save_state(save_path)
    #     except Exception as e:
    #         logger.error(f"Error saving state on shutdown: {e}")


# --- Main Execution ---
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8001))
    host = os.environ.get("HOST", "0.0.0.0")
    log_level = os.environ.get("LOG_LEVEL", "info").lower()

    logger.info(f"Starting Synthians Neural Memory API on http://{host}:{port}")
    print(f"-> Using TensorFlow version: {tf.__version__}")
    print(f"-> Using NumPy version: {np.__version__}")
    if not np.__version__.startswith("1."):
        print("\n\n!!!! WARNING: Numpy version is not < 2.0.0. This may cause issues with TensorFlow/other libs. !!!!\n\n")

    uvicorn.run(app, host=host, port=port, log_level=log_level) 
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328125215_17601e61940.json

```json
{
  "trace_id": "intent_20250328125215_17601e61940",
  "timestamp": "2025-03-28T12:52:15.763028",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: 500: Memory processing failed: 'GeometryManager' object has no attribute '_align_vectors'",
    "confidence": 0.0,
    "timestamp": "2025-03-28T12:52:23.473811"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328130303_2137fb81610.json

```json
{
  "trace_id": "intent_20250328130303_2137fb81610",
  "timestamp": "2025-03-28T13:03:03.308840",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: 500: Memory processing failed: 'GeometryManager' object has no attribute '_normalize'",
    "confidence": 0.0,
    "timestamp": "2025-03-28T13:03:07.602136"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328130801_1bc7ba55940.json

```json
{
  "trace_id": "intent_20250328130801_1bc7ba55940",
  "timestamp": "2025-03-28T13:08:01.317040",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: 500: Memory processing failed: 'GeometryManager' object has no attribute '_normalize'",
    "confidence": 0.0,
    "timestamp": "2025-03-28T13:08:05.429991"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328130929_214e62c1cd0.json

```json
{
  "trace_id": "intent_20250328130929_214e62c1cd0",
  "timestamp": "2025-03-28T13:09:29.344468",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: 500: Memory processing failed: 'GeometryManager' object has no attribute '_normalize'",
    "confidence": 0.0,
    "timestamp": "2025-03-28T13:09:29.495253"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328131045_1a8719f5dc0.json

```json
{
  "trace_id": "intent_20250328131045_1a8719f5dc0",
  "timestamp": "2025-03-28T13:10:45.822208",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_42a6db77856e",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:10:49.703839"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328131509_19afbcc5a00.json

```json
{
  "trace_id": "intent_20250328131509_19afbcc5a00",
  "timestamp": "2025-03-28T13:15:09.695051",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_efb78605e912",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:15:24.462168"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328131642_15e6866d190.json

```json
{
  "trace_id": "intent_20250328131642_15e6866d190",
  "timestamp": "2025-03-28T13:16:42.460427",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_b7adf251707e",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:16:42.750453"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328132121_2297f0f5520.json

```json
{
  "trace_id": "intent_20250328132121_2297f0f5520",
  "timestamp": "2025-03-28T13:21:21.593474",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6196844577789307,
    "grad_norm": 3.0371012687683105,
    "timestamp": "2025-03-28T13:21:22.194439"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6197, grad_norm=3.0371)",
    "\u2192 Boosted memory mem_e448cc7dedf9 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_e448cc7dedf9",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:21:22.226802"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328132916_2bfdae74a70.json

```json
{
  "trace_id": "intent_20250328132916_2bfdae74a70",
  "timestamp": "2025-03-28T13:29:16.447132",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.7034170627593994,
    "grad_norm": 3.310447931289673,
    "timestamp": "2025-03-28T13:29:21.671043"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.7034, grad_norm=3.3104)",
    "\u2192 Boosted memory mem_53e1646c988f QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_53e1646c988f",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:29:21.703684"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328133258_1ab6fe56120.json

```json
{
  "trace_id": "intent_20250328133258_1ab6fe56120",
  "timestamp": "2025-03-28T13:32:58.082169",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6607450842857361,
    "grad_norm": 3.1318135261535645,
    "timestamp": "2025-03-28T13:33:02.752793"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6607, grad_norm=3.1318)",
    "\u2192 Boosted memory mem_b0ba34039c02 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_b0ba34039c02",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:33:02.790035"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328133554_1887e929fd0.json

```json
{
  "trace_id": "intent_20250328133554_1887e929fd0",
  "timestamp": "2025-03-28T13:35:54.218105",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6500575542449951,
    "grad_norm": 3.089069366455078,
    "timestamp": "2025-03-28T13:35:54.859924"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6501, grad_norm=3.0891)",
    "\u2192 Boosted memory mem_febc4eb7ec62 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_febc4eb7ec62",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:35:54.893921"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328134128_1877b5caba0.json

```json
{
  "trace_id": "intent_20250328134128_1877b5caba0",
  "timestamp": "2025-03-28T13:41:28.579220",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6635435819625854,
    "grad_norm": 3.1921162605285645,
    "timestamp": "2025-03-28T13:41:33.463407"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6635, grad_norm=3.1921)",
    "\u2192 Boosted memory mem_93c8e1a3c865 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_93c8e1a3c865",
    "confidence": 0.5,
    "timestamp": "2025-03-28T13:41:33.503477"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328134317_26a809cfbf0.json

```json
{
  "trace_id": "intent_20250328134317_26a809cfbf0",
  "timestamp": "2025-03-28T13:43:17.410769",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_28110eb9a3ec_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6244530081748962,
    "grad_norm": 2.9935226440429688,
    "timestamp": "2025-03-28T13:43:18.015688"
  },
  "emotional_modulation": {
    "user_emotion": "curiosity"
  },
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6245, grad_norm=2.9935)",
    "\u2192 Boosted memory mem_28110eb9a3ec QuickRecal by 0.2000 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_28110eb9a3ec",
    "confidence": 1.0,
    "timestamp": "2025-03-28T13:43:18.062117"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328162322_7f548d91e4a0.json

```json
{
  "trace_id": "intent_20250328162322_7f548d91e4a0",
  "timestamp": "2025-03-28T16:23:22.649632",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Connection refused or failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T16:23:22.656156"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328163003_7f6762ad1f60.json

```json
{
  "trace_id": "intent_20250328163003_7f6762ad1f60",
  "timestamp": "2025-03-28T16:30:03.491482",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Connection refused or failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T16:30:07.493351"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328163119_7fa1be0caf20.json

```json
{
  "trace_id": "intent_20250328163119_7fa1be0caf20",
  "timestamp": "2025-03-28T16:31:19.562211",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Connection refused or failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T16:31:23.526575"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328163233_7ff4735cdf30.json

```json
{
  "trace_id": "intent_20250328163233_7ff4735cdf30",
  "timestamp": "2025-03-28T16:32:33.240116",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.6416096091270447,
    "grad_norm": 3.140026569366455,
    "timestamp": "2025-03-28T16:32:40.852472"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.6416, grad_norm=3.1400)",
    "\u2192 Boosted memory mem_00790152fab6 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_00790152fab6",
    "confidence": 0.5,
    "timestamp": "2025-03-28T16:32:40.894895"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328163322_7efcb58d2020.json

```json
{
  "trace_id": "intent_20250328163322_7efcb58d2020",
  "timestamp": "2025-03-28T16:33:22.241265",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.2
  },
  "neural_memory_trace": {
    "loss": 0.5404383540153503,
    "grad_norm": 2.4210591316223145,
    "timestamp": "2025-03-28T16:33:22.389749"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.5404, grad_norm=2.4211)",
    "\u2192 Boosted memory mem_715e0719f653 QuickRecal by 0.2000 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_715e0719f653",
    "confidence": 0.5,
    "timestamp": "2025-03-28T16:33:22.424870"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328163347_7fc19decdf30.json

```json
{
  "trace_id": "intent_20250328163347_7fc19decdf30",
  "timestamp": "2025-03-28T16:33:47.524171",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.19324893951416017
  },
  "neural_memory_trace": {
    "loss": 0.4021265506744385,
    "grad_norm": 1.9324893951416016,
    "timestamp": "2025-03-28T16:33:47.654876"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.4021, grad_norm=1.9325)",
    "\u2192 Boosted memory mem_272e7ed4b572 QuickRecal by 0.1932 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_272e7ed4b572",
    "confidence": 0.5,
    "timestamp": "2025-03-28T16:33:47.691738"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328172955_7f9e509cdf30.json

```json
{
  "trace_id": "intent_20250328172955_7f9e509cdf30",
  "timestamp": "2025-03-28T17:29:55.611741",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.13993334770202637
  },
  "neural_memory_trace": {
    "loss": 0.3136737048625946,
    "grad_norm": 1.3993334770202637,
    "timestamp": "2025-03-28T17:29:56.075844"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.3137, grad_norm=1.3993)",
    "\u2192 Boosted memory mem_b6dbc378418b QuickRecal by 0.1399 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_b6dbc378418b",
    "confidence": 0.5,
    "timestamp": "2025-03-28T17:29:56.140067"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328174115_7f52463bd9c0.json

```json
{
  "trace_id": "intent_20250328174115_7f52463bd9c0",
  "timestamp": "2025-03-28T17:41:15.424577",
  "memory_trace": {
    "retrieved": [],
    "boost_applied": 0.1150374174118042
  },
  "neural_memory_trace": {
    "loss": 0.23371055722236633,
    "grad_norm": 1.150374174118042,
    "timestamp": "2025-03-28T17:41:15.669229"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.2337, grad_norm=1.1504)",
    "\u2192 Boosted memory mem_35fce9e12625 QuickRecal by 0.1150 due to surprise"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_35fce9e12625",
    "confidence": 0.5,
    "timestamp": "2025-03-28T17:41:15.700138"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328174250_7f8e9c2ce080.json

```json
{
  "trace_id": "intent_20250328174250_7f8e9c2ce080",
  "timestamp": "2025-03-28T17:42:50.108547",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_73a5584e6e8c",
    "confidence": 0.5,
    "timestamp": "2025-03-28T17:42:58.530089"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328175111_7feeca2c60b0.json

```json
{
  "trace_id": "intent_20250328175111_7feeca2c60b0",
  "timestamp": "2025-03-28T17:51:11.858261",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_e46ba92b0e4e",
    "confidence": 0.5,
    "timestamp": "2025-03-28T17:51:12.392012"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328175357_7f09250ca110.json

```json
{
  "trace_id": "intent_20250328175357_7f09250ca110",
  "timestamp": "2025-03-28T17:53:57.912299",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_c0bdc7c26774",
    "confidence": 0.5,
    "timestamp": "2025-03-28T17:53:58.186126"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328175436_7f163baca080.json

```json
{
  "trace_id": "intent_20250328175436_7f163baca080",
  "timestamp": "2025-03-28T17:54:36.379079",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_6d164d8e233f_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003935945220291615
  },
  "neural_memory_trace": {
    "loss": 0.0008068761671893299,
    "grad_norm": 0.0039359452202916145,
    "timestamp": "2025-03-28T17:54:37.047505"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0008, grad_norm=0.0039)",
    "\u2192 Boosted memory mem_6d164d8e233f QuickRecal by 0.0004 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_6d164d8e233f",
    "confidence": 1.0,
    "timestamp": "2025-03-28T17:54:37.090200"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328183049_7f41361473a0.json

```json
{
  "trace_id": "intent_20250328183049_7f41361473a0",
  "timestamp": "2025-03-28T18:30:49.042782",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_f1f4191cab2b_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003781659994274378
  },
  "neural_memory_trace": {
    "loss": 0.0007671408820897341,
    "grad_norm": 0.003781659994274378,
    "timestamp": "2025-03-28T18:30:50.050661"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0008, grad_norm=0.0038)",
    "\u2192 Boosted memory mem_f1f4191cab2b QuickRecal by 0.0004 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_f1f4191cab2b",
    "confidence": 1.0,
    "timestamp": "2025-03-28T18:30:50.149131"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328183556_7fa3b78d5c30.json

```json
{
  "trace_id": "intent_20250328183556_7fa3b78d5c30",
  "timestamp": "2025-03-28T18:35:56.483458",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_c7b00fe37a83_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003109997604042292
  },
  "neural_memory_trace": {
    "loss": 0.0006745746359229088,
    "grad_norm": 0.0031099976040422916,
    "timestamp": "2025-03-28T18:35:56.803905"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0031)",
    "\u2192 Boosted memory mem_c7b00fe37a83 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_c7b00fe37a83",
    "confidence": 1.0,
    "timestamp": "2025-03-28T18:35:56.866614"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328184001_7f680f0d1c00.json

```json
{
  "trace_id": "intent_20250328184001_7f680f0d1c00",
  "timestamp": "2025-03-28T18:40:01.895401",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_a4f3fd066279_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0002861972898244858
  },
  "neural_memory_trace": {
    "loss": 0.0006753114867024124,
    "grad_norm": 0.002861972898244858,
    "timestamp": "2025-03-28T18:40:02.600124"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0029)",
    "\u2192 Boosted memory mem_a4f3fd066279 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_a4f3fd066279",
    "confidence": 1.0,
    "timestamp": "2025-03-28T18:40:02.646289"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328185341_7f9270ed9c60.json

```json
{
  "trace_id": "intent_20250328185341_7f9270ed9c60",
  "timestamp": "2025-03-28T18:53:41.638845",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_5bd3d689db43_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0002533602295443416
  },
  "neural_memory_trace": {
    "loss": 0.0006202238146215677,
    "grad_norm": 0.0025336022954434156,
    "timestamp": "2025-03-28T18:53:41.956737"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0006, grad_norm=0.0025)",
    "\u2192 Boosted memory mem_5bd3d689db43 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_5bd3d689db43",
    "confidence": 1.0,
    "timestamp": "2025-03-28T18:53:42.019203"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328191231_7f7c6325ad10.json

```json
{
  "trace_id": "intent_20250328191231_7f7c6325ad10",
  "timestamp": "2025-03-28T19:12:31.866357",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_5be7b810f7a3_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0002520052716135979
  },
  "neural_memory_trace": {
    "loss": 0.0006532114348374307,
    "grad_norm": 0.0025200527161359787,
    "timestamp": "2025-03-28T19:12:32.165991"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0025)",
    "\u2192 Boosted memory mem_5be7b810f7a3 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_5be7b810f7a3",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:12:32.219087"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328191334_7f347ae52c20.json

```json
{
  "trace_id": "intent_20250328191334_7f347ae52c20",
  "timestamp": "2025-03-28T19:13:34.317212",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_mem_8b135a21ce54_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.00025090742856264113
  },
  "neural_memory_trace": {
    "loss": 0.0006816776585765183,
    "grad_norm": 0.0025090742856264114,
    "timestamp": "2025-03-28T19:13:34.715030"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0025)",
    "\u2192 Boosted memory mem_8b135a21ce54 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved associated embedding for memory mem_8b135a21ce54",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:13:34.766143"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328195500_7f938a381cc0.json

```json
{
  "trace_id": "intent_20250328195500_7f938a381cc0",
  "timestamp": "2025-03-28T19:55:00.062085",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.00037442808970808986
  },
  "neural_memory_trace": {
    "loss": 0.0007820589817129076,
    "grad_norm": 0.0037442808970808983,
    "timestamp": "2025-03-28T19:55:00.756703"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0008, grad_norm=0.0037)",
    "\u2192 Boosted memory mem_446fc6179096 QuickRecal by 0.0004 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:55:00.795396"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328195533_7f22d9ff20e0.json

```json
{
  "trace_id": "intent_20250328195533_7f22d9ff20e0",
  "timestamp": "2025-03-28T19:55:33.593328",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003245685948058963
  },
  "neural_memory_trace": {
    "loss": 0.0007444422226399183,
    "grad_norm": 0.003245685948058963,
    "timestamp": "2025-03-28T19:55:33.817250"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0032)",
    "\u2192 Boosted memory mem_ed223fe94686 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:55:33.891738"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328195546_7f22461e20e0.json

```json
{
  "trace_id": "intent_20250328195546_7f22461e20e0",
  "timestamp": "2025-03-28T19:55:46.739877",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.00028283023275434973
  },
  "neural_memory_trace": {
    "loss": 0.0006729270680807531,
    "grad_norm": 0.002828302327543497,
    "timestamp": "2025-03-28T19:55:47.225380"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0028)",
    "\u2192 Boosted memory mem_8fac3e5f6a6b QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:55:47.313321"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328195600_7fc311cea0e0.json

```json
{
  "trace_id": "intent_20250328195600_7fc311cea0e0",
  "timestamp": "2025-03-28T19:56:00.167973",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0002454044762998819
  },
  "neural_memory_trace": {
    "loss": 0.00063512590713799,
    "grad_norm": 0.0024540447629988194,
    "timestamp": "2025-03-28T19:56:00.386984"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0006, grad_norm=0.0025)",
    "\u2192 Boosted memory mem_6a542bb5cd04 QuickRecal by 0.0002 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-28T19:56:00.434376"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328211848_7f27ec71f580.json

```json
{
  "trace_id": "intent_20250328211848_7f27ec71f580",
  "timestamp": "2025-03-28T21:18:48.071529",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:18:52.054642"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328212418_7f45a6d1b430.json

```json
{
  "trace_id": "intent_20250328212418_7f45a6d1b430",
  "timestamp": "2025-03-28T21:24:18.011845",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:24:21.968867"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328212549_7f45a6d1b430.json

```json
{
  "trace_id": "intent_20250328212549_7f45a6d1b430",
  "timestamp": "2025-03-28T21:25:49.712341",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:25:57.715795"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328213036_7f45a6d1b430.json

```json
{
  "trace_id": "intent_20250328213036_7f45a6d1b430",
  "timestamp": "2025-03-28T21:30:36.862823",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:30:44.832562"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328213406_7f45a6d1b430.json

```json
{
  "trace_id": "intent_20250328213406_7f45a6d1b430",
  "timestamp": "2025-03-28T21:34:06.565163",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:34:10.568256"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250328213800_7f303a697430.json

```json
{
  "trace_id": "intent_20250328213800_7f303a697430",
  "timestamp": "2025-03-28T21:38:00.348887",
  "memory_trace": {
    "retrieved": []
  },
  "neural_memory_trace": {},
  "emotional_modulation": {},
  "reasoning_steps": [],
  "final_output": {
    "response_text": "Error: Memory storage failed",
    "confidence": 0.0,
    "timestamp": "2025-03-28T21:38:08.348193"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250329104259_7fed76197400.json

```json
{
  "trace_id": "intent_20250329104259_7fed76197400",
  "timestamp": "2025-03-29T10:42:59.006655",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003789004171267152
  },
  "neural_memory_trace": {
    "loss": 0.0008071609190665185,
    "grad_norm": 0.003789004171267152,
    "timestamp": "2025-03-29T10:42:59.770033"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0008, grad_norm=0.0038)",
    "\u2192 Boosted memory mem_44f0f7948e17 QuickRecal by 0.0004 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-29T10:42:59.818547"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250329104640_7fed76197400.json

```json
{
  "trace_id": "intent_20250329104640_7fed76197400",
  "timestamp": "2025-03-29T10:46:40.291029",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0003280433360487223
  },
  "neural_memory_trace": {
    "loss": 0.0007431074045598507,
    "grad_norm": 0.0032804333604872227,
    "timestamp": "2025-03-29T10:46:40.523160"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0033)",
    "\u2192 Boosted memory mem_3a9d15c542a4 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-29T10:46:40.569621"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250329104859_7fed76197400.json

```json
{
  "trace_id": "intent_20250329104859_7fed76197400",
  "timestamp": "2025-03-29T10:48:59.682809",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.00029041040688753127
  },
  "neural_memory_trace": {
    "loss": 0.0007035359158180654,
    "grad_norm": 0.002904104068875313,
    "timestamp": "2025-03-29T10:48:59.860028"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0029)",
    "\u2192 Boosted memory mem_a4d05a43dbe6 QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-29T10:48:59.901882"
  }
}
```

# synthians_trainer_server\logs\intent_graphs\intent_20250329105716_7fed76197400.json

```json
{
  "trace_id": "intent_20250329105716_7fed76197400",
  "timestamp": "2025-03-29T10:57:16.690742",
  "memory_trace": {
    "retrieved": [
      {
        "memory_id": "synthetic_associated",
        "quickrecal_score": 0.0,
        "dominant_emotion": null,
        "emotion_confidence": 0.0
      }
    ],
    "boost_applied": 0.0002642100211232901
  },
  "neural_memory_trace": {
    "loss": 0.0006853328086435795,
    "grad_norm": 0.0026421002112329006,
    "timestamp": "2025-03-29T10:57:16.840758"
  },
  "emotional_modulation": {},
  "reasoning_steps": [
    "\u2192 Updated Neural Memory with new embedding (loss=0.0007, grad_norm=0.0026)",
    "\u2192 Boosted memory mem_7974ce41c27c QuickRecal by 0.0003 due to surprise",
    "\u2192 Retrieved 1 memories based on query"
  ],
  "final_output": {
    "response_text": "Retrieved: 768 dims",
    "confidence": 1.0,
    "timestamp": "2025-03-29T10:57:16.912214"
  }
}
```

# synthians_trainer_server\logs\memory_updates.jsonl

```jsonl
{"timestamp": "2025-03-28T13:19:27.012220", "intent_id": null, "loss": 0.5985302329063416, "grad_norm": 3.0188727378845215, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:19:27.010132", "input_dim": 768}}
{"timestamp": "2025-03-28T13:19:27.028056", "intent_id": "intent_20250328131926_1cbfccb5400", "loss": 0.5985302329063416, "grad_norm": 3.0188727378845215, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_9ca344764818", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.6438470492114289}}
{"timestamp": "2025-03-28T13:21:22.189592", "intent_id": null, "loss": 0.6196844577789307, "grad_norm": 3.0371012687683105, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:21:22.187450", "input_dim": 768}}
{"timestamp": "2025-03-28T13:21:22.194439", "intent_id": "intent_20250328132121_2297f0f5520", "loss": 0.6196844577789307, "grad_norm": 3.0371012687683105, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_e448cc7dedf9", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.5589206536487461}}
{"timestamp": "2025-03-28T13:29:21.661348", "intent_id": null, "loss": 0.7034170627593994, "grad_norm": 3.310447931289673, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:29:21.659173", "input_dim": 768}}
{"timestamp": "2025-03-28T13:29:21.671043", "intent_id": "intent_20250328132916_2bfdae74a70", "loss": 0.7034170627593994, "grad_norm": 3.310447931289673, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_53e1646c988f", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.6450518791599862}}
{"timestamp": "2025-03-28T13:33:02.750061", "intent_id": null, "loss": 0.6607450842857361, "grad_norm": 3.1318135261535645, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:33:02.747595", "input_dim": 768}}
{"timestamp": "2025-03-28T13:33:02.752793", "intent_id": "intent_20250328133258_1ab6fe56120", "loss": 0.6607450842857361, "grad_norm": 3.1318135261535645, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_b0ba34039c02", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.5840457341877383}}
{"timestamp": "2025-03-28T13:35:54.864249", "intent_id": null, "loss": 0.6500575542449951, "grad_norm": 3.089069366455078, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:35:54.862235", "input_dim": 768}}
{"timestamp": "2025-03-28T13:35:54.859924", "intent_id": "intent_20250328133554_1887e929fd0", "loss": 0.6500575542449951, "grad_norm": 3.089069366455078, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_febc4eb7ec62", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.5147387724904321}}
{"timestamp": "2025-03-28T13:41:33.448902", "intent_id": null, "loss": 0.6635435819625854, "grad_norm": 3.1921162605285645, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:41:33.446658", "input_dim": 768}}
{"timestamp": "2025-03-28T13:41:33.463407", "intent_id": "intent_20250328134128_1877b5caba0", "loss": 0.6635435819625854, "grad_norm": 3.1921162605285645, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_93c8e1a3c865", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.5498046149603236}}
{"timestamp": "2025-03-28T13:43:17.991268", "intent_id": null, "loss": 0.6244530081748962, "grad_norm": 2.9935226440429688, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T13:43:17.988911", "input_dim": 768}}
{"timestamp": "2025-03-28T13:43:18.015688", "intent_id": "intent_20250328134317_26a809cfbf0", "loss": 0.6244530081748962, "grad_norm": 2.9935226440429688, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": "curiosity", "metadata": {"memory_id": "mem_28110eb9a3ec", "content_preview": "What were the key design principles behind the Tit", "quickrecal_initial": 0.5433542521174316}}
{"timestamp": "2025-03-28T16:32:40.834524", "intent_id": null, "loss": 0.6416096091270447, "grad_norm": 3.140026569366455, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T16:32:40.834504", "input_dim": 768}}
{"timestamp": "2025-03-28T16:32:40.852472", "intent_id": "intent_20250328163233_7ff4735cdf30", "loss": 0.6416096091270447, "grad_norm": 3.140026569366455, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_00790152fab6", "content_preview": "This is a test of the MAC Titans variant", "quickrecal_initial": 0.5543885417225467}}
{"timestamp": "2025-03-28T16:33:22.375983", "intent_id": null, "loss": 0.5404383540153503, "grad_norm": 2.4210591316223145, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T16:33:22.375962", "input_dim": 768}}
{"timestamp": "2025-03-28T16:33:22.389749", "intent_id": "intent_20250328163322_7efcb58d2020", "loss": 0.5404383540153503, "grad_norm": 2.4210591316223145, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_715e0719f653", "content_preview": "This is a test of the MAG Titans variant", "quickrecal_initial": 0.645345684244842}}
{"timestamp": "2025-03-28T16:33:47.641542", "intent_id": null, "loss": 0.4021265506744385, "grad_norm": 1.9324893951416016, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T16:33:47.641523", "input_dim": 768}}
{"timestamp": "2025-03-28T16:33:47.654876", "intent_id": "intent_20250328163347_7fc19decdf30", "loss": 0.4021265506744385, "grad_norm": 1.9324893951416016, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_272e7ed4b572", "content_preview": "This is a test of the MAL Titans variant", "quickrecal_initial": 0.5262281089605335}}
{"timestamp": "2025-03-28T17:29:56.054108", "intent_id": null, "loss": 0.3136737048625946, "grad_norm": 1.3993334770202637, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T17:29:56.053861", "input_dim": 768}}
{"timestamp": "2025-03-28T17:29:56.075844", "intent_id": "intent_20250328172955_7f9e509cdf30", "loss": 0.3136737048625946, "grad_norm": 1.3993334770202637, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_b6dbc378418b", "content_preview": "This is a test of the MAG Titans variant", "quickrecal_initial": 0.658370701722247}}
{"timestamp": "2025-03-28T17:41:15.652709", "intent_id": null, "loss": 0.23371055722236633, "grad_norm": 1.150374174118042, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T17:41:15.652672", "input_dim": 768}}
{"timestamp": "2025-03-28T17:41:15.669229", "intent_id": "intent_20250328174115_7f52463bd9c0", "loss": 0.23371055722236633, "grad_norm": 1.150374174118042, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_35fce9e12625", "content_preview": "This is a test of the MAG Titans variant", "quickrecal_initial": 0.5688081714862355, "variant_type": "MAG"}}
{"timestamp": "2025-03-28T17:54:37.027889", "intent_id": null, "loss": 0.0008068761671893299, "grad_norm": 0.0039359452202916145, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T17:54:37.025636", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T17:54:37.047505", "intent_id": "intent_20250328175436_7f163baca080", "loss": 0.0008068761671893299, "grad_norm": 0.0039359452202916145, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_6d164d8e233f", "content_preview": "This is a test of the MAG Titans variant", "quickrecal_initial": 0.5974839517515492, "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:30:50.007627", "intent_id": null, "loss": 0.0007671408820897341, "grad_norm": 0.003781659994274378, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T18:30:49.994874", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T18:30:50.050661", "intent_id": "intent_20250328183049_7f41361473a0", "loss": 0.0007671408820897341, "grad_norm": 0.003781659994274378, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_f1f4191cab2b", "content_preview": "This is a test of the MAG Titans variant", "quickrecal_initial": 0.6007667428435506, "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:35:56.788923", "intent_id": null, "loss": 0.0006745746359229088, "grad_norm": 0.0031099976040422916, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T18:35:56.788901", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T18:35:56.803905", "intent_id": "intent_20250328183556_7fa3b78d5c30", "loss": 0.0006745746359229088, "grad_norm": 0.0031099976040422916, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_c7b00fe37a83", "content_preview": "The neural memory adapts gate values dynamically", "quickrecal_initial": 0.5927101845298544, "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:40:02.583314", "intent_id": null, "loss": 0.0006753114867024124, "grad_norm": 0.002861972898244858, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T18:40:02.583292", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T18:40:02.600124", "intent_id": "intent_20250328184001_7f680f0d1c00", "loss": 0.0006753114867024124, "grad_norm": 0.002861972898244858, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_a4f3fd066279", "content_preview": "Lucidia's cognitive system adapts its memory throu", "quickrecal_initial": 0.5589977731940781, "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:53:41.927704", "intent_id": null, "loss": 0.0006202238146215677, "grad_norm": 0.0025336022954434156, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T18:53:41.927640", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T18:53:41.956737", "intent_id": "intent_20250328185341_7f9270ed9c60", "loss": 0.0006202238146215677, "grad_norm": 0.0025336022954434156, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_5bd3d689db43", "content_preview": "This is a test of the MAL Titans variant", "quickrecal_initial": 0.6278911673581282, "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:12:32.149312", "intent_id": null, "loss": 0.0006532114348374307, "grad_norm": 0.0025200527161359787, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:12:32.149261", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:12:32.165991", "intent_id": "intent_20250328191231_7f7c6325ad10", "loss": 0.0006532114348374307, "grad_norm": 0.0025200527161359787, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_5be7b810f7a3", "content_preview": "Testing defensive coding for diagnostics data poin", "quickrecal_initial": 0.6306412646814934, "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:13:34.698433", "intent_id": null, "loss": 0.0006816776585765183, "grad_norm": 0.0025090742856264114, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:13:34.698405", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:13:34.715030", "intent_id": "intent_20250328191334_7f347ae52c20", "loss": 0.0006816776585765183, "grad_norm": 0.0025090742856264114, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_8b135a21ce54", "content_preview": "Final test of improved diagnostics handling", "quickrecal_initial": 0.5474431340962839, "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:51:29.888918", "intent_id": null, "loss": 0.000912700139451772, "grad_norm": 0.0043389564380049706, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:51:29.886748", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:51:29.904224", "intent_id": "intent_20250328195129_7f6506a85d20", "loss": 0.000912700139451772, "grad_norm": 0.0043389564380049706, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_fe49141047b4", "content_preview": "Testing NONE variant after type fixes", "variant_type": "NONE"}}
{"timestamp": "2025-03-28T19:55:00.740549", "intent_id": null, "loss": 0.0007820589817129076, "grad_norm": 0.0037442808970808983, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:55:00.738111", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:55:00.756703", "intent_id": "intent_20250328195500_7f938a381cc0", "loss": 0.0007820589817129076, "grad_norm": 0.0037442808970808983, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_446fc6179096", "content_preview": "Testing NONE variant after type fixes", "variant_type": "NONE"}}
{"timestamp": "2025-03-28T19:55:33.801363", "intent_id": null, "loss": 0.0007444422226399183, "grad_norm": 0.003245685948058963, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:55:33.801344", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:55:33.817250", "intent_id": "intent_20250328195533_7f22d9ff20e0", "loss": 0.0007444422226399183, "grad_norm": 0.003245685948058963, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_ed223fe94686", "content_preview": "Testing MAC variant after refactoring", "variant_type": "MAC"}}
{"timestamp": "2025-03-28T19:55:47.211048", "intent_id": null, "loss": 0.0006729270680807531, "grad_norm": 0.002828302327543497, "embedding_norm": 1.0000001192092896, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:55:47.210997", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:55:47.225380", "intent_id": "intent_20250328195546_7f22461e20e0", "loss": 0.0006729270680807531, "grad_norm": 0.002828302327543497, "embedding_norm": 1.0000001192092896, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_8fac3e5f6a6b", "content_preview": "Testing MAG variant after refactoring", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T19:56:00.372101", "intent_id": null, "loss": 0.00063512590713799, "grad_norm": 0.0024540447629988194, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-28T19:56:00.372079", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-28T19:56:00.386984", "intent_id": "intent_20250328195600_7fc311cea0e0", "loss": 0.00063512590713799, "grad_norm": 0.0024540447629988194, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_6a542bb5cd04", "content_preview": "Testing MAL variant with new calculate_v_prime", "variant_type": "MAL"}}
{"timestamp": "2025-03-29T10:42:59.748081", "intent_id": null, "loss": 0.0008071609190665185, "grad_norm": 0.003789004171267152, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-29T10:42:59.744508", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-29T10:42:59.770033", "intent_id": "intent_20250329104259_7fed76197400", "loss": 0.0008071609190665185, "grad_norm": 0.003789004171267152, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_44f0f7948e17", "content_preview": "A surprising development regarding Quantum Entangl", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:46:40.508149", "intent_id": null, "loss": 0.0007431074045598507, "grad_norm": 0.0032804333604872227, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-29T10:46:40.508090", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-29T10:46:40.523160", "intent_id": "intent_20250329104640_7fed76197400", "loss": 0.0007431074045598507, "grad_norm": 0.0032804333604872227, "embedding_norm": 1.0, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_3a9d15c542a4", "content_preview": "A surprising development regarding Quantum Entangl", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:48:59.847492", "intent_id": null, "loss": 0.0007035359158180654, "grad_norm": 0.002904104068875313, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-29T10:48:59.847455", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-29T10:48:59.860028", "intent_id": "intent_20250329104859_7fed76197400", "loss": 0.0007035359158180654, "grad_norm": 0.002904104068875313, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_a4d05a43dbe6", "content_preview": "A surprising development regarding Quantum Entangl", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:57:16.825944", "intent_id": null, "loss": 0.0006853328086435795, "grad_norm": 0.0026421002112329006, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"timestamp": "2025-03-29T10:57:16.825920", "input_dim": 768, "external_projections_used": false, "external_gates_used": false}}
{"timestamp": "2025-03-29T10:57:16.840758", "intent_id": "intent_20250329105716_7fed76197400", "loss": 0.0006853328086435795, "grad_norm": 0.0026421002112329006, "embedding_norm": 0.9999999403953552, "embedding_dim": 768, "emotion": null, "metadata": {"memory_id": "mem_7974ce41c27c", "content_preview": "A surprising development regarding Quantum Entangl", "variant_type": "NONE"}}

```

# synthians_trainer_server\logs\quickrecal_boosts.jsonl

```jsonl
{"timestamp": "2025-03-28T13:21:22.200359", "intent_id": "intent_20250328132121_2297f0f5520", "memory_id": "mem_e448cc7dedf9", "base_score": 0.5589206536487461, "boost_amount": 0.2, "final_score": 0.7589206536487461, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.6196844577789307, "grad_norm": 3.0371012687683105, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T13:29:21.676391", "intent_id": "intent_20250328132916_2bfdae74a70", "memory_id": "mem_53e1646c988f", "base_score": 0.6450518791599862, "boost_amount": 0.2, "final_score": 0.8450518791599861, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.7034170627593994, "grad_norm": 3.310447931289673, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T13:33:02.759440", "intent_id": "intent_20250328133258_1ab6fe56120", "memory_id": "mem_b0ba34039c02", "base_score": 0.5840457341877383, "boost_amount": 0.2, "final_score": 0.7840457341877383, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.6607450842857361, "grad_norm": 3.1318135261535645, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T13:35:54.865255", "intent_id": "intent_20250328133554_1887e929fd0", "memory_id": "mem_febc4eb7ec62", "base_score": 0.5147387724904321, "boost_amount": 0.2, "final_score": 0.7147387724904322, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.6500575542449951, "grad_norm": 3.089069366455078, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T13:41:33.469326", "intent_id": "intent_20250328134128_1877b5caba0", "memory_id": "mem_93c8e1a3c865", "base_score": 0.5498046149603236, "boost_amount": 0.2, "final_score": 0.7498046149603237, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.6635435819625854, "grad_norm": 3.1921162605285645, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T13:43:18.021550", "intent_id": "intent_20250328134317_26a809cfbf0", "memory_id": "mem_28110eb9a3ec", "base_score": 0.5433542521174316, "boost_amount": 0.2, "final_score": 0.7433542521174317, "emotion": "curiosity", "surprise_source": "neural_memory", "metadata": {"loss": 0.6244530081748962, "grad_norm": 2.9935226440429688, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T16:32:40.863281", "intent_id": "intent_20250328163233_7ff4735cdf30", "memory_id": "mem_00790152fab6", "base_score": 0.5543885417225467, "boost_amount": 0.2, "final_score": 0.7543885417225467, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.6416096091270447, "grad_norm": 3.140026569366455, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T16:33:22.401872", "intent_id": "intent_20250328163322_7efcb58d2020", "memory_id": "mem_715e0719f653", "base_score": 0.645345684244842, "boost_amount": 0.2, "final_score": 0.8453456842448419, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.5404383540153503, "grad_norm": 2.4210591316223145, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T16:33:47.667072", "intent_id": "intent_20250328163347_7fc19decdf30", "memory_id": "mem_272e7ed4b572", "base_score": 0.5262281089605335, "boost_amount": 0.19324893951416017, "final_score": 0.7194770484746937, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.4021265506744385, "grad_norm": 1.9324893951416016, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T17:29:56.091788", "intent_id": "intent_20250328172955_7f9e509cdf30", "memory_id": "mem_b6dbc378418b", "base_score": 0.658370701722247, "boost_amount": 0.13993334770202637, "final_score": 0.7983040494242734, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.3136737048625946, "grad_norm": 1.3993334770202637, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T17:41:15.685476", "intent_id": "intent_20250328174115_7f52463bd9c0", "memory_id": "mem_35fce9e12625", "base_score": 0.5688081714862355, "boost_amount": 0.1150374174118042, "final_score": 0.6838455888980397, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.23371055722236633, "grad_norm": 1.150374174118042, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T17:54:37.059938", "intent_id": "intent_20250328175436_7f163baca080", "memory_id": "mem_6d164d8e233f", "base_score": 0.5974839517515492, "boost_amount": 0.0003935945220291615, "final_score": 0.5978775462735784, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0008068761671893299, "grad_norm": 0.0039359452202916145, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T18:30:50.090823", "intent_id": "intent_20250328183049_7f41361473a0", "memory_id": "mem_f1f4191cab2b", "base_score": 0.6007667428435506, "boost_amount": 0.0003781659994274378, "final_score": 0.601144908842978, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0007671408820897341, "grad_norm": 0.003781659994274378, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T18:35:56.819021", "intent_id": "intent_20250328183556_7fa3b78d5c30", "memory_id": "mem_c7b00fe37a83", "base_score": 0.5927101845298544, "boost_amount": 0.0003109997604042292, "final_score": 0.5930211842902586, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006745746359229088, "grad_norm": 0.0031099976040422916, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T18:40:02.612870", "intent_id": "intent_20250328184001_7f680f0d1c00", "memory_id": "mem_a4f3fd066279", "base_score": 0.5589977731940781, "boost_amount": 0.0002861972898244858, "final_score": 0.5592839704839025, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006753114867024124, "grad_norm": 0.002861972898244858, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T18:53:41.976568", "intent_id": "intent_20250328185341_7f9270ed9c60", "memory_id": "mem_5bd3d689db43", "base_score": 0.6278911673581282, "boost_amount": 0.0002533602295443416, "final_score": 0.6281445275876726, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006202238146215677, "grad_norm": 0.0025336022954434156, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:12:32.181791", "intent_id": "intent_20250328191231_7f7c6325ad10", "memory_id": "mem_5be7b810f7a3", "base_score": 0.6306412646814934, "boost_amount": 0.0002520052716135979, "final_score": 0.630893269953107, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006532114348374307, "grad_norm": 0.0025200527161359787, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:13:34.730095", "intent_id": "intent_20250328191334_7f347ae52c20", "memory_id": "mem_8b135a21ce54", "base_score": 0.5474431340962839, "boost_amount": 0.00025090742856264113, "final_score": 0.5476940415248465, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006816776585765183, "grad_norm": 0.0025090742856264114, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:51:29.915193", "intent_id": "intent_20250328195129_7f6506a85d20", "memory_id": "mem_fe49141047b4", "base_score": 0.61518422685717, "boost_amount": 0.0004338956438004971, "final_score": 0.6156181225009705, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.000912700139451772, "grad_norm": 0.0043389564380049706, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:55:00.767596", "intent_id": "intent_20250328195500_7f938a381cc0", "memory_id": "mem_446fc6179096", "base_score": 0.5949985340647745, "boost_amount": 0.00037442808970808986, "final_score": 0.5953729621544827, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0007820589817129076, "grad_norm": 0.0037442808970808983, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:55:33.829108", "intent_id": "intent_20250328195533_7f22d9ff20e0", "memory_id": "mem_ed223fe94686", "base_score": 0.6094970326649796, "boost_amount": 0.0003245685948058963, "final_score": 0.6098216012597855, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0007444422226399183, "grad_norm": 0.003245685948058963, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:55:47.238608", "intent_id": "intent_20250328195546_7f22461e20e0", "memory_id": "mem_8fac3e5f6a6b", "base_score": 0.6416814132426562, "boost_amount": 0.00028283023275434973, "final_score": 0.6419642434754105, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006729270680807531, "grad_norm": 0.002828302327543497, "reason": "NM Surprise"}}
{"timestamp": "2025-03-28T19:56:00.399218", "intent_id": "intent_20250328195600_7fc311cea0e0", "memory_id": "mem_6a542bb5cd04", "base_score": 0.5977799385058595, "boost_amount": 0.0002454044762998819, "final_score": 0.5980253429821594, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.00063512590713799, "grad_norm": 0.0024540447629988194, "reason": "NM Surprise"}}
{"timestamp": "2025-03-29T10:42:59.782655", "intent_id": "intent_20250329104259_7fed76197400", "memory_id": "mem_44f0f7948e17", "base_score": 0.535122603169681, "boost_amount": 0.0003789004171267152, "final_score": 0.5355015035868077, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0008071609190665185, "grad_norm": 0.003789004171267152, "reason": "NM Surprise"}}
{"timestamp": "2025-03-29T10:46:40.534420", "intent_id": "intent_20250329104640_7fed76197400", "memory_id": "mem_3a9d15c542a4", "base_score": 0.6611617336433391, "boost_amount": 0.0003280433360487223, "final_score": 0.6614897769793878, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0007431074045598507, "grad_norm": 0.0032804333604872227, "reason": "NM Surprise"}}
{"timestamp": "2025-03-29T10:48:59.868678", "intent_id": "intent_20250329104859_7fed76197400", "memory_id": "mem_a4d05a43dbe6", "base_score": 0.6471339773859797, "boost_amount": 0.00029041040688753127, "final_score": 0.6474243877928672, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0007035359158180654, "grad_norm": 0.002904104068875313, "reason": "NM Surprise"}}
{"timestamp": "2025-03-29T10:57:16.878143", "intent_id": "intent_20250329105716_7fed76197400", "memory_id": "mem_7974ce41c27c", "base_score": 0.6649938326346033, "boost_amount": 0.0002642100211232901, "final_score": 0.6652580426557266, "emotion": null, "surprise_source": "neural_memory", "metadata": {"loss": 0.0006853328086435795, "grad_norm": 0.0026421002112329006, "reason": "NM Surprise"}}

```

# synthians_trainer_server\logs\retrievals.jsonl

```jsonl
{"timestamp": "2025-03-28T13:43:18.032322", "intent_id": null, "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_memory"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"timestamp": "2025-03-28T13:43:18.032311", "embedding_dim": 768}}
{"timestamp": "2025-03-28T13:43:18.061046", "intent_id": "intent_20250328134317_26a809cfbf0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_28110eb9a3ec_associated"], "memory_emotions": [], "user_emotion": "curiosity", "emotion_match": false, "metadata": {"original_memory_id": "mem_28110eb9a3ec", "embedding_dim": 768, "timestamp": "2025-03-28T13:43:18.061046"}}
{"timestamp": "2025-03-28T17:54:37.084878", "intent_id": "intent_20250328175436_7f163baca080", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_6d164d8e233f_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_6d164d8e233f", "embedding_dim": 768, "timestamp": "2025-03-28T17:54:37.084854", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:30:50.139339", "intent_id": "intent_20250328183049_7f41361473a0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_f1f4191cab2b_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_f1f4191cab2b", "embedding_dim": 768, "timestamp": "2025-03-28T18:30:50.139316", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:35:56.863105", "intent_id": "intent_20250328183556_7fa3b78d5c30", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_c7b00fe37a83_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_c7b00fe37a83", "embedding_dim": 768, "timestamp": "2025-03-28T18:35:56.863083", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:40:02.640935", "intent_id": "intent_20250328184001_7f680f0d1c00", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_a4f3fd066279_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_a4f3fd066279", "embedding_dim": 768, "timestamp": "2025-03-28T18:40:02.640912", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T18:53:42.011629", "intent_id": "intent_20250328185341_7f9270ed9c60", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_5bd3d689db43_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_5bd3d689db43", "embedding_dim": 768, "timestamp": "2025-03-28T18:53:42.011608", "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:12:32.213177", "intent_id": "intent_20250328191231_7f7c6325ad10", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_5be7b810f7a3_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_5be7b810f7a3", "embedding_dim": 768, "timestamp": "2025-03-28T19:12:32.213153", "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:13:34.757366", "intent_id": "intent_20250328191334_7f347ae52c20", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_mem_8b135a21ce54_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"original_memory_id": "mem_8b135a21ce54", "embedding_dim": 768, "timestamp": "2025-03-28T19:13:34.757347", "variant_type": "MAL"}}
{"timestamp": "2025-03-28T19:51:29.940709", "intent_id": "intent_20250328195129_7f6506a85d20", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-28T19:51:29.940688", "variant_type": "NONE"}}
{"timestamp": "2025-03-28T19:55:00.790764", "intent_id": "intent_20250328195500_7f938a381cc0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-28T19:55:00.790748", "variant_type": "NONE"}}
{"timestamp": "2025-03-28T19:55:33.875384", "intent_id": "intent_20250328195533_7f22d9ff20e0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-28T19:55:33.875364", "variant_type": "MAC"}}
{"timestamp": "2025-03-28T19:55:47.268864", "intent_id": "intent_20250328195546_7f22461e20e0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-28T19:55:47.268845", "variant_type": "MAG"}}
{"timestamp": "2025-03-28T19:56:00.429004", "intent_id": "intent_20250328195600_7fc311cea0e0", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-28T19:56:00.428984", "variant_type": "MAL"}}
{"timestamp": "2025-03-29T10:42:59.810538", "intent_id": "intent_20250329104259_7fed76197400", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-29T10:42:59.810498", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:46:40.559906", "intent_id": "intent_20250329104640_7fed76197400", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-29T10:46:40.559887", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:48:59.895064", "intent_id": "intent_20250329104859_7fed76197400", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-29T10:48:59.895048", "variant_type": "NONE"}}
{"timestamp": "2025-03-29T10:57:16.902385", "intent_id": "intent_20250329105716_7fed76197400", "embedding_dim": 768, "num_results": 1, "memory_ids": ["synthetic_associated"], "memory_emotions": [], "user_emotion": null, "emotion_match": false, "metadata": {"embedding_dim": 768, "timestamp": "2025-03-29T10:57:16.902369", "variant_type": "NONE"}}

```

# synthians_trainer_server\metrics_store.py

```py
# synthians_trainer_server/metrics_store.py

import time
import logging
import json
import datetime
import threading
import os
from typing import Dict, List, Any, Optional, Union, Deque
from collections import deque, defaultdict
import numpy as np

logger = logging.getLogger(__name__)

class MetricsStore:
    """Captures and stores cognitive flow metrics for introspection and diagnostics.
    
    This lightweight metrics collection system records data about memory operations,
    surprise signals, and emotional feedback to enable real-time diagnostics of
    Lucidia's cognitive processes without requiring complex UI infrastructure.
    
    The store maintains an in-memory buffer of recent metrics while offering
    optional persistence to log files for post-session analysis.
    """
    
    def __init__(self, max_buffer_size: int = 1000, 
                intent_graph_enabled: bool = True,
                log_dir: Optional[str] = None):
        """Initialize the metrics store.
        
        Args:
            max_buffer_size: Maximum number of events to keep in memory
            intent_graph_enabled: Whether to generate IntentGraph logs
            log_dir: Directory to save logs (None = no file logging)
        """
        self.max_buffer_size = max_buffer_size
        self.intent_graph_enabled = intent_graph_enabled
        self.log_dir = log_dir
        
        # Create log directory if needed
        if self.log_dir and not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir, exist_ok=True)
            logger.info(f"Created metrics log directory: {self.log_dir}")
        
        # In-memory metric buffers (thread-safe)
        self._lock = threading.RLock()  # Reentrant lock for thread safety
        self._memory_updates = deque(maxlen=max_buffer_size)  # Update events
        self._retrievals = deque(maxlen=max_buffer_size)  # Retrieval events
        self._quickrecal_boosts = deque(maxlen=max_buffer_size)  # QuickRecal boost events
        self._emotion_metrics = deque(maxlen=max_buffer_size)  # Emotional response events
        
        # Track current intent/interaction session
        self._current_intent_id = None
        self._intent_graph_buffer = {}
        
        # Emotional state tracking
        self._emotion_counts = defaultdict(int)
        self._user_emotion_matches = [0, 0]  # [matches, total]
        
        logger.info(f"MetricsStore initialized with buffer size {max_buffer_size}")
    
    def begin_intent(self, intent_id: Optional[str] = None) -> str:
        """Start a new intent/interaction tracking session.
        
        Returns:
            str: The intent_id (generated if not provided)
        """
        with self._lock:
            # Generate ID if not provided
            if not intent_id:
                intent_id = f"intent_{datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')}_{id(self):x}"
            
            self._current_intent_id = intent_id
            
            # Initialize intent graph for this session
            if self.intent_graph_enabled:
                self._intent_graph_buffer[intent_id] = {
                    "trace_id": intent_id,
                    "timestamp": datetime.datetime.utcnow().isoformat(),
                    "memory_trace": {"retrieved": []},
                    "neural_memory_trace": {},
                    "emotional_modulation": {},
                    "reasoning_steps": [],
                    "final_output": {}
                }
            
            logger.debug(f"Started new intent tracking: {intent_id}")
            return intent_id
    
    def log_memory_update(self, input_embedding: List[float], loss: float, grad_norm: float, 
                        emotion: Optional[str] = None, intent_id: Optional[str] = None,
                        metadata: Optional[Dict[str, Any]] = None) -> None:
        """Log metrics from a memory update operation.
        
        Args:
            input_embedding: The embedding that was sent to update memory
            loss: The loss value from the memory update
            grad_norm: The gradient norm from the memory update
            emotion: Optional emotion tag associated with this update
            intent_id: Optional intent ID (uses current if not provided)
            metadata: Additional metadata to store with the update
        """
        event_time = datetime.datetime.utcnow()
        intent_id = intent_id or self._current_intent_id
        
        # Calculate embedding norm for reference
        embedding_norm = float(np.linalg.norm(np.array(input_embedding, dtype=np.float32)))
        
        event = {
            "timestamp": event_time.isoformat(),
            "intent_id": intent_id,
            "loss": float(loss),
            "grad_norm": float(grad_norm),
            "embedding_norm": embedding_norm,
            "embedding_dim": len(input_embedding),
            "emotion": emotion,
            "metadata": metadata or {}
        }
        
        with self._lock:
            # Store in memory buffer
            self._memory_updates.append(event)
            
            # Update emotion counts if provided
            if emotion:
                self._emotion_counts[emotion] += 1
            
            # Update intent graph if enabled
            if self.intent_graph_enabled and intent_id in self._intent_graph_buffer:
                self._intent_graph_buffer[intent_id]["neural_memory_trace"] = {
                    **self._intent_graph_buffer[intent_id].get("neural_memory_trace", {}),
                    "loss": float(loss),
                    "grad_norm": float(grad_norm),
                    "timestamp": event_time.isoformat()
                }
                # Add reasoning step
                self._intent_graph_buffer[intent_id]["reasoning_steps"].append(
                    f"→ Updated Neural Memory with new embedding (loss={loss:.4f}, grad_norm={grad_norm:.4f})"
                )
        
        # Optionally log to file
        self._maybe_write_event_log("memory_updates", event)
        logger.debug(f"Logged memory update: loss={loss:.4f}, grad_norm={grad_norm:.4f}")
    
    def log_quickrecal_boost(self, memory_id: str, base_score: float, boost_amount: float,
                           emotion: Optional[str] = None, surprise_source: str = "neural_memory",
                           intent_id: Optional[str] = None, 
                           metadata: Optional[Dict[str, Any]] = None) -> None:
        """Log a QuickRecal score boost event.
        
        Args:
            memory_id: ID of the memory whose QuickRecal score was boosted
            base_score: Original QuickRecal score before boost
            boost_amount: Amount the score was boosted by
            emotion: Emotion associated with this memory/boost
            surprise_source: Source of the surprise signal (neural_memory, direct, etc.)
            intent_id: Optional intent ID (uses current if not provided)
            metadata: Additional metadata to store with the boost event
        """
        event_time = datetime.datetime.utcnow()
        intent_id = intent_id or self._current_intent_id
        
        event = {
            "timestamp": event_time.isoformat(),
            "intent_id": intent_id,
            "memory_id": memory_id,
            "base_score": float(base_score),
            "boost_amount": float(boost_amount),
            "final_score": float(base_score + boost_amount),
            "emotion": emotion,
            "surprise_source": surprise_source,
            "metadata": metadata or {}
        }
        
        with self._lock:
            # Store in memory buffer
            self._quickrecal_boosts.append(event)
            
            # Update intent graph if enabled
            if self.intent_graph_enabled and intent_id in self._intent_graph_buffer:
                # Add to memory trace
                memory_trace = self._intent_graph_buffer[intent_id]["memory_trace"]
                memory_trace["boost_applied"] = boost_amount
                
                # Add reasoning step
                self._intent_graph_buffer[intent_id]["reasoning_steps"].append(
                    f"→ Boosted memory {memory_id} QuickRecal by {boost_amount:.4f} due to surprise"
                )
        
        # Optionally log to file
        self._maybe_write_event_log("quickrecal_boosts", event)
        logger.debug(f"Logged QuickRecal boost: memory={memory_id}, amount={boost_amount:.4f}")
    
    def log_retrieval(self, query_embedding: List[float], retrieved_memories: List[Dict[str, Any]],
                     user_emotion: Optional[str] = None, intent_id: Optional[str] = None,
                     metadata: Optional[Dict[str, Any]] = None) -> None:
        """Log a memory retrieval operation.
        
        Args:
            query_embedding: Embedding used for retrieval
            retrieved_memories: List of retrieved memories with their metadata
            user_emotion: Current user emotion if known
            intent_id: Optional intent ID (uses current if not provided) 
            metadata: Additional metadata to store with the retrieval
        """
        event_time = datetime.datetime.utcnow()
        intent_id = intent_id or self._current_intent_id
        
        # Extract memory emotions if available
        memory_emotions = []
        for mem in retrieved_memories:
            if "dominant_emotion" in mem and mem["dominant_emotion"]:
                memory_emotions.append(mem["dominant_emotion"])
        
        # Calculate emotion match rate if user emotion is known
        emotion_match = False
        if user_emotion and memory_emotions:
            emotion_match = user_emotion in memory_emotions
            with self._lock:
                self._user_emotion_matches[0] += 1 if emotion_match else 0
                self._user_emotion_matches[1] += 1
        
        event = {
            "timestamp": event_time.isoformat(),
            "intent_id": intent_id,
            "embedding_dim": len(query_embedding),
            "num_results": len(retrieved_memories),
            "memory_ids": [m.get("memory_id", "unknown") for m in retrieved_memories],
            "memory_emotions": memory_emotions,
            "user_emotion": user_emotion,
            "emotion_match": emotion_match,
            "metadata": metadata or {}
        }
        
        with self._lock:
            # Store in memory buffer
            self._retrievals.append(event)
            
            # Update intent graph if enabled
            if self.intent_graph_enabled and intent_id in self._intent_graph_buffer:
                memory_trace = self._intent_graph_buffer[intent_id]["memory_trace"]
                # Add retrieved memories
                memory_trace["retrieved"] = [
                    {
                        "memory_id": mem.get("memory_id", "unknown"),
                        "quickrecal_score": mem.get("quickrecal_score", 0.0),
                        "dominant_emotion": mem.get("dominant_emotion", None),
                        "emotion_confidence": mem.get("emotion_confidence", 0.0)
                    } for mem in retrieved_memories
                ]
                
                # Add emotion info if available
                if user_emotion or memory_emotions:
                    emo_mod = self._intent_graph_buffer[intent_id]["emotional_modulation"]
                    emo_mod["user_emotion"] = user_emotion
                    if memory_emotions:
                        # Find most frequent emotion
                        from collections import Counter
                        counts = Counter(memory_emotions)
                        dominant = counts.most_common(1)[0][0] if counts else None
                        emo_mod["retrieved_emotion_dominance"] = dominant
                        emo_mod["conflict_flag"] = user_emotion != dominant if user_emotion and dominant else False
                
                # Add reasoning step
                self._intent_graph_buffer[intent_id]["reasoning_steps"].append(
                    f"→ Retrieved {len(retrieved_memories)} memories based on query"
                )
        
        # Optionally log to file
        self._maybe_write_event_log("retrievals", event)
        logger.debug(f"Logged retrieval: {len(retrieved_memories)} memories retrieved")
    
    def _maybe_write_event_log(self, event_type: str, event: Dict[str, Any]) -> None:
        """Write event to log file if logging is enabled."""
        if not self.log_dir:
            return
        
        try:
            log_file = os.path.join(self.log_dir, f"{event_type}.jsonl")
            with open(log_file, "a") as f:
                f.write(json.dumps(event) + "\n")
        except Exception as e:
            logger.warning(f"Failed to write event log: {e}")
    
    def finalize_intent(self, intent_id: Optional[str] = None, 
                       response_text: Optional[str] = None,
                       confidence: Optional[float] = None) -> Optional[Dict[str, Any]]:
        """Finalize the current intent/interaction and return its IntentGraph.
        
        Args:
            intent_id: Optional intent ID (uses current if not provided)
            response_text: Final response text if available
            confidence: Confidence score for the response
            
        Returns:
            Optional[Dict[str, Any]]: The completed IntentGraph or None if not enabled
        """
        intent_id = intent_id or self._current_intent_id
        if not intent_id or not self.intent_graph_enabled:
            return None
        
        with self._lock:
            if intent_id not in self._intent_graph_buffer:
                logger.warning(f"Cannot finalize unknown intent: {intent_id}")
                return None
            
            # Complete the intent graph
            intent_graph = self._intent_graph_buffer[intent_id]
            
            # Add final output
            if response_text:
                intent_graph["final_output"] = {
                    "response_text": response_text,
                    "confidence": confidence,
                    "timestamp": datetime.datetime.utcnow().isoformat()
                }
            
            # Write to file if logging enabled
            if self.log_dir:
                log_file = os.path.join(self.log_dir, "intent_graphs", f"{intent_id}.json")
                os.makedirs(os.path.dirname(log_file), exist_ok=True)
                try:
                    with open(log_file, "w") as f:
                        json.dump(intent_graph, f, indent=2)
                except Exception as e:
                    logger.warning(f"Failed to write intent graph: {e}")
            
            # Remove from buffer to free memory
            graph_copy = intent_graph.copy()
            del self._intent_graph_buffer[intent_id]
            
            logger.info(f"Finalized intent {intent_id} with {len(intent_graph['reasoning_steps'])} reasoning steps")
            return graph_copy
    
    def get_diagnostic_metrics(self, window: str = "last_100", 
                             emotion_filter: Optional[str] = None) -> Dict[str, Any]:
        """Get diagnostic metrics for the emotional feedback loop.
        
        Args:
            window: Time/count window to analyze ("last_100", "last_hour", etc.)
            emotion_filter: Optional filter to specific emotion
            
        Returns:
            Dict[str, Any]: Diagnostic metrics for the emotional feedback loop
        """
        with self._lock:
            # Determine slice of data to analyze based on window
            memory_updates = list(self._memory_updates)
            quickrecal_boosts = list(self._quickrecal_boosts)
            retrievals = list(self._retrievals)
            
            # Filter by time window if needed
            if window.startswith("last_") and window[5:].isdigit():
                # "last_N" format - take last N items
                count = int(window[5:])
                memory_updates = memory_updates[-count:] if len(memory_updates) > count else memory_updates
                quickrecal_boosts = quickrecal_boosts[-count:] if len(quickrecal_boosts) > count else quickrecal_boosts
                retrievals = retrievals[-count:] if len(retrievals) > count else retrievals
            elif window == "last_hour":
                # Last hour - filter by timestamp
                cutoff = datetime.datetime.utcnow() - datetime.timedelta(hours=1)
                cutoff_str = cutoff.isoformat()
                memory_updates = [e for e in memory_updates if e["timestamp"] >= cutoff_str]
                quickrecal_boosts = [e for e in quickrecal_boosts if e["timestamp"] >= cutoff_str]
                retrievals = [e for e in retrievals if e["timestamp"] >= cutoff_str]
            
            # Apply emotion filter if specified
            if emotion_filter and emotion_filter != "all":
                memory_updates = [e for e in memory_updates if e.get("emotion") == emotion_filter]
                quickrecal_boosts = [e for e in quickrecal_boosts if e.get("emotion") == emotion_filter]
            
            # Calculate average metrics
            avg_loss = np.mean([e["loss"] for e in memory_updates]) if memory_updates else 0.0
            avg_grad_norm = np.mean([e["grad_norm"] for e in memory_updates]) if memory_updates else 0.0
            avg_boost = np.mean([e["boost_amount"] for e in quickrecal_boosts]) if quickrecal_boosts else 0.0
            
            # Find dominant emotions boosted
            emotion_boost_counts = defaultdict(float)
            for e in quickrecal_boosts:
                if e.get("emotion"):
                    emotion_boost_counts[e["emotion"]] += e["boost_amount"]
            
            # Sort by boost amount and take top 5
            dominant_emotions = sorted(emotion_boost_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            dominant_emotions = [e[0] for e in dominant_emotions if e[1] > 0]
            
            # Calculate emotion entropy (diversity measure)
            emotion_counts = {k: v for k, v in self._emotion_counts.items() if v > 0}
            total_emotions = sum(emotion_counts.values())
            if total_emotions > 0:
                probs = [count/total_emotions for count in emotion_counts.values()]
                entropy = -sum(p * np.log(p) for p in probs if p > 0)
            else:
                entropy = 0.0
            
            # Calculate user emotion match rate
            match_rate = self._user_emotion_matches[0] / self._user_emotion_matches[1] \
                if self._user_emotion_matches[1] > 0 else 0.0
            
            # Find cluster hotspots (memory IDs with most updates)
            memory_update_counts = defaultdict(int)
            for e in quickrecal_boosts:
                memory_id = e["memory_id"]
                if memory_id:
                    memory_update_counts[memory_id] += 1
            
            # Get top clusters by update count
            cluster_hotspots = sorted(memory_update_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            cluster_hotspots = [{"cluster_id": cid, "updates": count} for cid, count in cluster_hotspots if count > 0]
            
            # Generate alerts based on metrics
            alerts = []
            recommendations = []
            
            # Alerts
            if entropy < 2.0 and total_emotions > 10:
                alerts.append("⚠️ Low emotional diversity detected (entropy < 2.0)")
                recommendations.append("Introduce more varied emotional inputs")
            else:
                alerts.append("✓ Emotional diversity stable.")
                
            if avg_loss > 0.2:
                alerts.append("⚠️ High average loss detected (> 0.2)")
                recommendations.append("Check for instability in memory patterns")
            else:
                alerts.append("✓ Surprise signals healthy.")
                
            if avg_grad_norm > 1.0:
                alerts.append("⚠️ High average gradient norm (> 1.0)")
                recommendations.append("Consider reducing learning rate or checking for oscillations")
            elif avg_grad_norm > 0.5:
                alerts.append("ℹ️ Grad norm average slightly elevated.")
                recommendations.append("Monitor grad norm trend.")
            
            if match_rate < 0.5 and self._user_emotion_matches[1] > 10:
                alerts.append("⚠️ Low user emotion match rate (< 50%)")
                recommendations.append("Review emotional alignment in retrieval process")
            
            # Add generic recommendation if list is empty
            if not recommendations:
                recommendations.append("Continue monitoring with current settings")
            
            # Calculate emotion bias index (0 = balanced, 1 = highly biased)
            if len(emotion_counts) > 1 and total_emotions > 0:
                max_count = max(emotion_counts.values())
                emotion_bias = (max_count / total_emotions) * (1 - 1/len(emotion_counts))
            else:
                emotion_bias = 0.0
            
            return {
                "diagnostic_window": window,
                "avg_loss": float(avg_loss),
                "avg_grad_norm": float(avg_grad_norm),
                "avg_quickrecal_boost": float(avg_boost),
                "dominant_emotions_boosted": dominant_emotions,
                "emotional_entropy": float(entropy),
                "emotion_bias_index": float(emotion_bias),
                "user_emotion_match_rate": float(match_rate),
                "cluster_update_hotspots": cluster_hotspots,
                "alerts": alerts,
                "recommendations": recommendations,
                "data_points": {
                    "memory_updates": len(memory_updates),
                    "quickrecal_boosts": len(quickrecal_boosts),
                    "retrievals": len(retrievals)
                }
            }
    
    def format_diagnostics_as_table(self, diagnostics: Dict[str, Any]) -> str:
        """Format diagnostics as an ASCII table for CLI output.
        
        Args:
            diagnostics: Diagnostics data from get_diagnostic_metrics()
            
        Returns:
            str: Formatted ASCII table
        """
        width = 80
        
        # Helper to create a section line
        def section(title):
            return f"\n{title.center(width, '=')}\n"
        
        # Ensure diagnostics dict has all expected keys with defaults
        defaults = {
            'diagnostic_window': 'Unknown',
            'avg_loss': 0.0,
            'avg_grad_norm': 0.0,
            'avg_quickrecal_boost': 0.0,
            'emotional_entropy': 0.0,
            'emotion_bias_index': 0.0,
            'user_emotion_match_rate': 0.0,
            'dominant_emotions_boosted': [],
            'cluster_update_hotspots': [],
            'alerts': [],
            'recommendations': [], 
            'data_points': {
                'memory_updates': 0,
                'quickrecal_boosts': 0,
                'retrievals': 0
            }
        }
        
        # Fill in any missing keys with defaults
        for key, default_value in defaults.items():
            if key not in diagnostics:
                diagnostics[key] = default_value
                # Only log warning for non-standard missing keys
                if key != 'data_points':  # data_points is commonly missing and handled with defaults
                    logger.warning(f"Missing key '{key}' in diagnostics, using default value")
                
        # Ensure data_points has all expected keys
        if 'data_points' not in diagnostics:
            diagnostics['data_points'] = {}
            
        for key, default_value in defaults['data_points'].items():
            if key not in diagnostics['data_points']:
                diagnostics['data_points'][key] = default_value
                # Only log warning for non-standard missing keys at debug level
                logger.debug(f"Missing key '{key}' in data_points, using default value")
        
        # Header
        output = []
        output.append("=" * width)
        output.append(f"LUCIDIA COGNITIVE DIAGNOSTICS: {diagnostics['diagnostic_window']}".center(width))
        output.append(f"[{datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}]".center(width))
        output.append("=" * width)
        
        # Core metrics
        output.append(section("CORE METRICS"))
        metrics = [
            ("Average Loss", f"{diagnostics['avg_loss']:.4f}"),
            ("Average Grad Norm", f"{diagnostics['avg_grad_norm']:.4f}"),
            ("Average QuickRecal Boost", f"{diagnostics['avg_quickrecal_boost']:.4f}"),
            ("Emotional Entropy", f"{diagnostics['emotional_entropy']:.2f}"),
            ("Emotion Bias Index", f"{diagnostics['emotion_bias_index']:.2f}"),
            ("User Emotion Match Rate", f"{diagnostics['user_emotion_match_rate']:.2%}")
        ]
        
        # Format metrics as two columns
        for i in range(0, len(metrics), 2):
            if i+1 < len(metrics):
                col1 = f"{metrics[i][0]}: {metrics[i][1]}"
                col2 = f"{metrics[i+1][0]}: {metrics[i+1][1]}"
                output.append(f"{col1.ljust(40)} | {col2.ljust(38)}")
            else:
                output.append(f"{metrics[i][0]}: {metrics[i][1]}")
        
        # Dominant emotions
        output.append(section("EMOTION ANALYSIS"))
        if diagnostics['dominant_emotions_boosted']:
            output.append("Dominant Boosted Emotions: " + ", ".join(diagnostics['dominant_emotions_boosted']))
        else:
            output.append("Dominant Boosted Emotions: None detected")
        
        # Cluster hotspots
        output.append(section("MEMORY HOTSPOTS"))
        if diagnostics['cluster_update_hotspots']:
            for hotspot in diagnostics['cluster_update_hotspots']:
                hotspot_id = hotspot.get('cluster_id', 'Unknown')
                updates = hotspot.get('updates', 0)
                output.append(f"* {hotspot_id}: {updates} updates")
        else:
            output.append("No significant memory hotspots detected")
        
        # Alerts and recommendations
        output.append(section("ALERTS"))
        for alert in diagnostics['alerts']:
            output.append(f"* {alert}")
        
        output.append(section("RECOMMENDATIONS"))
        for rec in diagnostics['recommendations']:
            output.append(f"* {rec}")
        
        # Data summary
        data_points = diagnostics.get('data_points', {})
        output.append(section("DATA SUMMARY"))
        output.append(f"Based on {data_points.get('memory_updates', 0)} updates, {data_points.get('quickrecal_boosts', 0)} boosts, and {data_points.get('retrievals', 0)} retrievals")
        output.append("=" * width)  
        
        return "\n".join(output)
    
# --- Global Instance ---
metrics_store = None

def get_metrics_store() -> MetricsStore:
    """Get or initialize the global MetricsStore instance."""
    global metrics_store
    if metrics_store is None:
        # Create log directory in the current directory
        log_dir = os.path.join(os.path.dirname(__file__), "logs")
        metrics_store = MetricsStore(log_dir=log_dir)
        logger.info("Global MetricsStore initialized")
    return metrics_store

```

# synthians_trainer_server\neural_memory.py

```py
# synthians_trainer_server/neural_memory.py

import tensorflow as tf
import numpy as np
import json
import os
import logging
from typing import Dict, Any, Optional, List, Tuple, Union, TYPE_CHECKING
from enum import Enum # Import Enum
import datetime

# Ensure TensorFlow uses float32 by default
tf.keras.backend.set_floatx('float32')
logger = logging.getLogger(__name__)

# --- Configuration Class ---
class NeuralMemoryConfig(dict):
    """Configuration for the NeuralMemoryModule."""
    def __init__(self, *args, **kwargs):
        defaults = {
            "input_dim": 768,
            "key_dim": 128,
            "value_dim": 768,
            "query_dim": 128,
            "memory_hidden_dims": [512],
            "gate_hidden_dims": [64],
            "alpha_init": -2.0,
            "theta_init": -3.0, # Controls inner loop LR
            "eta_init": 2.0,
            "outer_learning_rate": 1e-4,
            "use_complex_gates": False
        }
        config = defaults.copy()
        # Apply kwargs first
        config.update(kwargs)
        # Then apply dict from args if provided
        if args and isinstance(args[0], dict):
            config.update(args[0])

        super().__init__(config)
        # Ensure integer dimensions after all updates
        for key in ["input_dim", "key_dim", "value_dim", "query_dim"]:
            if key in self: self[key] = int(self[key])
        if "memory_hidden_dims" in self:
            self["memory_hidden_dims"] = [int(d) for d in self["memory_hidden_dims"]]
        if "gate_hidden_dims" in self:
            self["gate_hidden_dims"] = [int(d) for d in self["gate_hidden_dims"]]

    # Allow attribute access (though we avoid relying on it internally now)
    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(f"'NeuralMemoryConfig' object has no attribute '{key}'")

    def __setattr__(self, key, value):
        self[key] = value


# --- Core Memory MLP ---
class MemoryMLP(tf.keras.layers.Layer):
    """The core MLP model (M) used for associative memory."""
    def __init__(self, key_dim, value_dim, hidden_dims, name="MemoryMLP", **kwargs):
        super().__init__(name=name, **kwargs)
        self.key_dim = int(key_dim)
        self.value_dim = int(value_dim)
        self.hidden_dims = [int(d) for d in hidden_dims]
        
        # Create layers in __init__ as instance attributes so they're properly tracked
        self.hidden_layers = []
        for i, units in enumerate(self.hidden_dims):
            self.hidden_layers.append(
                tf.keras.layers.Dense(
                    units, 
                    activation='relu',
                    name=f"mem_hidden_{i+1}"
                )
            )
        
        # Output Layer
        self.output_layer = tf.keras.layers.Dense(self.value_dim, name="mem_output")

    def build(self, input_shape):
        # input_shape is expected to be [batch_size, key_dim]
        shape = tf.TensorShape(input_shape)
        last_dim = shape[-1]
        if last_dim is None:
             raise ValueError(f"Input dimension must be defined for {self.name}. Received shape: {input_shape}")
        if last_dim != self.key_dim:
             logger.warning(f"{self.name} input shape last dim {last_dim} != config key_dim {self.key_dim}. Ensure config matches data.")

        # Build all layers with explicit input shapes
        current_shape = shape
        for layer in self.hidden_layers:
            layer.build(current_shape)
            current_shape = layer.compute_output_shape(current_shape)
            
        # Build output layer
        self.output_layer.build(current_shape)
        
        # Call super build to ensure proper tracking
        super().build(input_shape)
        logger.info(f"{self.name} built successfully with input shape {input_shape}. Found {len(self.trainable_variables)} trainable vars.")

    def call(self, inputs, training=None):
        x = inputs
        # Pass through hidden layers
        for layer in self.hidden_layers:
            x = layer(x, training=training)
        # Pass through output layer
        return self.output_layer(x, training=training)

    def get_config(self):
        config = super().get_config()
        config.update({"key_dim": self.key_dim, "value_dim": self.value_dim, "hidden_dims": self.hidden_dims})
        return config

# --- Neural Memory Module ---
class NeuralMemoryModule(tf.keras.Model):
    """
    Implements the Titans Neural Memory module that learns at test time.
    Inherits from tf.keras.Model for easier weight management and saving.
    """
    def __init__(self, config: Optional[Union[NeuralMemoryConfig, Dict]] = None, **kwargs):
        super().__init__(**kwargs)
        if isinstance(config, dict) or config is None: self.config = NeuralMemoryConfig(**(config or {}))
        elif isinstance(config, NeuralMemoryConfig): self.config = config
        else: raise TypeError("config must be a dict or NeuralMemoryConfig")

        logger.info(f"Initializing NeuralMemoryModule with config: {dict(self.config)}")

        # --- Outer Loop Parameters ---
        initializer_outer = tf.keras.initializers.GlorotUniform()
        key_dim, value_dim, query_dim, input_dim = self.config['key_dim'], self.config['value_dim'], self.config['query_dim'], self.config['input_dim']

        self.WK_layer = tf.keras.layers.Dense(key_dim, name="WK_proj", use_bias=False, kernel_initializer=initializer_outer)
        self.WV_layer = tf.keras.layers.Dense(value_dim, name="WV_proj", use_bias=False, kernel_initializer=initializer_outer)
        self.WQ_layer = tf.keras.layers.Dense(query_dim, name="WQ_proj", use_bias=False, kernel_initializer=initializer_outer)
        
        # Initialize gate projection layers for MAG variant (used by calculate_gates)
        self.attention_to_alpha = tf.keras.layers.Dense(1, name="attention_to_alpha")
        self.attention_to_theta = tf.keras.layers.Dense(1, name="attention_to_theta")
        self.attention_to_eta = tf.keras.layers.Dense(1, name="attention_to_eta")
        
        # Storage for last computed gate values
        self.last_applied_gates = {}

        if not self.config.get('use_complex_gates', False):
            self.alpha_logit = tf.Variable(tf.constant(self.config['alpha_init'], dtype=tf.float32), name="alpha_logit", trainable=True)
            self.theta_logit = tf.Variable(tf.constant(self.config['theta_init'], dtype=tf.float32), name="theta_logit", trainable=True)
            self.eta_logit = tf.Variable(tf.constant(self.config['eta_init'], dtype=tf.float32), name="eta_logit", trainable=True)
            self._gate_params = [self.alpha_logit, self.theta_logit, self.eta_logit]
        else:
            logger.warning("Complex gates not implemented, using simple scalar gates.")
            self.alpha_logit = tf.Variable(tf.constant(self.config['alpha_init'], dtype=tf.float32), name="alpha_logit", trainable=True)
            self.theta_logit = tf.Variable(tf.constant(self.config['theta_init'], dtype=tf.float32), name="theta_logit", trainable=True)
            self.eta_logit = tf.Variable(tf.constant(self.config['eta_init'], dtype=tf.float32), name="eta_logit", trainable=True)
            self._gate_params = [self.alpha_logit, self.theta_logit, self.eta_logit]

        # --- Inner Loop Parameters (Memory Model M) ---
        self.memory_mlp = MemoryMLP(
            key_dim=key_dim, value_dim=value_dim, hidden_dims=self.config['memory_hidden_dims'], name="MemoryMLP"
        )
        # --- Force build with a defined input shape ---
        # Create a dummy input tensor with batch size 1 and correct key_dim
        dummy_mlp_input = tf.TensorSpec(shape=[1, key_dim], dtype=tf.float32)
        # Build the MLP now
        self.memory_mlp.build(dummy_mlp_input.shape)
        # Verify build
        if not self.memory_mlp.built:
             logger.error("MemoryMLP failed to build during init!")
        self._inner_trainable_variables = self.memory_mlp.trainable_variables
        logger.info(f"MemoryMLP built. Trainable variables: {len(self._inner_trainable_variables)}")
        if not self._inner_trainable_variables: logger.error("MemoryMLP has NO trainable variables!")

        # --- Momentum State ---
        self.momentum_state = [
            tf.Variable(tf.zeros_like(var), trainable=False, name=f"momentum_{i}")
            for i, var in enumerate(self._inner_trainable_variables)
        ]
        logger.info(f"Momentum state variables created: {len(self.momentum_state)}")

        # --- Optimizer for Outer Loop ---
        self.outer_optimizer = tf.keras.optimizers.Adam(learning_rate=self.config['outer_learning_rate'])

        # Build projection layers
        self.WK_layer.build(input_shape=(None, input_dim))
        self.WV_layer.build(input_shape=(None, input_dim))
        self.WQ_layer.build(input_shape=(None, input_dim))
        logger.info("Projection layers built.")

        # Build gate projection layers
        self.attention_to_alpha.build(input_shape=(None, query_dim))
        self.attention_to_theta.build(input_shape=(None, query_dim))
        self.attention_to_eta.build(input_shape=(None, query_dim))
        logger.info("Gate projection layers built.")

    @property
    def inner_trainable_variables(self):
        return self.memory_mlp.trainable_variables

    @property
    def outer_trainable_variables(self):
         return self.WK_layer.trainable_variables + \
                self.WV_layer.trainable_variables + \
                self.WQ_layer.trainable_variables + \
                self.attention_to_alpha.trainable_variables + \
                self.attention_to_theta.trainable_variables + \
                self.attention_to_eta.trainable_variables + \
                self._gate_params

    def get_projections(self, x_t: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        """Calculate key, value, query projections from input.
        
        Args:
            x_t: Input tensor with shape [batch_size, input_dim]
            
        Returns:
            Tuple of (key_projection, value_projection, query_projection)
        """
        x_t = tf.convert_to_tensor(x_t, dtype=tf.float32)
        
        # Ensure input has right shape
        if len(tf.shape(x_t)) == 1:
            # Add batch dimension if missing
            x_t = tf.expand_dims(x_t, 0)
            
        # Get projections
        k_t = self.WK_layer(x_t)  # [batch_size, key_dim]
        v_t = self.WV_layer(x_t)  # [batch_size, value_dim]
        q_t = self.WQ_layer(x_t)  # [batch_size, query_dim]
        
        return k_t, v_t, q_t
    
    def calculate_gates(self, attention_output) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        """Calculate gate values from attention output for MAG variant.
        
        Args:
            attention_output: Output tensor from attention mechanism
            
        Returns:
            Tuple of (alpha_t, theta_t, eta_t) gate values
        """
        # Default gates (fallback if computation fails)
        alpha_logit = self.alpha_logit
        theta_logit = self.theta_logit
        eta_logit = self.eta_logit
        
        try:
            # Ensure attention_output has the right shape
            attention_output = tf.convert_to_tensor(attention_output, dtype=tf.float32)
            if len(tf.shape(attention_output)) == 1:
                attention_output = tf.expand_dims(attention_output, 0)
            
            # Project attention output to gate logits using dedicated layers
            alpha_logit = self.attention_to_alpha(attention_output)
            theta_logit = self.attention_to_theta(attention_output)
            eta_logit = self.attention_to_eta(attention_output)
            
            # Remove the extra dimensions
            alpha_logit = tf.squeeze(alpha_logit)
            theta_logit = tf.squeeze(theta_logit)
            eta_logit = tf.squeeze(eta_logit)
            
            logger.debug(f"Calculated gate logits from attention: alpha={alpha_logit.numpy()}, theta={theta_logit.numpy()}, eta={eta_logit.numpy()}")
            
        except Exception as e:
            logger.warning(f"Error calculating gates from attention output: {e}. Using default gates.")
        
        # Apply sigmoid to get gate values in [0,1] range
        alpha_t = tf.nn.sigmoid(alpha_logit)  # Forget rate
        theta_t = tf.nn.sigmoid(theta_logit)  # Inner learning rate
        eta_t = tf.nn.sigmoid(eta_logit)      # Momentum
        
        return alpha_t, theta_t, eta_t

    def __call__(self, q_t: tf.Tensor, training=False):
        """Retrieve value from memory given query q_t (inference only)."""
        # Ensure q_t has correct shape with batch dimension
        q_t = tf.convert_to_tensor(q_t, dtype=tf.float32)
        if len(tf.shape(q_t)) == 1:
            q_t = tf.expand_dims(q_t, 0)  # Add batch dim
            
        return self.memory_mlp(q_t, training=training)

    def update_step(self, x_t: tf.Tensor, 
                   external_k_t: Optional[tf.Tensor] = None,
                   external_v_t: Optional[tf.Tensor] = None,
                   external_alpha_t: Optional[float] = None,
                   external_theta_t: Optional[float] = None,
                   external_eta_t: Optional[float] = None) -> Tuple[tf.Tensor, List[tf.Tensor]]:
        """Update memory weights based on input x_t.
        
        Args:
            x_t: Input tensor with shape [batch_size, input_dim]
            external_k_t: Optional external key projection (MAL variant)
            external_v_t: Optional external value projection (MAL variant)
            external_alpha_t: Optional external alpha gate (MAG variant - single value)
            external_theta_t: Optional external theta gate (MAG variant - single value)
            external_eta_t: Optional external eta gate (MAG variant - single value)
            
        Returns:
            Tuple of (loss, gradients)
        """
        # Ensure x_t has correct shape with batch dimension
        x_t = tf.convert_to_tensor(x_t, dtype=tf.float32)
        if len(tf.shape(x_t)) == 1:
            x_t = tf.expand_dims(x_t, 0)  # Add batch dim
        
        # Get projections if not provided externally
        if external_k_t is None or external_v_t is None:
            k_t, v_t, _ = self.get_projections(x_t)
            k_t = external_k_t if external_k_t is not None else k_t
            v_t = external_v_t if external_v_t is not None else v_t
        else:
            # Both projections provided externally
            k_t, v_t = external_k_t, external_v_t
        
        # Determine gate values - use externals if provided, otherwise use defaults
        alpha_t = tf.convert_to_tensor(external_alpha_t, dtype=tf.float32) if external_alpha_t is not None else tf.nn.sigmoid(self.alpha_logit)  # Forget rate
        theta_t = tf.convert_to_tensor(external_theta_t, dtype=tf.float32) if external_theta_t is not None else tf.nn.sigmoid(self.theta_logit)  # Inner learning rate
        eta_t = tf.convert_to_tensor(external_eta_t, dtype=tf.float32) if external_eta_t is not None else tf.nn.sigmoid(self.eta_logit)      # Momentum
        
        # Log gate values for debugging
        logger.debug(f"Applied gate values - alpha_t: {float(alpha_t.numpy()) if hasattr(alpha_t, 'numpy') else float(alpha_t)}, "
                  f"theta_t: {float(theta_t.numpy()) if hasattr(theta_t, 'numpy') else float(theta_t)}, "
                  f"eta_t: {float(eta_t.numpy()) if hasattr(eta_t, 'numpy') else float(eta_t)}")
        
        # Store the applied gates for downstream monitoring
        self.last_applied_gates = {
            "alpha_t": float(alpha_t.numpy()) if hasattr(alpha_t, 'numpy') else float(alpha_t),
            "theta_t": float(theta_t.numpy()) if hasattr(theta_t, 'numpy') else float(theta_t),
            "eta_t": float(eta_t.numpy()) if hasattr(eta_t, 'numpy') else float(eta_t)
        }
        
        # --- Gradient Calculation using GradientTape ---
        inner_vars = self.inner_trainable_variables
        with tf.GradientTape() as tape:
            # Forward pass through memory MLP
            predicted_v_t = self.memory_mlp(k_t, training=True) # Use k_t here
            # Calculate loss using potentially modified v_t (from MAL or original)
            loss = 0.5 * tf.reduce_mean(tf.square(predicted_v_t - v_t))

        grads = tape.gradient(loss, inner_vars)
        # --- End Gradient Calculation ---

        # --- Momentum and Weight Updates ---
        valid_grads_indices = [i for i, g in enumerate(grads) if g is not None]
        if len(valid_grads_indices) != len(inner_vars):
            logger.warning(f"Found {len(inner_vars) - len(valid_grads_indices)} None gradients in inner loop.")

        for i in valid_grads_indices:
            grad = grads[i]
            s_var = self.momentum_state[i]
            # Update momentum state
            s_new = eta_t * s_var - theta_t * grad
            s_var.assign(s_new)

        for i in valid_grads_indices:
            s_t = self.momentum_state[i]
            m_var = inner_vars[i]
            # Update memory weights
            m_new = (1.0 - alpha_t) * m_var + s_t
            m_var.assign(m_new)
        # --- End Updates ---

        return loss, grads # Return original grads list (may contain None)

    # Inner loop update step - NO @tf.function for now
    def train_step(self, data):
        input_sequence, target_sequence = data
        
        # Ensure memory_mlp has trainable variables
        if not self.memory_mlp.trainable_variables:
            logger.warning("No trainable variables in memory_mlp during train_step. Attempting to rebuild...")
            dummy_key = tf.zeros((1, self.config['key_dim']), dtype=tf.float32)
            _ = self.memory_mlp(dummy_key)  # Force model execution
        
        # Store initial state
        initial_memory_weights = [tf.identity(v) for v in self.memory_mlp.trainable_variables]
        initial_momentum_state = [tf.identity(s) for s in self.momentum_state]
        
        # Get sequence dimensions
        batch_size = tf.shape(input_sequence)[0]
        seq_len = tf.shape(input_sequence)[1]
        total_outer_loss = tf.constant(0.0, dtype=tf.float32)

        # Get outer trainable variables to track
        outer_vars = self.outer_trainable_variables # Get current list

        with tf.GradientTape() as tape:
            # Explicitly watch outer variables
            for var in outer_vars:
                tape.watch(var)

            # Reset inner memory and momentum state
            for i, var in enumerate(self.memory_mlp.trainable_variables):
                var.assign(tf.zeros_like(var))
            for i, s_var in enumerate(self.momentum_state):
                s_var.assign(tf.zeros_like(s_var))

            # Process sequence
            for t in tf.range(seq_len):
                x_t_batch = input_sequence[:, t, :]
                target_t_batch = target_sequence[:, t, :]

                # Generate predictions (use projection layers - outer params)
                _, _, q_t_batch = self.get_projections(x_t_batch)
                retrieved_y_t_batch = self(q_t_batch, training=False) # Uses memory_mlp - inner params

                # Compute loss against target
                tf.debugging.assert_equal(tf.shape(retrieved_y_t_batch)[-1], tf.shape(target_t_batch)[-1], 
                                          message="Outer loss target dim mismatch")
                step_loss = tf.reduce_mean(tf.square(retrieved_y_t_batch - target_t_batch))
                total_outer_loss += step_loss

                # Inner update loop - process one example at a time for now
                # This is inefficient for batch>1 but ensures correct updates
                for b in tf.range(batch_size):
                    x_t = tf.expand_dims(x_t_batch[b], axis=0)
                    _, _ = self.update_step(x_t)  # Apply inner loop update

        # Check validity of outer vars
        valid_outer_vars = [v for v in outer_vars if v is not None]
        if len(valid_outer_vars) < len(outer_vars):
            logger.warning(f"Found {len(outer_vars) - len(valid_outer_vars)} None variables in outer_vars!")
        
        # Calculate outer gradients
        outer_grads = tape.gradient(total_outer_loss, valid_outer_vars)
        
        # Check for None gradients in outer loop
        none_grads = sum(1 for g in outer_grads if g is None)
        if none_grads > 0:
            logger.warning(f"Found {none_grads} None gradients in outer loop.")

        # Apply outer gradients
        non_none_grads = []
        non_none_vars = []
        for i, (grad, var) in enumerate(zip(outer_grads, valid_outer_vars)):
            if grad is not None:
                non_none_grads.append(grad)
                non_none_vars.append(var)
        
        # Apply valid gradients only
        if non_none_grads:
            self.outer_optimizer.apply_gradients(zip(non_none_grads, non_none_vars))
        
        # Restore original memory state
        for i, var in enumerate(self.memory_mlp.trainable_variables):
            if i < len(initial_memory_weights):
                var.assign(initial_memory_weights[i])
                
        for i, s_var in enumerate(self.momentum_state):
            if i < len(initial_momentum_state):
                s_var.assign(initial_momentum_state[i])

        return {"loss": total_outer_loss / tf.cast(seq_len, dtype=tf.float32)}

    # --- Persistence ---
    def save_state(self, path: str) -> None:
        if path.startswith("file://"): path = path[7:]
        os.makedirs(os.path.dirname(path), exist_ok=True)
        try:
            state = {
                "config": self.get_config_dict(),
                "inner_weights": {v.name: v.numpy().tolist() for v in self.inner_trainable_variables},
                "outer_weights": {v.name: v.numpy().tolist() for v in self.outer_trainable_variables},
                "momentum_state": {s.name: s.numpy().tolist() for s in self.momentum_state},
                "timestamp": datetime.datetime.now().isoformat(),
            }
            
            with open(path, 'w') as f:
                json.dump(state, f, indent=2)
            logger.info(f"Neural Memory state saved to {path}")
        except Exception as e:
            logger.error(f"Error saving Neural Memory state: {e}", exc_info=True)
            raise

    def load_state(self, path: str) -> bool:
        if path.startswith("file://"): path = path[7:]
        if not os.path.exists(path): 
            logger.error(f"State file not found: {path}")
            return False

        logger.info(f"Loading Neural Memory state from {path}")
        try:
            with open(path, 'r') as f: 
                state = json.load(f)
                
            loaded_config_dict = state.get("config")
            if not loaded_config_dict: 
                logger.error("State missing 'config'")
                return False

            # Check if we need to re-initialize with the loaded config
            current_config_dict = self.get_config_dict()
            config_changed = current_config_dict != loaded_config_dict
            if config_changed:
                logger.warning(f"Loaded config differs from current config")
                # We don't attempt to rebuild the model here - that needs to be done externally
                # Just log a warning that configs don't match

            # Load inner weights (memory model)
            inner_weights_loaded = state.get("inner_weights", {})
            inner_vars_dict = {v.name: v for v in self.inner_trainable_variables}
            loaded_count = 0
            for name, loaded_list in inner_weights_loaded.items():
                if name in inner_vars_dict:
                    var = inner_vars_dict[name]
                    loaded_val = tf.convert_to_tensor(loaded_list, dtype=tf.float32)
                    if var.shape == loaded_val.shape:
                        var.assign(loaded_val)
                        loaded_count += 1
                    else: 
                        logger.error(f"Shape mismatch loading inner var {name}: {var.shape} vs {loaded_val.shape}")
                else: 
                    logger.warning(f"Inner var {name} not in current model.")
            logger.info(f"Loaded {loaded_count} inner weights.")

            # Load outer weights (projection layers)
            outer_weights_loaded = state.get("outer_weights", {})
            outer_vars_dict = {v.name: v for v in self.outer_trainable_variables}
            loaded_count = 0
            for name, loaded_list in outer_weights_loaded.items():
                if name in outer_vars_dict:
                    var = outer_vars_dict[name]
                    loaded_val = tf.convert_to_tensor(loaded_list, dtype=tf.float32)
                    if var.shape == loaded_val.shape:
                        var.assign(loaded_val)
                        loaded_count += 1
                    else: 
                        logger.error(f"Shape mismatch loading outer var {name}: {var.shape} vs {loaded_val.shape}")
                else: 
                    logger.warning(f"Outer var {name} not in current model.")
            logger.info(f"Loaded {loaded_count} outer weights.")

            # Load momentum state
            momentum_loaded = state.get("momentum_state", {})
            # Rebuild momentum state if needed (without calling assign directly)
            if len(self.momentum_state) != len(self.inner_trainable_variables):
                logger.warning("Momentum state size doesn't match inner vars. Creating new state.")
                # Create new momentum variables without assigning to self yet
                new_momentum = []
                for i, var in enumerate(self.inner_trainable_variables):
                    new_momentum.append(tf.Variable(tf.zeros_like(var), trainable=False, name=f"momentum_{i}"))
                # Now replace the list (safer than assigning individual vars)
                self.momentum_state = new_momentum

            loaded_count = 0
            mom_vars_dict = {v.name: v for v in self.momentum_state}
            for name, loaded_list in momentum_loaded.items():
                if name in mom_vars_dict:
                    var = mom_vars_dict[name]
                    loaded_val = tf.convert_to_tensor(loaded_list, dtype=tf.float32)
                    if var.shape == loaded_val.shape:
                        var.assign(loaded_val)
                        loaded_count += 1
                    else: 
                        logger.error(f"Shape mismatch loading momentum var {name}: {var.shape} vs {loaded_val.shape}")
                else: 
                    logger.warning(f"Momentum var {name} not in current model.")
            logger.info(f"Loaded {loaded_count} momentum states.")

            logger.info(f"Neural Memory state successfully loaded from {path}")
            return True
            
        except json.JSONDecodeError as e:
             logger.error(f"Error decoding JSON state file {path}: {e}")
             return False
        except Exception as e:
            logger.error(f"Error loading Neural Memory state: {e}", exc_info=True)
            return False

    def get_config_dict(self) -> Dict:
         """Return config as a serializable dict."""
         # Convert Enum members to strings if necessary
         serializable_config = {}
         for k, v in self.config.items():
              serializable_config[k] = v.value if isinstance(v, Enum) else v
         return serializable_config
```

# synthians_trainer_server\surprise_detector.py

```py
import numpy as np
# Remove tensorflow dependency - use only numpy for vector operations
# import tensorflow as tf
from typing import List, Dict, Any, Optional, Union, Tuple
import logging
from synthians_memory_core.geometry_manager import GeometryManager, GeometryType

logger = logging.getLogger(__name__)

class SurpriseDetector:
    """Detects surprising patterns in embedding sequences.
    
    This class analyzes semantic shifts in embeddings to identify moments that
    break the expected narrative flow, enabling the system to recognize pattern
    discontinuities and meaningful context shifts.
    """
    
    def __init__(self, 
                 geometry_manager: Optional[GeometryManager] = None,
                 surprise_threshold: float = 0.6,
                 max_sequence_length: int = 10,
                 surprise_decay: float = 0.9):
        """Initialize the surprise detector.
        
        Args:
            geometry_manager: Shared GeometryManager instance for consistent vector operations
            surprise_threshold: Threshold above which an embedding is considered surprising (0-1)
            max_sequence_length: Maximum number of recent embeddings to track
            surprise_decay: Decay factor for historical surprise (0-1)
        """
        # Use provided GeometryManager or create a default one
        self.geometry_manager = geometry_manager or GeometryManager()
        self.surprise_threshold = surprise_threshold
        self.max_sequence_length = max_sequence_length
        self.surprise_decay = surprise_decay
        
        # Internal memory of recent embeddings
        self.recent_embeddings: List[np.ndarray] = []
        self.recent_surprises: List[float] = []
        
        # Adaptive threshold tracking
        self.min_surprise_seen = 1.0
        self.max_surprise_seen = 0.0
        self.surprise_history: List[float] = []
        
        logger.info(f"SurpriseDetector initialized with geometry type: {self.geometry_manager.config['geometry_type']}")
    
    def _standardize_embedding(self, embedding: Union[List[float], np.ndarray]) -> np.ndarray:
        """Standardize an embedding to a normalized numpy array.
        
        Args:
            embedding: Input embedding
            
        Returns:
            Normalized numpy array
        """
        # Use the public method now
        return self.geometry_manager.normalize_embedding(embedding)
    
    def calculate_surprise(self, 
                           predicted_embedding: Union[List[float], np.ndarray],
                           actual_embedding: Union[List[float], np.ndarray]) -> Dict[str, Any]:
        """Calculate surprise between predicted and actual embeddings.
        
        Args:
            predicted_embedding: The embedding predicted by the trainer
            actual_embedding: The actual embedding observed
            
        Returns:
            Dictionary with surprise metrics
        """
        # Standardize inputs using GeometryManager
        pred_vec = self.geometry_manager.normalize_embedding(predicted_embedding)
        actual_vec = self.geometry_manager.normalize_embedding(actual_embedding)
        
        # Calculate similarity using GeometryManager
        similarity = self.geometry_manager.calculate_similarity(pred_vec, actual_vec)
        
        # Calculate surprise (1 - cosine similarity, rescaled to 0-1)
        cosine_surprise = (1.0 - similarity) / 2.0
        
        # Calculate delta vector (using GeometryManager for any needed alignment)
        aligned_pred, aligned_actual = self.geometry_manager.align_vectors(pred_vec, actual_vec)
        delta_vec = aligned_actual - aligned_pred
        delta_norm = float(np.linalg.norm(delta_vec))
        
        # Calculate context shift by comparing to recent embeddings
        context_surprise = 0.0
        if len(self.recent_embeddings) > 0:
            # Calculate average similarity to recent embeddings using GeometryManager
            similarities = [self.geometry_manager.calculate_similarity(actual_vec, e) for e in self.recent_embeddings]
            avg_similarity = sum(similarities) / len(similarities)
            context_surprise = (1.0 - avg_similarity) / 2.0
        
        # Combine surprise metrics (weighted average)
        prediction_weight = 0.7  # Weight for prediction error
        context_weight = 0.3     # Weight for context shift
        
        total_surprise = (prediction_weight * cosine_surprise + 
                          context_weight * context_surprise)
        
        # Update surprise history
        self.surprise_history.append(total_surprise)
        if len(self.surprise_history) > 100:  # Keep history manageable
            self.surprise_history = self.surprise_history[-100:]
            
        # Update min/max tracking for adaptive thresholds
        self.min_surprise_seen = min(self.min_surprise_seen, total_surprise)
        self.max_surprise_seen = max(self.max_surprise_seen, total_surprise)
        
        # Update recent embeddings memory
        self.recent_embeddings.append(actual_vec)
        if len(self.recent_embeddings) > self.max_sequence_length:
            self.recent_embeddings = self.recent_embeddings[-self.max_sequence_length:]
            
        # Update recent surprises
        self.recent_surprises.append(total_surprise)
        if len(self.recent_surprises) > self.max_sequence_length:
            self.recent_surprises = self.recent_surprises[-self.max_sequence_length:]
        
        # Calculate adaptive threshold
        if len(self.surprise_history) >= 10:
            mean_surprise = np.mean(self.surprise_history)
            std_surprise = np.std(self.surprise_history)
            adaptive_threshold = mean_surprise + std_surprise
        else:
            adaptive_threshold = self.surprise_threshold
            
        # Determine if this is surprising
        is_surprising = total_surprise > adaptive_threshold
        
        # Calculate surprise volatility (how much does surprise vary?)
        if len(self.recent_surprises) >= 3:
            volatility = float(np.std(self.recent_surprises))
        else:
            volatility = 0.0
            
        return {
            "surprise": float(total_surprise),
            "cosine_surprise": float(cosine_surprise),
            "context_surprise": float(context_surprise),
            "delta_norm": delta_norm,
            "is_surprising": is_surprising,
            "adaptive_threshold": float(adaptive_threshold),
            "volatility": float(volatility),
            "delta": delta_vec.tolist()
        }
    
    def calculate_quickrecal_boost(self, surprise_metrics: Dict[str, Any]) -> float:
        """Calculate how much to boost a memory's quickrecal score based on surprise.
        
        Args:
            surprise_metrics: Output from calculate_surprise method
            
        Returns:
            QuickRecal score boost (0-1 range)
        """
        # Extract metrics
        total_surprise = surprise_metrics["surprise"]
        is_surprising = surprise_metrics["is_surprising"]
        volatility = surprise_metrics["volatility"]
        
        # Base multiplier depends on whether it's actually surprising
        if not is_surprising:
            return 0.0
            
        # Scale boost based on how surprising it is
        # Apply sigmoid scaling to make boost more aggressive for very surprising items
        def sigmoid(x):
            return 1 / (1 + np.exp(-10 * (x - 0.5)))
            
        # Apply sigmoid scaling to boost (0.5-1.0 range becomes steeper)
        scaled_surprise = sigmoid(total_surprise)
        
        # Incorporate volatility - higher volatility increases the boost
        # Max volatility boost is 1.5x
        volatility_multiplier = 1.0 + (volatility * 0.5)
        
        # Calculate final boost (max 0.5 adjustment to quickrecal)
        boost = scaled_surprise * volatility_multiplier * 0.5
        
        # Ensure boost is in 0-0.5 range (we don't want to boost by more than 0.5)
        return float(min(0.5, max(0.0, boost)))

```

# synthians_trainer_server\tests\__init__.py

```py

```

# synthians_trainer_server\tests\test_http_server.py

```py
import pytest
import json
import numpy as np
from unittest.mock import patch, MagicMock
from fastapi.testclient import TestClient

from ...geometry_manager import GeometryManager
from ..http_server import app
from ..neural_memory import NeuralMemoryModule


@pytest.fixture
def test_client():
    """Create a test client for the FastAPI app."""
    return TestClient(app)


@pytest.fixture
def mock_neural_memory():
    """Create a mock NeuralMemoryModule instance."""
    with patch('synthians_memory_core.synthians_trainer_server.http_server.get_neural_memory', autospec=True) as mock_get:
        mock_instance = MagicMock(spec=NeuralMemoryModule)
        mock_get.return_value = mock_instance
        
        # Configure mocked methods for new Neural Memory API
        mock_instance.retrieve.return_value = np.random.randn(768)
        mock_instance.update_memory.return_value = (0.1, 0.2)  # loss, grad_norm
        
        yield mock_instance


def test_health_endpoint(test_client):
    """Test that the health endpoint returns a 200 status code."""
    response = test_client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "ok"


def test_retrieve_endpoint(test_client, mock_neural_memory):
    """Test the retrieve endpoint of the Neural Memory API."""
    # Prepare request data
    input_embedding = np.random.randn(768).tolist()
    
    request_data = {
        "input_embedding": input_embedding
    }
    
    # Make the request
    response = test_client.post("/retrieve", json=request_data)
    
    # Verify the response
    assert response.status_code == 200
    result = response.json()
    assert "retrieved_embedding" in result
    assert len(result["retrieved_embedding"]) == 768
    
    # Verify the mock was called correctly
    mock_neural_memory.retrieve.assert_called_once()
    # First positional arg should be the input embedding (as numpy array)
    call_args = mock_neural_memory.retrieve.call_args[0]
    assert len(call_args) >= 1
    np.testing.assert_array_almost_equal(call_args[0], input_embedding)


def test_update_memory_endpoint(test_client, mock_neural_memory):
    """Test the update_memory endpoint of the Neural Memory API."""
    # Prepare request data
    input_embedding = np.random.randn(768).tolist()
    
    request_data = {
        "input_embedding": input_embedding
    }
    
    # Make the request
    response = test_client.post("/update_memory", json=request_data)
    
    # Verify the response
    assert response.status_code == 200
    result = response.json()
    assert "status" in result
    assert result["status"] == "success"
    assert "loss" in result
    assert "grad_norm" in result
    
    # Verify the mock was called correctly
    mock_neural_memory.update_memory.assert_called_once()
    # First positional arg should be the input embedding (as numpy array)
    call_args = mock_neural_memory.update_memory.call_args[0]
    assert len(call_args) >= 1
    np.testing.assert_array_almost_equal(call_args[0], input_embedding)
```

# synthians_trainer_server\tests\test_synthians_trainer.py

```py

```

# synthians_trainer_server\types.py

```py

```

# test_faiss_integration.py

```py
#!/usr/bin/env python

import os
import sys
import time
import logging
import numpy as np
import asyncio
import signal
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("faiss_integration_test")

# Set a timeout for operations that might hang
DEFAULT_TIMEOUT = 30  # seconds

# Define timeout handler
class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError("Operation timed out")

# Import FAISS
try:
    import faiss
    logger.info(f"FAISS version {getattr(faiss, '__version__', 'unknown')} loaded successfully")
    logger.info(f"FAISS has GPU support: {hasattr(faiss, 'StandardGpuResources')}")
except ImportError:
    logger.error("FAISS not found. Tests cannot proceed.")
    sys.exit(1)

# Import vector index implementation
from synthians_memory_core.vector_index import MemoryVectorIndex

# Import client if available for end-to-end test
try:
    from synthians_memory_core.api.client.client import SynthiansClient
    client_available = True
except ImportError:
    logger.warning("SynthiansClient not available, skipping API tests")
    client_available = False


class FAISSIntegrationTest:
    """Test suite for FAISS vector index implementation"""
    
    def __init__(self, use_gpu=True):
        self.test_results = {}
        self.test_dir = os.path.join(os.getcwd(), 'test_index')
        os.makedirs(self.test_dir, exist_ok=True)
        self.use_gpu = use_gpu
        logger.info(f"Test initialized with use_gpu={use_gpu}")
    
    def run_tests(self):
        """Run all tests and report results"""
        logger.info("\n===== STARTING FAISS INTEGRATION TESTS =====")
        
        # Run all tests
        self.test_results["basic_functionality"] = self.test_basic_functionality()
        self.test_results["dimension_mismatch"] = self.test_dimension_mismatch()
        self.test_results["malformed_embeddings"] = self.test_malformed_embeddings()
        self.test_results["persistence"] = self.test_persistence()
        
        # Report results
        logger.info("\n===== TEST RESULTS =====")
        for test_name, result in self.test_results.items():
            status = "PASSED" if result else "FAILED"
            logger.info(f"{test_name.replace('_', ' ').title()}: {status}")
        
        # Final status
        if all(self.test_results.values()):
            logger.info("\nu2705 ALL TESTS PASSED u2705")
            return True
        else:
            failed = [name for name, result in self.test_results.items() if not result]
            logger.error(f"\nu274c {len(failed)} TESTS FAILED: {', '.join(failed)} u274c")
            return False
    
    def test_basic_functionality(self):
        """Test basic FAISS vector index functionality"""
        logger.info("\n----- Testing Basic Functionality -----")
        try:
            # Set timeout for operations
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(DEFAULT_TIMEOUT)
            
            # Create vector index
            dimension = 768
            logger.info("Creating vector index...")
            index = MemoryVectorIndex({
                'embedding_dim': dimension,
                'storage_path': self.test_dir,
                'index_type': 'L2',
                'use_gpu': self.use_gpu
            })
            
            logger.info(f"Created index with dimension {dimension}, GPU usage: {index.is_using_gpu}")
            
            # Add vectors
            vectors_to_add = 50  # Reduced from 100 to speed up tests
            logger.info(f"Adding {vectors_to_add} vectors to index...")
            start_time = time.time()
            for i in range(vectors_to_add):
                memory_id = f"test_{i}"
                vector = np.random.random(dimension).astype('float32')
                index.add(memory_id, vector)
                # Log progress for every 10 vectors
                if i % 10 == 0 and i > 0:
                    logger.info(f"Added {i} vectors so far...")
            
            add_time = time.time() - start_time
            logger.info(f"Added {vectors_to_add} vectors in {add_time:.4f}s ({vectors_to_add/add_time:.2f} vectors/s)")
            
            # Search vectors
            logger.info("Searching for similar vectors...")
            query = np.random.random(dimension).astype('float32')
            search_start = time.time()
            results = index.search(query, 5)  # Reduced from 10
            search_time = time.time() - search_start
            
            logger.info(f"Search completed in {search_time:.4f}s, returned {len(results)} results")
            if results:
                logger.info(f"First result: {results[0]}")
            
            # Verify count
            logger.info("Verifying vector count...")
            count = index.count()
            logger.info(f"Index count: {count}, expected: {vectors_to_add}")
            assert count == vectors_to_add, f"Expected {vectors_to_add} vectors, got {count}"
            
            # Cancel timeout
            signal.alarm(0)
            
            logger.info("Basic functionality test passed")
            return True
        except TimeoutError:
            logger.error("Basic functionality test timed out")
            return False
        except Exception as e:
            logger.error(f"Basic functionality test failed: {str(e)}")
            # Cancel timeout in case of exception
            signal.alarm(0)
            return False
    
    def test_dimension_mismatch(self):
        """Test handling of vectors with different dimensions"""
        logger.info("\n----- Testing Dimension Mismatch Handling -----")
        try:
            # Set timeout for operations
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(DEFAULT_TIMEOUT)
            
            # Create index with specific dimension
            dimension = 768
            logger.info(f"Creating index with dimension {dimension}...")
            index = MemoryVectorIndex({
                'embedding_dim': dimension,
                'storage_path': self.test_dir,
                'index_type': 'L2',
                'use_gpu': self.use_gpu
            })
            
            # Test vectors with different dimensions
            dimensions = {
                'smaller': 384,   # Common dimension mismatch case
                'standard': dimension,
                'larger': 1024
            }
            
            # Add vectors with different dimensions
            for name, dim in dimensions.items():
                logger.info(f"Testing {name} vector with dimension {dim}...")
                vector = np.random.random(dim).astype('float32')
                try:
                    index.add(f"vector_{name}", vector)
                    logger.info(f"Successfully added {name} vector with dimension {dim}")
                except Exception as e:
                    logger.error(f"Failed to add {name} vector: {str(e)}")
                    signal.alarm(0)
                    return False
            
            # Search with different dimension vectors
            for name, dim in dimensions.items():
                logger.info(f"Searching with {name} vector ({dim} dimensions)...")
                query = np.random.random(dim).astype('float32')
                try:
                    results = index.search(query, 3)
                    logger.info(f"Successfully searched with {name} vector, got {len(results)} results")
                except Exception as e:
                    logger.error(f"Failed to search with {name} vector: {str(e)}")
                    signal.alarm(0)
                    return False
            
            # Cancel timeout
            signal.alarm(0)
            
            logger.info("Dimension mismatch test passed")
            return True
        except TimeoutError:
            logger.error("Dimension mismatch test timed out")
            return False
        except Exception as e:
            logger.error(f"Dimension mismatch test failed: {str(e)}")
            # Cancel timeout in case of exception
            signal.alarm(0)
            return False
    
    def test_malformed_embeddings(self):
        """Test handling of malformed embeddings (NaN/Inf)"""
        logger.info("\n----- Testing Malformed Embedding Handling -----")
        try:
            # Set timeout for operations
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(DEFAULT_TIMEOUT)
            
            # Create index
            dimension = 768
            logger.info(f"Creating index with dimension {dimension}...")
            index = MemoryVectorIndex({
                'embedding_dim': dimension,
                'storage_path': self.test_dir,
                'index_type': 'L2',
                'use_gpu': self.use_gpu
            })
            
            # Create test vectors
            normal = np.random.random(dimension).astype('float32')
            
            # Vector with NaN values
            nan_vector = np.random.random(dimension).astype('float32')
            nan_vector[10:20] = np.nan
            
            # Vector with Inf values
            inf_vector = np.random.random(dimension).astype('float32')
            inf_vector[30:40] = np.inf
            
            # Mixed vector
            mixed_vector = np.random.random(dimension).astype('float32')
            mixed_vector[5:10] = np.nan
            mixed_vector[50:55] = np.inf
            
            # Add vectors
            test_vectors = {
                'normal': normal,
                'nan': nan_vector,
                'inf': inf_vector,
                'mixed': mixed_vector
            }
            
            for name, vector in test_vectors.items():
                logger.info(f"Testing {name} vector...")
                try:
                    index.add(f"vector_{name}", vector)
                    logger.info(f"Successfully added {name} vector")
                except Exception as e:
                    logger.error(f"Failed to add {name} vector: {str(e)}")
                    if name == 'normal':  # Normal vectors must be added successfully
                        signal.alarm(0)
                        return False
            
            # Search with malformed query vectors
            for name, vector in test_vectors.items():
                logger.info(f"Searching with {name} vector...")
                try:
                    results = index.search(vector, 3)
                    logger.info(f"Successfully searched with {name} vector, got {len(results)} results")
                except Exception as e:
                    logger.error(f"Failed to search with {name} vector: {str(e)}")
                    if name == 'normal':  # Normal vectors must be searchable
                        signal.alarm(0)
                        return False
            
            # Cancel timeout
            signal.alarm(0)
            
            logger.info("Malformed embedding test passed")
            return True
        except TimeoutError:
            logger.error("Malformed embedding test timed out")
            return False
        except Exception as e:
            logger.error(f"Malformed embedding test failed: {str(e)}")
            # Cancel timeout in case of exception
            signal.alarm(0)
            return False
    
    def test_persistence(self):
        """Test index persistence (save/load)"""
        logger.info("\n----- Testing Index Persistence -----")
        try:
            # Set timeout for operations
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(DEFAULT_TIMEOUT)
            
            # Create and populate index
            dimension = 768
            logger.info(f"Creating index with dimension {dimension}...")
            index = MemoryVectorIndex({
                'embedding_dim': dimension,
                'storage_path': self.test_dir,
                'index_type': 'L2',
                'use_gpu': self.use_gpu
            })
            
            # Add vectors with known IDs
            vectors_to_add = 20  # Reduced from 50
            known_ids = []
            logger.info(f"Adding {vectors_to_add} vectors to index...")
            
            for i in range(vectors_to_add):
                memory_id = f"persistent_{i}"
                known_ids.append(memory_id)
                vector = np.random.random(dimension).astype('float32')
                index.add(memory_id, vector)
            
            # Save index
            index_path = os.path.join(self.test_dir, 'persistence_test.faiss')
            logger.info(f"Saving index to {index_path}...")
            index.save(index_path)
            logger.info(f"Saved index to {index_path}")
            
            # Create new index and load
            logger.info("Creating new index and loading saved data...")
            new_index = MemoryVectorIndex({
                'embedding_dim': dimension,
                'storage_path': self.test_dir,
                'index_type': 'L2',
                'use_gpu': self.use_gpu
            })
            
            new_index.load(index_path)
            logger.info(f"Loaded index with {new_index.count()} vectors")
            
            # Verify counts match
            logger.info("Verifying vector counts match...")
            assert new_index.count() == index.count(), "Vector counts don't match after loading"
            
            # Clean up
            if os.path.exists(index_path):
                os.remove(index_path)
                logger.info(f"Cleaned up test index file {index_path}")
            
            # Cancel timeout
            signal.alarm(0)
            
            logger.info("Persistence test passed")
            return True
        except TimeoutError:
            logger.error("Persistence test timed out")
            return False
        except Exception as e:
            logger.error(f"Persistence test failed: {str(e)}")
            # Cancel timeout in case of exception
            signal.alarm(0)
            return False

async def test_api_integration():
    """Test integration with the memory API"""
    logger.info("\n----- Testing API Integration -----")
    
    if not client_available:
        logger.warning("SynthiansClient not available, skipping API test")
        return False
    
    try:
        # Set timeout for operations
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(DEFAULT_TIMEOUT)
        
        logger.info("Connecting to API...")
        client = SynthiansClient()
        await client.connect()
        
        # Create unique test memories
        timestamp = datetime.now().isoformat()
        unique_prefix = f"faiss_test_{timestamp}"
        
        logger.info(f"Creating test memories with prefix: {unique_prefix}")
        
        # Create memories
        memories = []
        for i in range(3):
            content = f"{unique_prefix} Memory {i}: This is a test memory for FAISS integration testing"
            logger.info(f"Creating memory {i}...")
            response = await client.process_memory(
                content=content,
                metadata={"test_type": "faiss_integration", "memory_number": i}
            )
            
            if response.get("success"):
                memory_id = response.get("memory_id")
                memories.append((memory_id, content))
                logger.info(f"Created memory {i} with ID: {memory_id}")
            else:
                logger.error(f"Failed to create memory {i}: {response.get('error')}")
        
        # Wait for indexing
        logger.info("Waiting for memories to be indexed...")
        await asyncio.sleep(1)
        
        # Retrieve memories
        query = unique_prefix
        logger.info(f"Retrieving memories with query: '{query}'")
        
        response = await client.retrieve_memories(query, top_k=5, threshold=0.2)
        
        if not response.get("success"):
            logger.error(f"Retrieval failed: {response.get('error')}")
            signal.alarm(0)
            return False
        
        results = response.get("memories", [])
        retrieved_ids = [m.get("id") for m in results]
        
        logger.info(f"Retrieved {len(results)} memories")
        
        # Verify that our memories were retrieved
        success = True
        for memory_id, _ in memories:
            if memory_id not in retrieved_ids:
                logger.error(f"Memory {memory_id} was not retrieved")
                success = False
        
        # Display similarity scores
        if results:
            logger.info("Similarity scores:")
            for memory in results:
                logger.info(f"  {memory.get('id')}: {memory.get('similarity_score', 'N/A')}")
        
        # Test with lower threshold
        logger.info("Testing with lower threshold (0.3)...")
        low_threshold_response = await client.retrieve_memories(
            query, top_k=5, threshold=0.3
        )
        
        low_results = low_threshold_response.get("memories", [])
        logger.info(f"Retrieved {len(low_results)} memories with lower threshold")
        
        await client.disconnect()
        
        # Cancel timeout
        signal.alarm(0)
        
        if success:
            logger.info("API integration test passed")
        else:
            logger.error("API integration test failed - not all memories were retrieved")
        
        return success
    except TimeoutError:
        logger.error("API integration test timed out")
        return False
    except Exception as e:
        logger.error(f"API integration test failed: {str(e)}")
        # Cancel timeout in case of exception
        signal.alarm(0)
        return False

async def main():
    # Run tests with and without GPU
    logger.info("\n===== FIRST RUNNING TESTS WITH CPU ONLY =====\n")
    cpu_test_suite = FAISSIntegrationTest(use_gpu=False)
    cpu_success = cpu_test_suite.run_tests()
    
    # Only try GPU if CPU tests pass
    if cpu_success:
        logger.info("\n===== NOW RUNNING TESTS WITH GPU =====\n")
        gpu_test_suite = FAISSIntegrationTest(use_gpu=True)
        gpu_success = gpu_test_suite.run_tests()
    else:
        logger.warning("Skipping GPU tests because CPU tests failed")
        gpu_success = False
    
    # Run API integration test
    api_success = await test_api_integration()
    
    if cpu_success and gpu_success and api_success:
        logger.info("\u2705 ALL TESTS PASSED INCLUDING GPU AND API INTEGRATION \u2705")
        return 0
    elif cpu_success and api_success:
        logger.warning("\u26a0ufe0f CPU AND API TESTS PASSED BUT GPU TESTS FAILED \u26a0ufe0f")
        return 1
    elif cpu_success:
        logger.warning("\u26a0ufe0f CPU TESTS PASSED BUT GPU AND API TESTS FAILED \u26a0ufe0f")
        return 2
    else:
        logger.error("\u274c ALL TESTS FAILED \u274c")
        return 3

if __name__ == "__main__":
    # Try to fix SIGALRM not available on Windows
    if sys.platform == "win32":
        logger.warning("Timeout functionality not available on Windows, disabling timeouts")
        # Define dummy functions
        def timeout_handler(signum, frame):
            pass
        signal.SIGALRM = signal.SIGTERM  # Just a placeholder
        signal.alarm = lambda x: None    # No-op function
    
    sys.exit(asyncio.run(main()))

```

# tests\conftest.py

```py
import os
import pytest
import pytest_asyncio
import asyncio
import aiohttp
import httpx  # Using httpx for health checks
import shutil
import tempfile
import time
from datetime import datetime

# Import the Memory Core client
from synthians_memory_core.api.client.client import SynthiansClient

# --- Configuration for variant integration tests ---
MC_URL = "http://localhost:5010"
NM_URL = "http://localhost:8001"
CCE_URL = "http://localhost:8002"

# --- Health Check Fixture (Function-Scoped) ---
@pytest_asyncio.fixture(autouse=False)  # Not auto-using by default to avoid affecting other tests
async def check_services_responsive(request):
    """
    Quickly check if core services are responsive before each test function.
    Uses httpx for simple async requests. Skips if test is marked 'skip_health_check'.
    """
    if "skip_health_check" in request.keywords:
        print("\nSkipping health check for this test.")
        yield
        return

    # Short timeout for quick check
    async with httpx.AsyncClient(timeout=3.0) as client:
        service_endpoints = {
            "Memory Core": f"{MC_URL}/health",
            "Neural Memory": f"{NM_URL}/health",
            "CCE": f"{CCE_URL}/"  # Basic root check for CCE
        }
        tasks = []
        for name, url in service_endpoints.items():
            tasks.append(client.get(url))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for (name, url), result in zip(service_endpoints.items(), results):
            if isinstance(result, Exception):
                pytest.fail(f"Health check failed: Service '{name}' at {url} unreachable. Error: {result}", pytrace=False)
            elif not result.is_success:
                pytest.fail(f"Health check failed: Service '{name}' at {url} returned status {result.status_code}", pytrace=False)
    yield  # Let the test run

# --- API Client Fixture (Function-Scoped) ---
@pytest_asyncio.fixture
async def api_clients():
    """
    Provides an aiohttp session and an initialized SynthiansClient (for MC).
    This fixture is used by variant integration tests to interact with the running Docker services.
    """
    # Provides aiohttp session for CCE/NM and dedicated client for MC
    async with aiohttp.ClientSession() as session, \
               SynthiansClient(base_url=MC_URL) as mc_client:
        yield session, mc_client  # Yield clients for the test function

# Helper function for variant tests to create test memories
async def create_test_memories(client, count=5, prefix="Test memory"):
    """Create a batch of test memories for testing."""
    memory_ids = []
    for i in range(count):
        content = f"{prefix} {i}"
        memory_id = f"test_variant_{os.environ.get('TITANS_VARIANT', 'UNKNOWN')}_{i}"
        
        # Create a test memory with random embedding
        embedding = [float(j) / 100 for j in range(384)]  # 384-dimensional embedding
        
        # Use the API to create the memory
        memory_entry = {
            "content": content,
            "embedding": embedding,
            "metadata": {
                "source": "test",
                "test_id": i,
                "test_batch": prefix,
                "variant": os.environ.get('TITANS_VARIANT', 'UNKNOWN')
            }
        }
        
        # Store the memory in the database
        await client.process_memory(memory_entry, memory_id)
        memory_ids.append(memory_id)
    
    return memory_ids

# --- Original fixtures for local testing ---
@pytest.fixture(scope="session")
def temp_test_dir():
    """Create a temporary directory for test data that's removed after tests finish."""
    test_dir = tempfile.mkdtemp(prefix="synthians_test_")
    print(f"\nCreated temporary test directory: {test_dir}")
    yield test_dir
    # Clean up after tests
    # Add retry logic for Windows file locking issues
    attempts = 3
    while attempts > 0:
        try:
            if os.path.exists(test_dir):
                shutil.rmtree(test_dir, ignore_errors=False)
                print(f"Removed temporary test directory: {test_dir}")
            else:
                print(f"Temporary test directory already removed: {test_dir}")
            break # Success
        except OSError as e:
            print(f"Warning: Error removing temp directory (attempt {4-attempts}): {e}")
            attempts -= 1
            if attempts == 0:
                print(f"ERROR: Failed to remove temp directory {test_dir} after multiple attempts.")
            else:
                time.sleep(0.5) # Wait before retrying

@pytest.fixture(scope="session")
def test_server_url():
    """Return the URL of the test server."""
    # Default to localhost:5010, but allow override through environment variable
    return os.environ.get("SYNTHIANS_TEST_URL", "http://localhost:5010")

# Configure markers for test categories
def pytest_configure(config):
    config.addinivalue_line(
        "markers", "smoke: mark test as a smoke test (basic functionality)"
    )
    config.addinivalue_line(
        "markers", "integration: mark test as an integration test"
    )
    config.addinivalue_line(
        "markers", "slow: mark test as a slow test"
    )
    config.addinivalue_line(
        "markers", "emotion: mark test as testing emotion analysis"
    )
    config.addinivalue_line(
        "markers", "retrieval: mark test as testing memory retrieval"
    )
    config.addinivalue_line(
        "markers", "stress: mark test as a stress test"
    )
    config.addinivalue_line(
        "markers", "skip_health_check: skip the services health check"
    )
    config.addinivalue_line(
        "markers", "variant: mark test as a Titans variant test (MAC, MAG, MAL)"
    )

```

# tests\README.md

```md
# Synthians Memory Core Test Suite

This comprehensive test suite is designed to validate the functionality, performance, and reliability of the Synthians Memory Core system. The tests are organized into modular, progressive phases to ensure full coverage of all components while allowing for targeted testing of specific subsystems.

## 🧪 Test Structure

The tests are organized into seven progressive phases, each focusing on different aspects of the system:

### 🔹 Phase 1: Core Infrastructure Validation
- `test_api_health.py` - Basic API endpoints, health, and stats tests

### 🔹 Phase 2: Memory Lifecycle Test
- `test_memory_lifecycle.py` - End-to-end memory creation, retrieval, feedback, deletion

### 🔹 Phase 3: Emotional & Cognitive Layer Test
- `test_emotion_and_cognitive.py` - Tests for emotion analysis, metadata enrichment, and cognitive load scoring

### 🔹 Phase 4: Transcription & Voice Pipeline Test
- `test_transcription_voice_flow.py` - Tests for speech transcription, interruption handling, and voice state management

### 🔹 Phase 5: Retrieval Dynamics Test
- `test_retrieval_dynamics.py` - Tests for memory retrieval with various conditions, thresholds, and filters

### 🔹 Phase 6: Tooling Integration Test
- `test_tool_integration.py` - Tests for tool interfaces that call core functions

### 🔹 Phase 7: Stress + Load Test
- `test_stress_load.py` - High-volume and performance tests 

## 📋 Prerequisites

\`\`\`bash
pip install pytest pytest-asyncio pytest-html aiohttp
\`\`\`

## 🚀 Running Tests

### Quick Start

\`\`\`bash
# Run all tests
python tests/run_tests.py

# Run with more detailed output
python tests/run_tests.py --verbose

# Run smoke tests only
python tests/run_tests.py --markers="smoke"

# Run a specific test module
python tests/run_tests.py --module="test_api_health.py"

# Run a specific test function
python tests/run_tests.py --test="test_health_and_stats"

# Generate HTML and XML reports
python tests/run_tests.py --report

# Run tests in parallel
python tests/run_tests.py --parallel=4

# Test against a different server
python tests/run_tests.py --url="http://test-server:5010"
\`\`\`

### Using pytest directly

\`\`\`bash
# Run all tests
pytest -xvs --asyncio-mode=auto

# Run a specific test module
pytest -xvs test_api_health.py --asyncio-mode=auto

# Run tests with a specific marker
pytest -xvs -m smoke --asyncio-mode=auto
\`\`\`

## 🏷️ Test Markers

Tests are categorized with the following markers:

- `smoke`: Basic functionality tests that should always pass
- `integration`: Tests that verify integration between components
- `slow`: Tests that take longer to run (e.g., stress tests)
- `emotion`: Tests focused on emotion analysis
- `retrieval`: Tests focused on memory retrieval
- `stress`: High-volume load tests

## 📊 Test Reports

When using the `--report` option, the test suite generates:

- HTML reports in `test_reports/report_TIMESTAMP.html`
- XML reports in `test_reports/report_TIMESTAMP.xml` (JUnit format for CI systems)

## 🔧 Configuration

The test suite can be configured using environment variables:

- `SYNTHIANS_TEST_URL`: URL of the test server (default: http://localhost:5010)

## ⚠️ Implementation Notes

1. Tests use a temporary directory for test data by default
2. Some tests expect specific API functionality which may not be implemented yet
3. Stress tests have reduced volumes by default to run faster - adjust constants in code for full stress testing
4. Pay attention to potential race conditions with concurrent tests
5. Some tests may fail if specific components (e.g., emotion analyzer) are not properly initialized

## 🛠 Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| Dimension mismatch warnings in logs | Expected during testing with different embedding dimensions |
| Empty embeddings | Check if the embedding model is properly loaded |
| HTTP connection errors | Ensure the server is running and accessible at the configured URL |
| File permission errors | Check that the test directory has proper write permissions |
| Test timeouts | Adjust timeout settings or reduce batch sizes in stress tests |

## 🔄 Continuous Integration

This test suite is designed to be integrated with CI/CD pipelines. XML reports in JUnit format can be consumed by most CI systems.

```

# tests\run_tests.py

```py
#!/usr/bin/env python

import os
import sys
import argparse
import subprocess
import time
from datetime import datetime

def run_tests(args):
    """Run the Synthians Memory Core test suite with the specified options."""
    # Construct the pytest command
    cmd = ["pytest"]
    
    # Add verbosity
    if args.verbose:
        cmd.append("-v")
    
    # Add test selection options
    if args.markers:
        for marker in args.markers.split(","):
            cmd.append(f"-m {marker}")
    
    if args.module:
        cmd.append(args.module)
    
    if args.test:
        cmd.append(f"-k {args.test}")
    
    # Add parallel execution if specified
    if args.parallel:
        cmd.append(f"-xvs -n {args.parallel}")
    
    # Add report options
    if args.report:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join("test_reports", f"report_{timestamp}")
        
        # Create the report directory if it doesn't exist
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        # Add HTML report
        cmd.append(f"--html={report_path}.html")
        
        # Add JUnit XML report for CI integration
        cmd.append(f"--junitxml={report_path}.xml")
    
    # Add asyncio mode
    cmd.append("--asyncio-mode=auto")
    
    # Join the command parts
    cmd_str = " ".join(cmd)
    print(f"Running: {cmd_str}")
    
    # Execute the command
    start_time = time.time()
    result = subprocess.run(cmd_str, shell=True)
    elapsed_time = time.time() - start_time
    
    print(f"\nTests completed in {elapsed_time:.2f} seconds with exit code {result.returncode}")
    return result.returncode

def main():
    parser = argparse.ArgumentParser(description="Run Synthians Memory Core test suite")
    parser.add_argument("-v", "--verbose", action="store_true", help="Enable verbose output")
    parser.add_argument("-m", "--markers", help="Comma-separated list of markers to run (e.g., 'smoke,integration')")
    parser.add_argument("-k", "--test", help="Expression to filter tests by name")
    parser.add_argument("-t", "--module", help="Specific test module to run (e.g., 'test_api_health.py')")
    parser.add_argument("-p", "--parallel", type=int, help="Run tests in parallel with specified number of processes")
    parser.add_argument("-r", "--report", action="store_true", help="Generate HTML and XML test reports")
    parser.add_argument("--url", help="Override the API server URL (default: http://localhost:5010)")
    
    args = parser.parse_args()
    
    # Set environment variables
    if args.url:
        os.environ["SYNTHIANS_TEST_URL"] = args.url
    
    print("=== Synthians Memory Core Test Runner ===")
    print(f"Starting tests at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Set the working directory to the script's directory
    original_dir = os.getcwd()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    try:
        return run_tests(args)
    finally:
        # Restore original directory
        os.chdir(original_dir)

if __name__ == "__main__":
    sys.exit(main())

```

# tests\test_api_health.py

```py
import pytest
import asyncio
import json
from synthians_memory_core.api.client.client import SynthiansClient

@pytest.mark.asyncio
async def test_health_and_stats():
    """Test basic health check and stats endpoints."""
    async with SynthiansClient() as client:
        # Test health endpoint
        health = await client.health_check()
        assert health.get("status") == "healthy", "Health check failed"
        assert "uptime_seconds" in health, "Health response missing uptime"
        assert "version" in health, "Health response missing version"
        
        # Test stats endpoint
        stats = await client.get_stats()
        assert stats.get("success") is True, "Stats endpoint failed"
        assert "api_server" in stats, "Stats missing api_server information"
        assert "memory_count" in stats.get("api_server", {}), "Stats missing memory count"
        
        # Output results for debugging
        print(f"Health check: {json.dumps(health, indent=2)}")
        print(f"Stats: {json.dumps(stats, indent=2)}")

@pytest.mark.asyncio
async def test_api_smoke_test():
    """Test all API endpoints to ensure they respond correctly."""
    async with SynthiansClient() as client:
        # Test embedding generation
        embed_resp = await client.generate_embedding("Test embedding generation")
        assert embed_resp.get("success") is True, "Embedding generation failed"
        assert "embedding" in embed_resp, "No embedding returned"
        assert "dimension" in embed_resp, "No dimension information"
        
        # Test emotion analysis
        emotion_resp = await client.analyze_emotion("I am feeling very happy today")
        assert emotion_resp.get("success") is True, "Emotion analysis failed"
        assert "emotions" in emotion_resp, "No emotions returned"
        assert "dominant_emotion" in emotion_resp, "No dominant emotion identified"
        
        # Test QuickRecal calculation
        qr_resp = await client.calculate_quickrecal(text="Testing QuickRecal API")
        assert qr_resp.get("success") is True, "QuickRecal calculation failed"
        assert "quickrecal_score" in qr_resp, "No QuickRecal score returned"
        
        # Test contradiction detection
        contradict_resp = await client.detect_contradictions(threshold=0.7)
        assert contradict_resp.get("success") is True, "Contradiction detection failed"

```

# tests\test_emotion_and_cognitive.py

```py
import pytest
import asyncio
import json
import numpy as np
from synthians_memory_core.api.client.client import SynthiansClient

@pytest.mark.asyncio
async def test_emotion_analysis_rich():
    """Test emotion analysis with various emotional inputs."""
    async with SynthiansClient() as client:
        # Test happy emotion
        happy_text = "I'm incredibly happy today! Everything is going wonderfully well!"
        happy_result = await client.analyze_emotion(happy_text)
        
        assert happy_result.get("success") is True, "Emotion analysis failed"
        assert happy_result.get("dominant_emotion") in ["joy", "happiness"], f"Expected happy emotion, got {happy_result.get('dominant_emotion')}"
        assert happy_result.get("emotions", {}).get("joy", 0) > 0.5, "Expected high joy score"
        
        print(f"Happy emotion result: {json.dumps(happy_result, indent=2)}")
        
        # Test sad emotion
        sad_text = "I feel so sad and depressed today. Everything is going wrong."
        sad_result = await client.analyze_emotion(sad_text)
        
        assert sad_result.get("success") is True, "Emotion analysis failed"
        assert sad_result.get("dominant_emotion") in ["sadness", "sorrow"], f"Expected sad emotion, got {sad_result.get('dominant_emotion')}"
        
        print(f"Sad emotion result: {json.dumps(sad_result, indent=2)}")
        
        # Test angry emotion
        angry_text = "I'm absolutely furious about how I was treated! This is outrageous!"
        angry_result = await client.analyze_emotion(angry_text)
        
        assert angry_result.get("success") is True, "Emotion analysis failed"
        assert angry_result.get("dominant_emotion") in ["anger", "rage"], f"Expected anger emotion, got {angry_result.get('dominant_emotion')}"
        
        print(f"Angry emotion result: {json.dumps(angry_result, indent=2)}")

@pytest.mark.asyncio
async def test_emotion_fallback_path():
    """Test emotion analysis fallback mechanisms when model fails."""
    # Note: This test assumes emotion analyzer has a fallback mechanism
    # when the primary model fails. We'll test with extreme text that might
    # cause issues for the model.
    
    async with SynthiansClient() as client:
        # Test with extremely long text that might cause issues
        long_text = "happy " * 1000  # Very long repetitive text
        result = await client.analyze_emotion(long_text)
        
        # Even if the main model fails, we should still get a result
        assert result.get("success") is True, "Emotion analysis completely failed"
        assert "dominant_emotion" in result, "No dominant emotion provided"
        
        # Test with empty text
        empty_result = await client.analyze_emotion("")
        assert empty_result.get("success") is True, "Empty text analysis failed"
        assert "dominant_emotion" in empty_result, "No dominant emotion for empty text"
        
        print(f"Empty text emotion result: {json.dumps(empty_result, indent=2)}")

@pytest.mark.asyncio
async def test_emotion_saved_in_metadata():
    """Test that emotional analysis is saved in memory metadata."""
    async with SynthiansClient() as client:
        # Create a memory with strong emotional content
        content = "I am absolutely thrilled about the amazing news I received today!"
        
        # Process the memory with emotion analysis enabled
        memory_resp = await client.process_memory(
            content=content,
            metadata={"analyze_emotion": True}
        )
        
        assert memory_resp.get("success") is True, "Memory creation failed"
        metadata = memory_resp.get("metadata", {})
        
        # Check that emotion data was added to metadata
        assert "dominant_emotion" in metadata, "No dominant emotion in metadata"
        assert "emotional_intensity" in metadata, "No emotional intensity in metadata"
        assert "emotions" in metadata, "No emotions dictionary in metadata"
        
        # Check that the emotion is reasonable for the content
        assert metadata.get("dominant_emotion") in ["joy", "happiness"], f"Expected happy emotion, got {metadata.get('dominant_emotion')}"
        
        print(f"Memory metadata with emotions: {json.dumps(metadata, indent=2)}")

@pytest.mark.asyncio
async def test_cognitive_load_score_range():
    """Test that cognitive load scoring works across different complexity levels."""
    async with SynthiansClient() as client:
        # Test with simple text
        simple_text = "This is a simple sentence."
        simple_memory = await client.process_memory(content=simple_text)
        
        # Test with complex text
        complex_text = """The quantum mechanical model is a theoretical framework that describes the behavior of subatomic 
        particles through probabilistic wave functions. It posits that particles exhibit both wave-like and 
        particle-like properties, a concept known as wave-particle duality. The Schrödinger equation, a fundamental 
        mathematical formulation in quantum mechanics, predicts how these wave functions evolve over time. 
        Unlike classical mechanics, quantum mechanics introduces inherent uncertainty in measurements, 
        as formalized in Heisenberg's uncertainty principle, which states that certain pairs of physical properties 
        cannot be precisely measured simultaneously."""
        complex_memory = await client.process_memory(content=complex_text)
        
        # Get metadata for both memories
        simple_metadata = simple_memory.get("metadata", {})
        complex_metadata = complex_memory.get("metadata", {})
        
        # If cognitive_complexity is in the metadata, verify it's higher for complex text
        if "cognitive_complexity" in simple_metadata and "cognitive_complexity" in complex_metadata:
            simple_complexity = simple_metadata.get("cognitive_complexity", 0)
            complex_complexity = complex_metadata.get("cognitive_complexity", 0)
            
            # The complex text should have higher cognitive complexity
            assert complex_complexity > simple_complexity, \
                f"Expected higher complexity for complex text: simple={simple_complexity}, complex={complex_complexity}"
            
            print(f"Simple text complexity: {simple_complexity}")
            print(f"Complex text complexity: {complex_complexity}")

@pytest.mark.asyncio
async def test_emotional_gating_blocks_mismatched():
    """Test that emotional gating blocks memories with mismatched emotions."""
    async with SynthiansClient() as client:
        # Create a happy memory
        happy_text = "I'm so happy and excited about my new job!"
        happy_memory = await client.process_memory(content=happy_text)
        
        # Wait briefly for processing
        await asyncio.sleep(0.5)
        
        # Try to retrieve with angry emotion context
        angry_emotion = {"dominant_emotion": "anger", "emotions": {"anger": 0.9}}
        retrieval_resp = await client.retrieve_memories(
            query="job",
            top_k=5,
            user_emotion=angry_emotion
        )
        
        memories = retrieval_resp.get("memories", [])
        
        # If emotional gating is working, the happy memory might be ranked lower or filtered
        # We can't assert exact behavior since it depends on implementation details
        # Instead, we'll log the results for inspection
        print(f"Retrieved {len(memories)} memories with mismatched emotion")
        
        # Create an angry memory
        angry_text = "I'm absolutely furious about how they handled my job application!"
        angry_memory = await client.process_memory(content=angry_text)
        
        # Wait briefly for processing
        await asyncio.sleep(0.5)
        
        # Retrieve again with the same angry emotion context
        angry_retrieval = await client.retrieve_memories(
            query="job",
            top_k=5,
            user_emotion=angry_emotion
        )
        
        # The angry memory should now be present and possibly ranked higher
        angry_memories = angry_retrieval.get("memories", [])
        
        print(f"Retrieved {len(angry_memories)} memories with matching emotion")
        
        # Print the scores for comparison (if available)
        if memories and angry_memories and "quickrecal_score" in memories[0] and "quickrecal_score" in angry_memories[0]:
            print(f"Mismatched emotion memory score: {memories[0].get('quickrecal_score')}")
            print(f"Matching emotion memory score: {angry_memories[0].get('quickrecal_score')}")

```

# tests\test_feedback_loop.py

```py
# tests/test_feedback_loop.py

import pytest
import asyncio
import json
import time
import os
import aiohttp
import numpy as np
from datetime import datetime

# Assuming SynthiansClient is available and targets the Memory Core API (e.g., port 5010)
from synthians_memory_core.api.client.client import SynthiansClient

# Add the get_memory_by_id method to SynthiansClient if not already present
async def get_memory_by_id(self, memory_id: str):
    """Retrieve a specific memory by its ID."""
    async with self.session.get(
        f"{self.base_url}/api/memories/{memory_id}" # Use the new endpoint path
    ) as response:
        if response.status == 404:
            return None # Return None if not found
        response.raise_for_status() # Raise an exception for other errors
        return await response.json()

if not hasattr(SynthiansClient, "get_memory_by_id"):
    SynthiansClient.get_memory_by_id = get_memory_by_id.__get__(None, SynthiansClient)


# --- Test Configuration ---
CCE_URL = os.environ.get("CCE_TEST_URL", "http://localhost:8002") # Orchestrator URL

# --- Test Case ---

@pytest.mark.asyncio
@pytest.mark.integration # Mark as integration test
async def test_quickrecal_boost_feedback_loop():
    """
    Tests the full CCE -> NM -> MC feedback loop for QuickRecal boost.

    1. Creates an initial memory (Memory A) directly in the Memory Core.
    2. Retrieves Memory A to get its initial QuickRecal score.
    3. Processes a second, related memory (Memory B) via the Context Cascade Engine API.
       This should trigger the NM update, surprise calculation, and boost request.
    4. Waits for the loop to complete.
    5. Retrieves Memory A again.
    6. Asserts that Memory A's QuickRecal score has increased.
    """
    print(f"\n--- Starting Feedback Loop Integration Test ---")
    print(f"Targeting CCE at: {CCE_URL}")

    test_timestamp = datetime.now().isoformat()
    memory_A_id = None
    initial_quickrecal_score = -1.0

    try:
        # Create client for Memory Core interactions
        async with SynthiansClient() as mc_client:
            print(f"Targeting MC at: {mc_client.base_url}")
            
            # --- Step 1: Create Initial Memory (Memory A) in Memory Core ---
            print("Step 1: Creating initial memory (Memory A) in Memory Core...")
            memory_A_content = f"Baseline memory for feedback loop test at {test_timestamp}. Topic: Quantum Entanglement."
            meta_A = {"test_id": "feedback_loop", "sequence": "A"}

            create_A_resp = await mc_client.process_memory(content=memory_A_content, metadata=meta_A)
            assert create_A_resp.get("success") is True, f"Failed to create Memory A: {create_A_resp.get('error')}"
            memory_A_id = create_A_resp.get("memory_id")
            assert memory_A_id is not None, "Memory A ID not returned"
            print(f"  - Memory A created successfully (ID: {memory_A_id})")

            # --- Step 2: Get Initial QuickRecal Score for Memory A ---
            print(f"Step 2: Retrieving Memory A ({memory_A_id}) to get initial score...")
            await asyncio.sleep(0.5) # Brief pause for persistence/indexing
            retrieved_A_initial = await mc_client.get_memory_by_id(memory_A_id)
            assert retrieved_A_initial is not None, f"Failed to retrieve Memory A ({memory_A_id}) after creation"
            initial_quickrecal_score = retrieved_A_initial.get("memory", {}).get("quickrecal_score", 0.0)
            print(f"  - Initial QuickRecal score for Memory A: {initial_quickrecal_score:.6f}")

            # --- Step 3: Process Second Memory (Memory B) via CCE ---
            # This memory should be related but different enough to cause surprise
            print("Step 3: Processing related memory (Memory B) via Context Cascade Engine...")
            memory_B_content = f"A surprising development regarding Quantum Entanglement measurement observed at {test_timestamp}."
            meta_B = {"test_id": "feedback_loop", "sequence": "B"}
            cce_payload = {
                "content": memory_B_content,
                "metadata": meta_B
                # Embedding will be generated by CCE/MC
            }

            async with aiohttp.ClientSession() as session:
                cce_process_url = f"{CCE_URL}/process_memory"
                print(f"  - Calling CCE at: {cce_process_url}")
                async with session.post(cce_process_url, json=cce_payload, timeout=30.0) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        pytest.fail(f"CCE /process_memory call failed with status {resp.status}: {error_text}")
                    cce_resp_B = await resp.json()
                    print(f"  - CCE processed Memory B. Response keys: {list(cce_resp_B.keys())}")
                    # Print out the full response from CCE for diagnosis
                    print(f"  - CCE FULL RESPONSE: {json.dumps(cce_resp_B, indent=2)}")

            # Check if CCE response indicates a boost was attempted (based on surprise metrics)
            surprise_metrics = cce_resp_B.get("surprise_metrics", {})
            loss = surprise_metrics.get("loss")
            grad_norm = surprise_metrics.get("grad_norm")
            boost_calculated = surprise_metrics.get("boost_calculated")

            print(f"  - Surprise Metrics from CCE: Loss={loss}, GradNorm={grad_norm}, BoostCalculated={boost_calculated}")
            # Make this assertion optional for local testing without Neural Memory
            if loss is None and grad_norm is None:
                print(f"  - WARNING: CCE response missing surprise metrics (loss/grad_norm).")
                print(f"  - This may be due to Neural Memory not being available or a connection issue.")
                print(f"  - Will continue test with the assumption that some boost was calculated.")
            else:
                assert loss is not None or grad_norm is not None, "CCE response missing surprise metrics (loss/grad_norm)"
            # We expect some boost to be calculated if there was surprise
            # Allow for very small/zero boost if NM is already well-adapted
            # assert boost_calculated is not None and boost_calculated > 1e-6, "CCE did not calculate a significant boost"

            # --- Step 4: Wait for the feedback loop to complete ---
            # This involves CCE calling MC API's update endpoint. Needs some time.
            wait_time = 3.0 # Adjust based on observed system latency
            print(f"Step 4: Waiting {wait_time} seconds for feedback loop...")
            await asyncio.sleep(wait_time)

            # --- Step 5: Retrieve Memory A Again ---
            print(f"Step 5: Retrieving Memory A ({memory_A_id}) again to check score...")
            
            # Check the memory details before the final check
            memory_before_check = await mc_client.get_memory_by_id(memory_A_id)
            if memory_before_check:
                print(f"  - DEBUG: Memory details before final check:")
                print(f"      - QuickRecal score: {memory_before_check.get('memory', {}).get('quickrecal_score', 'N/A')}")
                print(f"      - Metadata: {json.dumps(memory_before_check.get('memory', {}).get('metadata', {}), indent=2)}")
                # Check if surprise_events exist in metadata
                metadata = memory_before_check.get('memory', {}).get('metadata', {})
                if 'surprise_events' in metadata:
                    print(f"      - Found {len(metadata['surprise_events'])} surprise events in metadata")
                    for i, event in enumerate(metadata['surprise_events']):
                        print(f"        - Event {i+1}: delta={event.get('delta')}, previous={event.get('previous_score')}, new={event.get('new_score')}, reason={event.get('reason')}")
                else:
                    print(f"      - No surprise events found in metadata")
            
            retrieved_A_final = await mc_client.get_memory_by_id(memory_A_id)
            assert retrieved_A_final is not None, f"Failed to retrieve Memory A ({memory_A_id}) after feedback loop"
            final_quickrecal_score = retrieved_A_final.get("memory", {}).get("quickrecal_score", 0.0)
            print(f"  - Final QuickRecal score for Memory A: {final_quickrecal_score:.6f}")

            # --- Step 6: Assert Score Increase ---
            print("Step 6: Verifying QuickRecal score increase...")
            # Allow for potential floating point noise, check for meaningful increase
            assert final_quickrecal_score > initial_quickrecal_score, \
                f"QuickRecal score for Memory A did not increase! Initial={initial_quickrecal_score}, Final={final_quickrecal_score}"
            print(f"  - SUCCESS: Score increased by {final_quickrecal_score - initial_quickrecal_score:.6f}")

            print("--- Feedback Loop Integration Test PASSED ---")

    except aiohttp.ClientConnectorError as e:
        pytest.fail(f"Connection Error: Could not connect to services. Ensure CCE ({CCE_URL}) and MC are running. Details: {e}")
    except Exception as e:
        pytest.fail(f"An unexpected error occurred during the feedback loop test: {e}\nTraceback: {e.__traceback__}")
    finally:
        # Optional cleanup: Delete created memories
        # if memory_A_id:
        #     pass # Add delete call if/when available
        pass

```

# tests\test_memory_core_updates.py

```py
# tests/test_memory_core_updates.py

import pytest
import pytest_asyncio  # Import the decorator
import asyncio
import json
import time
import os
import shutil
import threading
from datetime import datetime, timedelta, timezone
import numpy as np
from typing import Dict, Any, List, Optional
import aiofiles # Ensure aiofiles is imported

# Import the necessary components from the core
from synthians_memory_core import (
    SynthiansMemoryCore,
    MemoryEntry,
    GeometryManager,
    MemoryPersistence
)
from synthians_memory_core.vector_index import MemoryVectorIndex

# --- Dummy Async Lock for Testing ---
class DummyAsyncLock:
    """A dummy lock that doesn't block, for testing purposes."""
    async def __aenter__(self):
        # print("DEBUG: Entering DummyAsyncLock") # Optional debug print
        pass
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # print("DEBUG: Exiting DummyAsyncLock") # Optional debug print
        pass

# Helper function for removing test directories
async def _remove_directory_with_retry(directory, max_attempts=5, delay=0.5):
    """Helper function to remove a directory with retry logic"""
    attempts = 0
    while attempts < max_attempts:
        try:
            shutil.rmtree(directory, ignore_errors=True)
            if not os.path.exists(directory):
                print(f"  - Successfully removed {directory}")
                return True
            raise OSError(f"Directory still exists after removal: {directory}")
        except OSError as e:
            attempts += 1
            print(f"  - Error removing directory (attempt {attempts}/{max_attempts}): {e}")
            if attempts < max_attempts:
                await asyncio.sleep(delay)
    
    print(f"  - Failed to remove directory after {max_attempts} attempts: {directory}")
    return False

# --- REVISED FIXTURE ---
@pytest_asyncio.fixture
async def memory_core(temp_test_dir, request):
    """Provides a properly configured SynthiansMemoryCore instance with cleanup."""
    # Create a test-specific directory within the temp directory
    test_name = request.node.name
    test_dir = os.path.join(temp_test_dir, test_name.replace('/', '_').replace(':', '_')) # Added replace for ':'
    os.makedirs(test_dir, exist_ok=True)
    print(f"\nSetting up memory_core fixture for test: {test_name} in {test_dir}")

    # Create components for testing
    geometry_manager = GeometryManager(
        config={
            'embedding_dim': 384,
            'geometry_type': 'euclidean',
        }
    )

    vector_index = MemoryVectorIndex(
        config={
            'embedding_dim': 384,
            'storage_path': test_dir,
            'index_type': 'L2',
            'use_gpu': False
        }
    )

    # Use the original MemoryPersistence, but we'll override its lock later
    persistence = MemoryPersistence(
        config={
            'storage_path': test_dir,
            # 'auto_save': True # Removed, rely on explicit saves/shutdown
        }
    )
    # Initialize persistence explicitly before creating core
    await persistence.initialize()

    # Create the memory core instance with only a config
    core = SynthiansMemoryCore(
        config={
            'embedding_dim': 384,
            'storage_path': test_dir,
            'vector_index_type': 'L2',
            'use_gpu': False,
            # Disable background tasks for unit testing updates
            'persistence_interval': 3600 * 24,
            'decay_interval': 3600 * 24,
            'prune_check_interval': 3600 * 24,
        }
    )

    # Manually replace the components for testing
    core.vector_index = vector_index
    core.persistence = persistence
    core.geometry_manager = geometry_manager

    # Replace the locks with dummy locks
    dummy_lock = DummyAsyncLock()
    core._lock = dummy_lock
    persistence._lock = dummy_lock

    # Initialize core - crucial step to load state and start components
    # Background tasks are disabled by high intervals in config
    await core.initialize()

    # Setup cleanup function
    async def async_finalizer():
        """Async cleanup function that properly awaits shutdown"""
        print(f"\n==== Cleaning up memory_core for test: {test_name} ====")
        
        # First, set the shutdown signal to stop background loops
        if hasattr(core, '_shutdown_signal'):
            core._shutdown_signal.set()
            print("- Set shutdown signal")
        
        # Wait a moment for tasks to observe the signal
        await asyncio.sleep(0.2)
        
        # Explicitly get all tasks that might be associated with this test
        # to ensure we don't leave anything hanging
        all_tasks = asyncio.all_tasks()
        tasks_to_cancel = [
            t for t in all_tasks 
            if not t.done() and 
               t is not asyncio.current_task() and
               'test_' in t.get_name()  # Only care about test-related tasks
        ]
        
        # Cancel all background tasks associated with the test
        if tasks_to_cancel:
            print(f"- Found {len(tasks_to_cancel)} tasks to cancel")
            for task in tasks_to_cancel:
                if not task.done() and not task.cancelled():
                    task.cancel()
                    print(f"  - Cancelled task: {task.get_name()}")
        
            # Wait for the tasks to finish cancelling
            try:
                await asyncio.wait(tasks_to_cancel, timeout=2)
                print("- Waited for tasks to cancel")
            except Exception as e:
                print(f"- Error waiting for tasks: {e}")
        
        # Now run the core's shutdown method
        if hasattr(core, 'shutdown'):
            print("- Running core.shutdown()...")
            try:
                await asyncio.wait_for(core.shutdown(), timeout=3)
                print("- Shutdown completed")
            except asyncio.TimeoutError:
                print("- Warning: Shutdown timed out")
            except Exception as e:
                print(f"- Error during shutdown: {e}")
        
        # Finally, remove the test directory
        if os.path.exists(test_dir):
            print(f"- Removing test directory: {test_dir}")
            await _remove_directory_with_retry(test_dir, max_attempts=3)
            
        print(f"==== Cleanup finished for test: {test_name} ====")
            
    def finalizer():
        """Sync wrapper for the async finalizer"""
        loop = asyncio.get_event_loop_policy().get_event_loop()
        
        # If we're in an event loop running from pytest_asyncio
        if loop.is_running():
            task = asyncio.create_task(
                async_finalizer(), 
                name=f"finalizer_{test_name}"
            )
            
            # We need to ensure this task completes, but we can't await directly
            # Create a shared event for signaling completion
            done_event = threading.Event()
            
            def _on_task_done(task):
                # Signal that the task is done, regardless of result
                done_event.set()
                
            task.add_done_callback(_on_task_done)
            
            # Wait for the task to complete with a timeout
            # This is a blocking wait, but it's necessary for cleanup
            if not done_event.wait(timeout=5):
                print("Warning: Cleanup task timed out!")
        else:
            # If we're not in a running loop (shouldn't happen with pytest_asyncio)
            loop.run_until_complete(async_finalizer())
    
    # Register the cleanup function
    request.addfinalizer(finalizer)
    
    # Return the memory core for use in tests
    return core

# --- Tests ---

@pytest.mark.asyncio
async def test_get_memory_by_id(memory_core: SynthiansMemoryCore):
    """Test retrieving a memory by ID."""
    print("\n--- Running test_get_memory_by_id ---")
    # Create a test memory using the correct method
    timestamp = datetime.now(timezone.utc)
    content = f"Test memory for retrieval at {timestamp.isoformat()}"
    original_embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32) # Use configured dim

    # Normalize the original embedding *before* sending, as the core will normalize it.
    normalized_original_embedding = memory_core.geometry_manager.normalize_embedding(original_embedding)

    print(f"Creating memory '{content[:20]}...'")
    # Use process_new_memory to store, passing the original (will be normalized inside)
    memory_entry = await memory_core.process_new_memory(
        content=content,
        embedding=original_embedding, # Send the original random one
        metadata={"source": "test_get_by_id", "importance": 0.75}
    )
    assert memory_entry is not None, "Failed to create Memory A"
    memory_id = memory_entry.id
    print(f"Memory created with ID: {memory_id}")

    # Retrieve the memory by ID using the core method
    print(f"Retrieving memory {memory_id}...")
    # Use the core's synchronous method directly (as it accesses internal dict)
    # Ensure the lock is the dummy one to avoid blocking
    assert isinstance(memory_core._lock, DummyAsyncLock)
    retrieved_memory = memory_core.get_memory_by_id(memory_id)

    # Assert memory was retrieved
    assert retrieved_memory is not None, f"Memory with ID {memory_id} was not found"
    assert isinstance(retrieved_memory, MemoryEntry), "get_memory_by_id did not return a MemoryEntry object"
    print("Memory retrieved successfully.")

    # Verify memory contents using object attributes
    assert retrieved_memory.id == memory_id
    assert retrieved_memory.content == content, "Retrieved memory content does not match original"
    assert retrieved_memory.embedding is not None, "Retrieved memory embedding is None"

    # Compare the *retrieved* embedding with the *normalized version* of the original
    print("Comparing embeddings...")
    assert np.allclose(retrieved_memory.embedding, normalized_original_embedding, atol=1e-6), \
        f"Retrieved memory embedding does not match the normalized original.\nRetrieved (first 5): {retrieved_memory.embedding[:5]}\nExpected (first 5): {normalized_original_embedding[:5]}"
    print("Embeddings match.")

    assert retrieved_memory.metadata.get("source") == "test_get_by_id", "Retrieved memory metadata does not match original"

    # Test retrieving non-existent memory
    print("Testing retrieval of non-existent memory...")
    non_existent_id = "non_existent_id_12345"
    non_existent_memory = memory_core.get_memory_by_id(non_existent_id)
    assert non_existent_memory is None, f"Memory with non-existent ID {non_existent_id} was found"
    print("Non-existent memory test passed.")
    print("--- test_get_memory_by_id PASSED ---")


@pytest.mark.asyncio
async def test_update_quickrecal_score(memory_core: SynthiansMemoryCore):
    """Test updating the QuickRecal score of a memory."""
    print("\n--- Running test_update_quickrecal_score ---")
    # Create a test memory
    timestamp = datetime.now(timezone.utc)
    content = f"Test memory for QuickRecal update at {timestamp.isoformat()}"
    embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32)

    # Use process_new_memory
    print("Creating memory...")
    try:
        async with asyncio.timeout(10):  # 10 second timeout for memory creation
            memory_entry = await memory_core.process_new_memory(
                content=content,
                embedding=embedding,
                metadata={"source": "test_update_quickrecal"}
            )
        assert memory_entry is not None, "Failed to create memory"
        memory_id = memory_entry.id
        initial_score_actual = memory_entry.quickrecal_score # Get the actual initial score
        print(f"Memory created (ID: {memory_id}), initial score: {initial_score_actual:.6f}")
    except asyncio.TimeoutError:
        pytest.fail("Timed out waiting for process_new_memory to complete - possible deadlock")

    # Give time for any background tasks to complete (though they shouldn't run)
    await asyncio.sleep(0.1)

    # Verify initial score
    # Use synchronous get_memory_by_id
    assert isinstance(memory_core._lock, DummyAsyncLock)
    memory_before = memory_core.get_memory_by_id(memory_id)
    assert memory_before is not None, f"Memory {memory_id} not found"
    assert abs(memory_before.quickrecal_score - initial_score_actual) < 1e-6

    # Update QuickRecal score
    new_score = 0.9
    print(f"Updating score to {new_score}...")

    # Use a timeout to prevent indefinite waiting if deadlock occurs
    try:
        async with asyncio.timeout(5):  # 5 seconds should be plenty
            updated = await memory_core.update_memory(
                memory_id=memory_id,
                updates={"quickrecal_score": new_score}
            )
        assert updated is True, "Memory update failed"
        print("Update successful.")
    except asyncio.TimeoutError:
        pytest.fail("Timed out waiting for update_memory to complete - possible deadlock")

    # Give time for persistence (even though loop is disabled, update_memory calls save)
    await asyncio.sleep(0.1)

    # Verify updated score
    # Use synchronous get_memory_by_id
    memory_after = memory_core.get_memory_by_id(memory_id)
    assert memory_after is not None, f"Memory {memory_id} not found after update"
    assert abs(memory_after.quickrecal_score - new_score) < 1e-6, \
        f"QuickRecal score was not updated correctly (Expected: {new_score}, Found: {memory_after.quickrecal_score})"
    print(f"Score updated to: {memory_after.quickrecal_score:.6f}")

    # Test update clamping (high)
    print("Testing high score clamping...")
    # Use a timeout to prevent indefinite waiting if deadlock occurs
    try:
        async with asyncio.timeout(5):  # 5 seconds timeout
            updated = await memory_core.update_memory(
                memory_id=memory_id,
                updates={"quickrecal_score": 1.5}  # Should be clamped to 1.0
            )
        assert updated is True, "Memory update (high score) failed"
    except asyncio.TimeoutError:
        pytest.fail("Timed out waiting for update_memory (high score) to complete - possible deadlock")

    # Verify clamped score
    await asyncio.sleep(0.1)
    memory_after_high = memory_core.get_memory_by_id(memory_id)
    assert memory_after_high is not None
    assert abs(memory_after_high.quickrecal_score - 1.0) < 1e-6, \
        f"Score was not properly clamped (Expected: 1.0, Found: {memory_after_high.quickrecal_score})"
    print(f"Score clamped to: {memory_after_high.quickrecal_score:.6f}")

    # Test update clamping (low)
    print("Testing low score clamping...")
    try:
        async with asyncio.timeout(5):  # 5 seconds timeout
            updated = await memory_core.update_memory(
                memory_id=memory_id,
                updates={"quickrecal_score": -0.5}  # Should be clamped to 0.0
            )
        assert updated is True, "Memory update (low score) failed"
    except asyncio.TimeoutError:
        pytest.fail("Timed out waiting for update_memory (low score) to complete - possible deadlock")

    # Verify clamped score
    await asyncio.sleep(0.1)
    memory_after_low = memory_core.get_memory_by_id(memory_id)
    assert memory_after_low is not None
    assert abs(memory_after_low.quickrecal_score - 0.0) < 1e-6, \
        f"Score was not properly clamped (Expected: 0.0, Found: {memory_after_low.quickrecal_score})"
    print(f"Score clamped to: {memory_after_low.quickrecal_score:.6f}")
    print("--- test_update_quickrecal_score PASSED ---")


@pytest.mark.asyncio
async def test_update_metadata(memory_core: SynthiansMemoryCore):
    """Test updating metadata of a memory."""
    print("\n--- Running test_update_metadata ---")
    # Create a test memory
    timestamp = datetime.now(timezone.utc)
    content = f"Test memory for metadata update at {timestamp.isoformat()}"
    embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32)
    initial_metadata = {
        "source": "test_update_metadata",
        "tags": ["test", "metadata"],
        "nested": {"key1": "value1", "key2": "value2"}
    }
    memory_entry = await memory_core.process_new_memory(
        content=content,
        embedding=embedding,
        metadata=initial_metadata.copy()
    )
    assert memory_entry is not None, "Failed to create memory"
    memory_id = memory_entry.id
    print(f"Memory created (ID: {memory_id})")

    # Verify initial custom metadata persisted (synthesized data also exists)
    memory_before = memory_core.get_memory_by_id(memory_id)
    assert memory_before is not None
    assert memory_before.metadata.get("source") == initial_metadata["source"]
    assert set(memory_before.metadata.get("tags", [])) == set(initial_metadata["tags"]) # Use set for order independence
    assert memory_before.metadata.get("nested") == initial_metadata["nested"]
    print("Initial metadata verified.")

    # Update metadata
    metadata_updates = {
        "category": "tested",
        "tags": ["test", "metadata", "updated"], # Replace list
        "nested": {"key1": "updated_value1", "key3": "new_value3"}, # Merge dict
        "another_new_field": 123
    }
    print(f"Updating metadata with: {metadata_updates}")
    updated = await memory_core.update_memory(
        memory_id=memory_id,
        updates={"metadata": metadata_updates}
    )
    assert updated is True, "Memory metadata update failed"
    print("Metadata update successful.")

    # Verify updated metadata
    await asyncio.sleep(0.1) # Allow persistence
    memory_after = memory_core.get_memory_by_id(memory_id)
    assert memory_after is not None
    final_metadata = memory_after.metadata
    print(f"Final Metadata: {json.dumps(final_metadata, indent=2)}")

    # Check updated and added fields
    assert final_metadata.get("category") == "tested"
    assert set(final_metadata.get("tags", [])) == set(["test", "metadata", "updated"])
    assert final_metadata.get("another_new_field") == 123

    # Check merged nested field updates
    assert final_metadata.get("nested", {}).get("key1") == "updated_value1"
    assert final_metadata.get("nested", {}).get("key3") == "new_value3"
    # Check original nested field persisted
    assert final_metadata.get("nested", {}).get("key2") == "value2"

    # Check original top-level field persisted
    assert final_metadata.get("source") == "test_update_metadata"
    print("--- test_update_metadata PASSED ---")


@pytest.mark.asyncio
async def test_update_invalid_fields(memory_core: SynthiansMemoryCore):
    """Test updating with invalid/non-existent fields."""
    print("\n--- Running test_update_invalid_fields ---")
    timestamp = datetime.now(timezone.utc)
    content = f"Test memory for invalid field update at {timestamp.isoformat()}"
    embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32)
    memory_entry = await memory_core.process_new_memory(
        content=content,
        embedding=embedding,
        metadata={"source": "test_invalid_fields"}
    )
    assert memory_entry is not None, "Failed to create memory"
    memory_id = memory_entry.id

    # Try to update with invalid field
    print("Attempting update with invalid field 'invalid_field_xyz'...")
    updated_invalid = await memory_core.update_memory(
        memory_id=memory_id,
        updates={"invalid_field_xyz": "some_value"}
    )
    # Update should still likely return True if it ignores bad fields
    print(f"Update call returned: {updated_invalid}")
    await asyncio.sleep(0.1)
    memory_after_invalid = memory_core.get_memory_by_id(memory_id)
    assert memory_after_invalid is not None
    assert not hasattr(memory_after_invalid, "invalid_field_xyz"), "Invalid field was added to memory object"
    assert "invalid_field_xyz" not in memory_after_invalid.metadata, "Invalid field was added to metadata"
    print("Verified invalid field was ignored.")

    # Try to update with a valid field and an invalid field
    initial_score = memory_after_invalid.quickrecal_score
    print(f"Attempting update with valid score and invalid field ('another_invalid_field'). Initial score: {initial_score}")
    updated_mixed = await memory_core.update_memory(
        memory_id=memory_id,
        updates={
            "quickrecal_score": 0.77,
            "another_invalid_field": "another_value"
        }
    )
    assert updated_mixed is True, "Mixed update failed"
    print("Mixed update successful.")

    await asyncio.sleep(0.1)
    memory_after_mixed = memory_core.get_memory_by_id(memory_id)
    assert memory_after_mixed is not None
    assert abs(memory_after_mixed.quickrecal_score - 0.77) < 1e-6, "Valid field 'quickrecal_score' was not updated during mixed update"
    assert not hasattr(memory_after_mixed, "another_invalid_field"), "Invalid field was added to memory object during mixed update"
    assert "another_invalid_field" not in memory_after_mixed.metadata, "Invalid field was added to metadata during mixed update"
    print("Verified mixed update handled correctly.")
    print("--- test_update_invalid_fields PASSED ---")


@pytest.mark.asyncio
async def test_update_nonexistent_memory(memory_core: SynthiansMemoryCore):
    """Test updating a memory that doesn't exist."""
    print("\n--- Running test_update_nonexistent_memory ---")
    non_existent_id = "non_existent_id_98765"
    print(f"Attempting update for non-existent ID: {non_existent_id}")
    updated = await memory_core.update_memory(
        memory_id=non_existent_id,
        updates={"quickrecal_score": 0.8}
    )
    assert updated is False, "Update to non-existent memory reported success"
    print("Verified update returned False for non-existent memory.")
    print("--- test_update_nonexistent_memory PASSED ---")


@pytest.mark.asyncio
async def test_update_persistence(memory_core: SynthiansMemoryCore, temp_test_dir, request):
    """Test that updates are persisted properly by reloading."""
    print("\n--- Running test_update_persistence ---")
    test_name = request.node.name
    test_dir = os.path.join(temp_test_dir, test_name.replace('/', '_').replace(':', '_'))
    print(f"Using test directory: {test_dir}")

    timestamp = datetime.now(timezone.utc)
    content = f"Test memory for persistence at {timestamp.isoformat()}"
    embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32)
    print("Creating initial memory...")
    memory_entry = await memory_core.process_new_memory(
        content=content,
        embedding=embedding,
        metadata={"source": "test_persistence"},
    )
    assert memory_entry is not None, "Failed to create memory"
    memory_id = memory_entry.id
    print(f"Memory created (ID: {memory_id})")

    # Update the memory
    new_score = 0.88
    new_meta_value = "updated_value"
    update_timestamp_iso = datetime.now(timezone.utc).isoformat()
    updates_dict = {
        "quickrecal_score": new_score,
        "metadata": {"update_status": new_meta_value, "last_update_iso": update_timestamp_iso }
    }
    print(f"Updating memory {memory_id} with: {updates_dict}")
    updated = await memory_core.update_memory(
        memory_id=memory_id,
        updates=updates_dict
    )
    assert updated is True, "Memory update failed"
    print("Memory update successful.")

    # Ensure persistence happens - explicitly call save if loops disabled
    await memory_core.persistence.save_memory(memory_core.get_memory_by_id(memory_id))
    await memory_core.persistence._save_index() # Force save index
    print("Explicit save performed.")
    await asyncio.sleep(0.2) # Small delay

    # --- Simulate Restart ---
    print("Shutting down original memory core...")
    # Need to shut down cleanly to ensure files are closed
    # await memory_core.shutdown() # Shutdown might cause issues with fixture cleanup

    # Create a NEW memory core instance using the SAME config
    config = memory_core.config # Reuse config dict
    print(f"Re-initializing Memory Core with storage path: {config['storage_path']}")
    new_memory_core = SynthiansMemoryCore(config=config)
    # Replace locks with dummy locks for the new instance too
    new_memory_core._lock = DummyAsyncLock()
    new_memory_core.persistence._lock = DummyAsyncLock()
    # Initialize the new core, which loads from persistence
    await new_memory_core.initialize()
    print("New memory core initialized, loading from persistence.")

    # Retrieve the memory from the new instance
    print(f"Retrieving memory {memory_id} from reloaded core...")
    memory_after_reload = new_memory_core.get_memory_by_id(memory_id)

    # Verify the updated values were loaded from persistence
    assert memory_after_reload is not None, f"Memory with ID {memory_id} was not found after reload"
    assert isinstance(memory_after_reload, MemoryEntry), "Did not get MemoryEntry object after reload"
    print("Memory retrieved after reload.")

    assert abs(memory_after_reload.quickrecal_score - new_score) < 1e-6, \
        f"Updated QuickRecal score was not persisted (Expected: {new_score}, Found: {memory_after_reload.quickrecal_score})"
    assert memory_after_reload.metadata.get("update_status") == new_meta_value, \
        "Updated metadata field 'update_status' was not persisted"
    assert memory_after_reload.metadata.get("last_update_iso") == update_timestamp_iso, \
        "Added metadata field 'last_update_iso' was not persisted"
    assert memory_after_reload.metadata.get("source") == "test_persistence", \
        "Original metadata field 'source' was lost during update/persistence"
    print("Verified persisted updates.")

    # await new_memory_core.shutdown() # Shutdown the new core instance
    print("--- test_update_persistence PASSED ---")


@pytest.mark.asyncio
async def test_quickrecal_updated_timestamp(memory_core: SynthiansMemoryCore):
    """Test that quickrecal_updated_at timestamp is set correctly in metadata."""
    print("\n--- Running test_quickrecal_updated_timestamp ---")
    content = "Test memory for quickrecal timestamp"
    embedding = np.random.rand(memory_core.config['embedding_dim']).astype(np.float32)

    print("Creating memory...")
    memory_entry = await memory_core.process_new_memory(content=content, embedding=embedding)
    assert memory_entry is not None, "Failed to create memory"
    memory_id = memory_entry.id

    memory_before = memory_core.get_memory_by_id(memory_id)
    assert memory_before.metadata.get('quickrecal_updated_at') is None, \
        "quickrecal_updated_at should be None initially in metadata"
    print("Initial state verified (no quickrecal_updated_at).")

    # Update score
    await asyncio.sleep(0.1)
    time_before_update = datetime.now(timezone.utc)
    await asyncio.sleep(0.1)

    print("Updating quickrecal_score...")
    updated = await memory_core.update_memory(
        memory_id=memory_id,
        updates={"quickrecal_score": 0.9}
    )
    assert updated is True

    await asyncio.sleep(0.1)
    time_after_update = datetime.now(timezone.utc)
    await asyncio.sleep(0.1) # Allow persistence

    memory_after = memory_core.get_memory_by_id(memory_id)
    assert memory_after is not None

    updated_at_str = memory_after.metadata.get('quickrecal_updated_at')
    assert updated_at_str is not None, "quickrecal_updated_at was not set in metadata"
    print(f"Found quickrecal_updated_at: {updated_at_str}")

    # Parse and compare timestamp
    try:
        if updated_at_str.endswith('Z'): updated_at_str = updated_at_str[:-1] + '+00:00'
        updated_at_dt = datetime.fromisoformat(updated_at_str)
        if updated_at_dt.tzinfo is None: updated_at_dt = updated_at_dt.replace(tzinfo=timezone.utc)
        if time_before_update.tzinfo is None: time_before_update = time_before_update.replace(tzinfo=timezone.utc)
        if time_after_update.tzinfo is None: time_after_update = time_after_update.replace(tzinfo=timezone.utc)

        assert time_before_update <= updated_at_dt <= time_after_update, \
            f"quickrecal_updated_at timestamp ({updated_at_dt}) is outside the expected update window ({time_before_update} - {time_after_update})"
        print("Timestamp is within expected range.")
    except ValueError:
        pytest.fail(f"Could not parse quickrecal_updated_at timestamp: {updated_at_str}")

    # Update metadata only, timestamp should NOT change
    await asyncio.sleep(0.1)
    print("Updating metadata only...")
    updated_meta = await memory_core.update_memory(
        memory_id=memory_id,
        updates={"metadata": {"another_field": "value"}}
    )
    assert updated_meta is True
    await asyncio.sleep(0.1) # Allow persistence
    memory_after_meta = memory_core.get_memory_by_id(memory_id)
    assert memory_after_meta.metadata.get('quickrecal_updated_at') == updated_at_str, \
        "quickrecal_updated_at changed when only metadata was updated"
    print("Verified quickrecal_updated_at unchanged after metadata-only update.")
    print("--- test_quickrecal_updated_timestamp PASSED ---")
```

# tests\test_memory_diagnostic.py

```py
import pytest
import asyncio
import json
import time
import numpy as np
import uuid
import logging
from datetime import datetime
from synthians_memory_core.api.client.client import SynthiansClient

# Configure logging to see detailed output
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

@pytest.mark.asyncio
async def test_memory_creation_and_retrieval_lifecycle():
    """Test the complete lifecycle from memory creation to retrieval to diagnose '0 memories' issue."""
    async with SynthiansClient() as client:
        # Generate a unique test identifier
        test_id = uuid.uuid4().hex[:8]
        print(f"\n\n*** MEMORY DIAGNOSTIC TEST ({test_id}) ***\n")
        print("Skipping initial stats check (method not available in client API)")
        
        # 2. Create a set of unique test memories with clear, distinctive content
        memory_contents = [
            f"This is a DIAGNOSTIC test memory ONE with unique ID {test_id}",
            f"This is a DIAGNOSTIC test memory TWO with completely different content {test_id}",
            f"The third DIAGNOSTIC test memory with yet another unique phrase {test_id}"
        ]
        
        memory_responses = []
        memory_ids = []
        
        print("\nCreating test memories...")
        for i, content in enumerate(memory_contents):
            metadata = {
                "source": "diagnostic_test",
                "test_id": test_id,
                "memory_number": i + 1,
                "timestamp": datetime.now().isoformat()
            }
            
            # Process the memory
            response = await client.process_memory(content=content, metadata=metadata)
            memory_responses.append(response)
            
            # Extract and store the memory ID
            if response.get("success") and "memory_id" in response:
                memory_id = response["memory_id"]
                memory_ids.append(memory_id)
                print(f"Created memory {i+1} with ID: {memory_id}")
            else:
                print(f"Failed to create memory {i+1}: {response}")
        
        # Wait to ensure memories are processed and indexed
        print("\nWaiting for memories to be processed and indexed...")
        await asyncio.sleep(2)
        
        # 3. Skip stats check after creation as get_stats not available
        print("\nSkipping stats check after creation (method not available in client API)")
        
        # 4. Attempt direct retrieval by ID
        print("\nSkipping direct memory retrieval by ID (method not available in client API)")
        
        # 5. Attempt retrieval with exact content match query
        for i, content in enumerate(memory_contents):
            # Extract a distinctive phrase for the query
            query = f"DIAGNOSTIC test memory {['ONE', 'TWO', 'third'][i]} {test_id}"
            
            print(f"\nQuerying for memory {i+1} with: '{query}'")
            retrieval_response = await client.retrieve_memories(
                query=query,
                top_k=5,
                threshold=0.0  # Set to 0 to ensure low threshold
            )
            
            memories = retrieval_response.get("memories", [])
            print(f"Retrieved {len(memories)} memories for query {i+1}")
            
            if memories:
                for j, mem in enumerate(memories[:3]):  # Show top 3
                    mem_content = mem.get("content", "")[:50]
                    mem_id = mem.get("id")
                    similarity = mem.get("similarity", 0.0)
                    print(f"  Result {j+1}: ID={mem_id}, Similarity={similarity:.4f}, Content={mem_content}...")
                
                # Check if our specific memory was returned
                found = any(test_id in mem.get("content", "") and f"{['ONE', 'TWO', 'third'][i]}" in mem.get("content", "") 
                           for mem in memories)
                print(f"Target memory found in results: {found}")
            else:
                print(f"  NO MEMORIES RETURNED for query {i+1}")
        
        # 6. Attempt retrieval with metadata filter
        print("\nAttempting retrieval with metadata filter...")
        metadata_response = await client.retrieve_memories(
            query="DIAGNOSTIC test",
            top_k=10,
            metadata_filter={"test_id": test_id}
        )
        
        meta_memories = metadata_response.get("memories", [])
        print(f"Retrieved {len(meta_memories)} memories with metadata filter")
        
        if meta_memories:
            for j, mem in enumerate(meta_memories[:3]):  # Show top 3
                mem_content = mem.get("content", "")[:50]
                mem_id = mem.get("id")
                print(f"  Result {j+1}: ID={mem_id}, Content={mem_content}...")
        else:
            print("  NO MEMORIES RETURNED with metadata filter")
        
        # 7. Skip final stats verification
        print("\nSkipping final stats check (method not available in client API)")
        
        # Simplified assertions to ensure test validity
        assert len(memory_ids) > 0, "No memories were created successfully"

```

# tests\test_memory_lifecycle.py

```py
import pytest
import asyncio
import json
import time
from datetime import datetime
from synthians_memory_core.api.client.client import SynthiansClient

@pytest.mark.asyncio
async def test_basic_memory_flow():
    """Test the basic memory creation, retrieval, and feedback flow."""
    async with SynthiansClient() as client:
        # Step 1: Create a unique memory with a timestamp
        current_time = datetime.now().isoformat()
        content = f"Testing memory processing lifecycle at {current_time}"
        memory_resp = await client.process_memory(
            content=content,
            metadata={"source": "test_suite", "importance": 0.8}
        )
        
        # Assert successful creation
        assert memory_resp.get("success") is True, f"Memory creation failed: {memory_resp.get('error')}"
        memory_id = memory_resp.get("memory_id")
        assert memory_id is not None, "No memory ID returned"
        
        # Print for debugging
        print(f"Memory created with ID: {memory_id}")
        print(f"Memory response: {json.dumps(memory_resp, indent=2)}")
        
        # Step 2: Retrieve the memory
        # Use a unique portion of the content to ensure we get this specific memory
        query = f"memory processing lifecycle at {current_time}"
        # Add a lower threshold to ensure retrieval works
        retrieval_resp = await client.retrieve_memories(query, top_k=3, threshold=0.2)
        
        # Assert successful retrieval
        assert retrieval_resp.get("success") is True, f"Memory retrieval failed: {retrieval_resp.get('error')}"
        memories = retrieval_resp.get("memories", [])
        assert len(memories) > 0, "No memories retrieved"
        
        # Check if our specific memory was retrieved
        retrieved_ids = [m.get("id") for m in memories]
        assert memory_id in retrieved_ids, f"Created memory {memory_id} not found in retrieved memories: {retrieved_ids}"
        
        # Print for debugging
        print(f"Retrieved {len(memories)} memories")
        print(f"Retrieved memory IDs: {retrieved_ids}")
        
        # Step 3: Provide feedback
        feedback_resp = await client.provide_feedback(
            memory_id=memory_id,
            similarity_score=0.85,
            was_relevant=True
        )
        
        # Assert successful feedback
        assert feedback_resp.get("success") is True, f"Feedback submission failed: {feedback_resp.get('error')}"
        assert "new_threshold" in feedback_resp, "No threshold adjustment information returned"
        
        # Print for debugging
        print(f"Feedback response: {json.dumps(feedback_resp, indent=2)}")

@pytest.mark.asyncio
async def test_memory_persistence_roundtrip():
    """Test that memories persist and can be retrieved after creation."""
    async with SynthiansClient() as client:
        # Create a unique memory
        unique_id = int(time.time() * 1000)
        content = f"Persistence test memory with unique ID: {unique_id}"
        
        # Create the memory
        creation_resp = await client.process_memory(content=content)
        assert creation_resp.get("success") is True, "Memory creation failed"
        memory_id = creation_resp.get("memory_id")
        
        # Wait briefly to ensure persistence
        await asyncio.sleep(0.5)
        
        # Retrieve the memory with the unique identifier
        retrieval_resp = await client.retrieve_memories(f"unique ID: {unique_id}", top_k=5, threshold=0.2)
        print(f"\nRetrieval response: {json.dumps(retrieval_resp, indent=2)}")
        assert retrieval_resp.get("success") is True, f"Memory retrieval failed: {retrieval_resp.get('error', 'No error specified')}"
        
        # Verify the memory was retrieved
        memories = retrieval_resp.get("memories", [])
        retrieved_ids = [m.get("id") for m in memories]
        assert memory_id in retrieved_ids, f"Memory {memory_id} not persisted/retrieved"

@pytest.mark.asyncio
async def test_metadata_enrichment_on_store():
    """Test that metadata is properly enriched when storing memories."""
    async with SynthiansClient() as client:
        # Create a memory with minimal metadata
        content = "Test memory for metadata enrichment"
        metadata = {"source": "test_suite", "custom_field": "custom_value"}
        
        response = await client.process_memory(content=content, metadata=metadata)
        assert response.get("success") is True, "Memory creation failed"
        
        # Verify metadata enrichment
        returned_metadata = response.get("metadata", {})
        
        # Check that our custom metadata was preserved
        assert returned_metadata.get("source") == "test_suite"
        assert returned_metadata.get("custom_field") == "custom_value"
        
        # Check that system metadata was added
        assert "timestamp" in returned_metadata, "Timestamp metadata missing"
        assert "length" in returned_metadata, "Length metadata missing"
        assert "uuid" in returned_metadata, "UUID metadata missing"
        
        # Optional checks for more advanced metadata
        if "cognitive_complexity" in returned_metadata:
            assert isinstance(returned_metadata["cognitive_complexity"], (int, float))
        
        print(f"Enriched metadata: {json.dumps(returned_metadata, indent=2)}")

@pytest.mark.asyncio
async def test_delete_memory_by_id():
    """Test memory deletion functionality."""
    async with SynthiansClient() as client:
        # Create a memory
        content = f"Memory to be deleted at {datetime.now().isoformat()}"
        creation_resp = await client.process_memory(content=content)
        assert creation_resp.get("success") is True, "Memory creation failed"
        memory_id = creation_resp.get("memory_id")
        
        # TODO: Implement actual delete endpoint call once available
        # This is a placeholder for when the delete endpoint is implemented
        
        # Example of how delete might be implemented:
        # delete_resp = await client.delete_memory(memory_id=memory_id)
        # assert delete_resp.get("success") is True, "Memory deletion failed"
        
        # After implementing deletion, verify the memory is gone:
        # retrieval_resp = await client.retrieve_memories(content, top_k=1)  
        # memories = retrieval_resp.get("memories", [])
        # assert memory_id not in [m.get("id") for m in memories], "Memory still exists after deletion"

```

# tests\test_retrieval_dynamics.py

```py
import pytest
import asyncio
import json
import time
import numpy as np
from synthians_memory_core.api.client.client import SynthiansClient

@pytest.mark.asyncio
async def test_retrieve_with_emotion_match():
    """Test retrieval with emotional matching."""
    async with SynthiansClient() as client:
        # Create memories with different emotions
        happy_memory = await client.process_memory(
            content="I'm so excited about this amazing project! Everything is going wonderfully!",
            metadata={"source": "emotion_test", "test_group": "retrieval_emotion"}
        )
        
        sad_memory = await client.process_memory(
            content="I'm feeling really down today. Nothing seems to be working out.",
            metadata={"source": "emotion_test", "test_group": "retrieval_emotion"}
        )
        
        angry_memory = await client.process_memory(
            content="I'm absolutely furious about how this situation was handled!",
            metadata={"source": "emotion_test", "test_group": "retrieval_emotion"}
        )
        
        # Wait briefly for processing
        await asyncio.sleep(1)
        
        # Retrieve with happy emotion context
        happy_emotion = {"dominant_emotion": "joy", "emotions": {"joy": 0.8, "surprise": 0.2}}
        happy_results = await client.retrieve_memories(
            query="feeling emotion test",
            top_k=5,
            user_emotion=happy_emotion
        )
        
        # Retrieve with sad emotion context
        sad_emotion = {"dominant_emotion": "sadness", "emotions": {"sadness": 0.9}}
        sad_results = await client.retrieve_memories(
            query="feeling emotion test",
            top_k=5,
            user_emotion=sad_emotion
        )
        
        # If emotional gating is working correctly, happy memories should rank higher
        # when queried with happy emotion, and sad memories with sad emotion
        happy_memories = happy_results.get("memories", [])
        sad_memories = sad_results.get("memories", [])
        
        print(f"Happy emotion results (first memory): {json.dumps(happy_memories[0] if happy_memories else {}, indent=2)}")
        print(f"Sad emotion results (first memory): {json.dumps(sad_memories[0] if sad_memories else {}, indent=2)}")
        
        # Note: These assertions might be too strict depending on implementation
        # The exact ranking will depend on many factors
        if happy_memories and sad_memories:
            for memory in happy_memories:
                if memory.get("content", "").startswith("I'm so excited"):
                    happy_rank = happy_memories.index(memory)
                    break
            else:
                happy_rank = -1
                
            for memory in sad_memories:
                if memory.get("content", "").startswith("I'm feeling really down"):
                    sad_rank = sad_memories.index(memory)
                    break
            else:
                sad_rank = -1
            
            print(f"Happy memory rank in happy query: {happy_rank}")
            print(f"Sad memory rank in sad query: {sad_rank}")

@pytest.mark.asyncio
async def test_retrieve_with_low_threshold():
    """Test retrieval with different threshold values."""
    async with SynthiansClient() as client:
        # Create a unique memory
        unique_id = int(time.time())
        unique_content = f"This is a unique threshold test memory {unique_id}"
        
        memory_resp = await client.process_memory(content=unique_content)
        memory_id = memory_resp.get("memory_id")
        
        # Wait briefly
        await asyncio.sleep(0.5)
        
        # Query with high threshold
        high_threshold_resp = await client.retrieve_memories(
            query=f"completely unrelated query {unique_id}",  # Unrelated but with unique ID
            top_k=10,
            threshold=0.9  # High threshold should filter out most memories
        )
        
        # Query with low threshold
        low_threshold_resp = await client.retrieve_memories(
            query=f"completely unrelated query {unique_id}",  # Same unrelated query
            top_k=10,
            threshold=0.1  # Low threshold should include most memories
        )
        
        high_threshold_memories = high_threshold_resp.get("memories", [])
        low_threshold_memories = low_threshold_resp.get("memories", [])
        
        # Low threshold should return more memories than high threshold
        print(f"High threshold returned {len(high_threshold_memories)} memories")
        print(f"Low threshold returned {len(low_threshold_memories)} memories")
        
        # Check if the unique memory is in the low threshold results
        low_thresh_ids = [m.get("id") for m in low_threshold_memories]
        memory_found = memory_id in low_thresh_ids
        
        print(f"Memory found in low threshold results: {memory_found}")
        print(f"Low threshold memory IDs: {low_thresh_ids}")

@pytest.mark.asyncio
async def test_metadata_filtering():
    """Test retrieval with metadata filters."""
    async with SynthiansClient() as client:
        # Create memories with different metadata
        timestamp = int(time.time())
        
        # Create memory with importance=high
        high_importance = await client.process_memory(
            content=f"High importance memory {timestamp}",
            metadata={"importance": "high", "category": "test", "filter_test": True}
        )
        
        # Create memory with importance=medium
        medium_importance = await client.process_memory(
            content=f"Medium importance memory {timestamp}",
            metadata={"importance": "medium", "category": "test", "filter_test": True}
        )
        
        # Create memory with importance=low
        low_importance = await client.process_memory(
            content=f"Low importance memory {timestamp}",
            metadata={"importance": "low", "category": "test", "filter_test": True}
        )
        
        # Create memory with different category
        different_category = await client.process_memory(
            content=f"Different category memory {timestamp}",
            metadata={"importance": "high", "category": "other", "filter_test": True}
        )
        
        # Wait briefly
        await asyncio.sleep(1)
        
        # Test if we can filter by metadata
        # Note: This assumes the retrieve_memories endpoint supports metadata filtering
        # If not, this test will need to be adapted
        
        try:
            # Query for high importance memories only
            # This might need to be updated based on actual API implementation
            high_imp_query = await client.retrieve_memories(
                query=f"memory {timestamp}",
                top_k=10,
                metadata_filter={"importance": "high"}
            )
            
            # Query for test category memories only
            test_category_query = await client.retrieve_memories(
                query=f"memory {timestamp}",
                top_k=10,
                metadata_filter={"category": "test"}
            )
            
            high_imp_memories = high_imp_query.get("memories", [])
            test_cat_memories = test_category_query.get("memories", [])
            
            print(f"High importance query returned {len(high_imp_memories)} memories")
            print(f"Test category query returned {len(test_cat_memories)} memories")
            
            # Check that our filtered queries worked as expected
            high_imp_contents = [m.get("content", "") for m in high_imp_memories]
            test_cat_contents = [m.get("content", "") for m in test_cat_memories]
            
            print(f"High importance memory contents: {high_imp_contents}")
            print(f"Test category memory contents: {test_cat_contents}")
            
        except Exception as e:
            # This test may fail if the API doesn't support metadata filtering
            print(f"Metadata filtering test failed: {str(e)}")
            print("This feature may not be implemented yet or works differently.")

@pytest.mark.asyncio
async def test_top_k_ranking_accuracy():
    """Test that memory retrieval respects top_k parameter and ranks by relevance."""
    async with SynthiansClient() as client:
        # Create a set of memories with varying relevance to a specific query
        base_content = "This is a test of the ranking system"
        timestamp = int(time.time())
        
        # Create memories with varying relevance
        await client.process_memory(
            content=f"{base_content} with direct relevance to ranking and sorting. {timestamp}"
        )
        
        await client.process_memory(
            content=f"{base_content} with some relevance to sorting. {timestamp}"
        )
        
        await client.process_memory(
            content=f"{base_content} with minimal relevance. {timestamp}"
        )
        
        await client.process_memory(
            content=f"Completely unrelated content that shouldn't be ranked highly. {timestamp}"
        )
        
        # Create 10 more filler memories
        for i in range(10):
            await client.process_memory(
                content=f"Filler memory {i} for ranking test. {timestamp}"
            )
        
        # Wait briefly
        await asyncio.sleep(1)
        
        # Test with different top_k values
        top_3_results = await client.retrieve_memories(
            query=f"ranking and sorting test {timestamp}",
            top_k=3
        )
        
        top_5_results = await client.retrieve_memories(
            query=f"ranking and sorting test {timestamp}",
            top_k=5
        )
        
        top_10_results = await client.retrieve_memories(
            query=f"ranking and sorting test {timestamp}",
            top_k=10
        )
        
        # Verify the correct number of results returned
        assert len(top_3_results.get("memories", [])) <= 3, "top_k=3 returned too many results"
        assert len(top_5_results.get("memories", [])) <= 5, "top_k=5 returned too many results"
        assert len(top_10_results.get("memories", [])) <= 10, "top_k=10 returned too many results"
        
        # Check the ranking - most relevant should be first
        if top_10_results.get("memories"):
            # Get first result content
            first_result = top_10_results["memories"][0]["content"]
            print(f"First ranked result: {first_result}")
            
            # It should contain "ranking and sorting"
            assert "ranking and sorting" in first_result.lower(), "Most relevant content not ranked first"

```

# tests\test_stress_load.py

```py
import pytest
import asyncio
import json
import time
import random
import numpy as np
from datetime import datetime, timedelta
from synthians_memory_core.api.client.client import SynthiansClient

# Optional marker for these slow tests
pytestmark = pytest.mark.slow

@pytest.mark.asyncio
async def test_1000_memory_ingestion():
    """Test the system with a large number of memories (stress test)."""
    async with SynthiansClient() as client:
        start_time = time.time()
        memory_ids = []
        batch_size = 10  # Process in batches to avoid overwhelming the server
        total_memories = 100  # Reduced from 1000 for faster testing - set to 1000 for full stress test
        
        print(f"Starting bulk ingestion of {total_memories} memories...")
        
        # Generate text templates for variety
        templates = [
            "Remember to {action} the {object} at {time}.",
            "I need to {action} {count} {object}s before {time}.",
            "Don't forget that {person} is coming to {location} at {time}.",
            "The {event} is scheduled for {day} at {time}.",
            "Make sure to check the {object} in the {location}."
        ]
        
        actions = ["review", "check", "update", "clean", "fix", "prepare", "send", "receive"]
        objects = ["document", "report", "presentation", "email", "meeting", "project", "task", "schedule"]
        times = ["9:00 AM", "10:30 AM", "noon", "2:15 PM", "4:00 PM", "5:30 PM", "this evening", "tomorrow"]
        people = ["John", "Sara", "Michael", "Emma", "David", "Lisa", "Alex", "Olivia"]
        locations = ["office", "conference room", "lobby", "home", "cafe", "downtown", "upstairs", "kitchen"]
        days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        events = ["meeting", "conference", "workshop", "presentation", "lunch", "dinner", "call", "interview"]
        counts = ["two", "three", "four", "five", "several", "many", "a few", "some"]
        
        # Create memories in batches
        for batch in range(0, total_memories, batch_size):
            batch_tasks = []
            
            for i in range(batch, min(batch + batch_size, total_memories)):
                # Generate a random memory with a template
                template = random.choice(templates)
                content = template.format(
                    action=random.choice(actions),
                    object=random.choice(objects),
                    time=random.choice(times),
                    person=random.choice(people),
                    location=random.choice(locations),
                    day=random.choice(days),
                    event=random.choice(events),
                    count=random.choice(counts)
                )
                
                # Add a unique identifier
                content += f" (Memory #{i+1})"
                
                # Generate random metadata
                metadata = {
                    "batch": batch // batch_size,
                    "index": i,
                    "importance": random.uniform(0.1, 1.0),
                    "category": random.choice(["work", "personal", "reminder", "event"]),
                    "stress_test": True
                }
                
                # Create memory task
                task = client.process_memory(content=content, metadata=metadata)
                batch_tasks.append(task)
            
            # Process the batch
            batch_results = await asyncio.gather(*batch_tasks)
            
            # Collect memory IDs
            for result in batch_results:
                if result.get("success"):
                    memory_ids.append(result.get("memory_id"))
            
            # Log progress
            elapsed = time.time() - start_time
            progress = min(100, (len(memory_ids) / total_memories) * 100)
            print(f"Progress: {progress:.1f}% - {len(memory_ids)}/{total_memories} memories created in {elapsed:.2f} seconds")
            
            # Pause briefly between batches to avoid overwhelming server
            await asyncio.sleep(0.1)
        
        # Final statistics
        total_time = time.time() - start_time
        rate = len(memory_ids) / total_time if total_time > 0 else 0
        
        print(f"Completed: {len(memory_ids)}/{total_memories} memories created in {total_time:.2f} seconds")
        print(f"Rate: {rate:.2f} memories/second")
        
        # Verify we can retrieve memories from the batch
        if memory_ids:
            # Try to retrieve a random memory by ID
            random_id = random.choice(memory_ids)
            retrieve_resp = await client.retrieve_memories(
                query=f"Memory #{random.randint(1, total_memories)}",
                top_k=5
            )
            
            assert retrieve_resp.get("success") is True, "Failed to retrieve after bulk ingestion"
            print(f"Successfully retrieved {len(retrieve_resp.get('memories', []))} memories from the bulk ingestion")

@pytest.mark.asyncio
async def test_concurrent_retrievals():
    """Test the system with many concurrent retrieval requests."""
    async with SynthiansClient() as client:
        # First, create some memories to retrieve
        timestamp = int(time.time())
        keyword = f"concurrent{timestamp}"
        
        # Create 10 memories with the same keyword
        create_tasks = []
        for i in range(10):
            content = f"Memory {i+1} for concurrent retrieval test with keyword {keyword}"
            task = client.process_memory(content=content)
            create_tasks.append(task)
        
        create_results = await asyncio.gather(*create_tasks)
        created_ids = [r.get("memory_id") for r in create_results if r.get("success")]
        
        assert len(created_ids) > 0, "Failed to create test memories for concurrent retrievals"
        print(f"Created {len(created_ids)} test memories for concurrent retrievals")
        
        # Wait briefly for processing
        await asyncio.sleep(1)
        
        # Now perform many concurrent retrievals
        concurrency = 20  # Number of concurrent requests
        start_time = time.time()
        
        retrieval_tasks = []
        for i in range(concurrency):
            task = client.retrieve_memories(
                query=f"{keyword} memory {random.randint(1, 10)}",
                top_k=5
            )
            retrieval_tasks.append(task)
        
        # Execute concurrently
        retrieval_results = await asyncio.gather(*retrieval_tasks)
        
        # Calculate statistics
        successful = sum(1 for r in retrieval_results if r.get("success"))
        total_time = time.time() - start_time
        rate = concurrency / total_time if total_time > 0 else 0
        
        print(f"Completed {successful}/{concurrency} concurrent retrievals in {total_time:.2f} seconds")
        print(f"Rate: {rate:.2f} retrievals/second")
        
        # Check that all retrievals worked
        assert successful == concurrency, f"Only {successful}/{concurrency} concurrent retrievals succeeded"

@pytest.mark.asyncio
async def test_batch_save_and_reload():
    """Test saving and reloading the memory store during batch operations."""
    # Note: This test assumes there's an endpoint to trigger a save/reload cycle
    # If not available, this can be skipped
    
    async with SynthiansClient() as client:
        try:
            # Create a batch of memories
            timestamp = int(time.time())
            memory_ids = []
            
            # Create 20 test memories
            for i in range(20):
                content = f"Memory {i+1} for save/reload test at {timestamp}"
                response = await client.process_memory(content=content)
                if response.get("success"):
                    memory_ids.append(response.get("memory_id"))
            
            # Call save endpoint (if available)
            # This is hypothetical - might need to be implemented
            save_response = await client.session.post(f"{client.base_url}/save_memory_store")
            save_result = await save_response.json()
            
            print(f"Save operation result: {json.dumps(save_result, indent=2)}")
            
            # Call reload endpoint (if available)
            reload_response = await client.session.post(f"{client.base_url}/reload_memory_store")
            reload_result = await reload_response.json()
            
            print(f"Reload operation result: {json.dumps(reload_result, indent=2)}")
            
            # Verify memories are still retrievable after reload
            retrieved_count = 0
            for memory_id in memory_ids[:5]:  # Check first 5 memories
                query = f"save/reload test at {timestamp}"
                result = await client.retrieve_memories(query=query, top_k=20)
                
                if result.get("success"):
                    result_ids = [m.get("id") for m in result.get("memories", [])]
                    if memory_id in result_ids:
                        retrieved_count += 1
            
            # Check that we could retrieve our memories after reload
            assert retrieved_count > 0, "Failed to retrieve memories after save/reload cycle"
            print(f"Successfully retrieved {retrieved_count}/5 test memories after save/reload cycle")
            
        except Exception as e:
            # The save/reload endpoints might not exist yet
            print(f"Save/reload test failed: {str(e)}")
            print("Save/reload endpoints may not be implemented yet.")

@pytest.mark.asyncio
async def test_memory_decay_pruning():
    """Test memory decay and pruning of old memories."""
    # This test is designed to verify that old memories can be pruned
    # It may need to be adapted based on actual implementation
    
    async with SynthiansClient() as client:
        try:
            # Create memories with backdated timestamps
            timestamp = int(time.time())
            
            # Current memory
            current_response = await client.process_memory(
                content=f"Current memory at {timestamp}",
                metadata={"timestamp": time.time()}
            )
            current_id = current_response.get("memory_id")
            
            # 1-day old memory
            day_old_time = time.time() - (60 * 60 * 24)  # 1 day ago
            day_old_response = await client.process_memory(
                content=f"One day old memory at {timestamp}",
                metadata={"timestamp": day_old_time}
            )
            day_old_id = day_old_response.get("memory_id")
            
            # 1-week old memory
            week_old_time = time.time() - (60 * 60 * 24 * 7)  # 1 week ago
            week_old_response = await client.process_memory(
                content=f"One week old memory at {timestamp}",
                metadata={"timestamp": week_old_time}
            )
            week_old_id = week_old_response.get("memory_id")
            
            # 1-month old memory
            month_old_time = time.time() - (60 * 60 * 24 * 30)  # ~1 month ago
            month_old_response = await client.process_memory(
                content=f"One month old memory at {timestamp}",
                metadata={"timestamp": month_old_time}
            )
            month_old_id = month_old_response.get("memory_id")
            
            # Verify all were created successfully
            assert all(r.get("success") for r in [
                current_response, day_old_response, week_old_response, month_old_response
            ]), "Failed to create test memories with different ages"
            
            print("Successfully created test memories with different timestamps")
            
            # Now trigger a pruning operation (if available)
            # This is hypothetical - might need to be implemented
            prune_response = await client.session.post(
                f"{client.base_url}/prune_old_memories",
                json={"max_age_days": 14}  # Prune memories older than 2 weeks
            )
            prune_result = await prune_response.json()
            
            print(f"Pruning operation result: {json.dumps(prune_result, indent=2)}")
            
            # Check which memories are still retrievable
            retrievable = []
            
            for memory_id, age in [
                (current_id, "current"),
                (day_old_id, "1-day"),
                (week_old_id, "1-week"),
                (month_old_id, "1-month")
            ]:
                query = f"memory at {timestamp}"
                result = await client.retrieve_memories(query=query, top_k=10)
                
                if result.get("success"):
                    result_ids = [m.get("id") for m in result.get("memories", [])]
                    if memory_id in result_ids:
                        retrievable.append(age)
            
            print(f"Still retrievable after pruning: {retrievable}")
            
            # Current and 1-day old should still be retrievable
            # 1-month old should be pruned
            # 1-week old depends on implementation details
            assert "current" in retrievable, "Current memory was incorrectly pruned"
            assert "1-day" in retrievable, "1-day old memory was incorrectly pruned"
            
            if "1-month" in retrievable:
                print("Warning: 1-month old memory was not pruned, pruning may not be implemented yet")
                
        except Exception as e:
            # The pruning endpoint might not exist yet
            print(f"Memory pruning test failed: {str(e)}")
            print("Memory pruning may not be implemented yet.")

```

# tests\test_tool_integration.py

```py
import pytest
import asyncio
import json
import time
from datetime import datetime
from synthians_memory_core.api.client.client import SynthiansClient

# Add the missing tool methods to SynthiansClient if needed
async def process_memory_tool(self, content: str, metadata: dict = None):
    """Process memory as a tool call (simulated)."""
    payload = {
        "content": content,
        "metadata": metadata or {},
        "tool_call": True  # Identify this as coming from a tool call
    }
    async with self.session.post(
        f"{self.base_url}/process_memory", json=payload
    ) as response:
        return await response.json()

async def retrieve_memories_tool(self, query: str, top_k: int = 5, user_emotion: dict = None):
    """Retrieve memories as a tool call (simulated)."""
    payload = {
        "query": query,
        "top_k": top_k,
        "user_emotion": user_emotion,
        "tool_call": True  # Identify this as coming from a tool call
    }
    async with self.session.post(
        f"{self.base_url}/retrieve_memories", json=payload
    ) as response:
        return await response.json()

async def detect_contradictions_tool(self, query: str, threshold: float = 0.75):
    """Detect contradictions as a tool call (simulated)."""
    payload = {
        "query": query,
        "threshold": threshold,
        "tool_call": True  # Identify this as coming from a tool call
    }
    async with self.session.post(
        f"{self.base_url}/detect_contradictions", json=payload
    ) as response:
        return await response.json()

# Add methods to SynthiansClient class if not present
if not hasattr(SynthiansClient, "process_memory_tool"):
    SynthiansClient.process_memory_tool = process_memory_tool

if not hasattr(SynthiansClient, "retrieve_memories_tool"):
    SynthiansClient.retrieve_memories_tool = retrieve_memories_tool

if not hasattr(SynthiansClient, "detect_contradictions_tool"):
    SynthiansClient.detect_contradictions_tool = detect_contradictions_tool

@pytest.mark.asyncio
async def test_tool_call_process_memory_tool():
    """Test processing memory through a simulated tool call."""
    async with SynthiansClient() as client:
        try:
            # Use a unique timestamp to ensure we can find this memory
            timestamp = int(time.time())
            content = f"Memory created through tool call at {timestamp}"
            metadata = {
                "source": "tool_test",
                "importance": 0.9,
                "tool_metadata": {
                    "tool_name": "process_memory_tool",
                    "llm_type": "test_model"
                }
            }
            
            # Process the memory through the tool call
            result = await client.process_memory_tool(content=content, metadata=metadata)
            
            # Verify successful processing
            assert result.get("success") is True, f"Tool call memory processing failed: {result.get('error')}"
            assert "memory_id" in result, "No memory ID returned from tool call"
            
            # Verify the memory was stored with correct metadata
            returned_metadata = result.get("metadata", {})
            assert returned_metadata.get("source") == "tool_test", "Tool metadata not preserved"
            assert "tool_metadata" in returned_metadata, "Tool-specific metadata not preserved"
            
            print(f"Tool memory processing result: {json.dumps(result, indent=2)}")
            
            # Wait briefly
            await asyncio.sleep(0.5)
            
            # Try to retrieve the memory to confirm it was stored
            memory_id = result.get("memory_id")
            retrieval = await client.retrieve_memories(query=f"tool call at {timestamp}", top_k=3)
            
            # Verify the memory can be retrieved
            memories = retrieval.get("memories", [])
            memory_ids = [m.get("id") for m in memories]
            
            assert memory_id in memory_ids, f"Memory created by tool call not retrievable. Expected {memory_id}, got {memory_ids}"
            
        except Exception as e:
            # The API might not support the tool_call parameter yet
            print(f"Tool call memory processing test failed: {str(e)}")
            print("Tool-specific endpoint might not be implemented yet.")

@pytest.mark.asyncio
async def test_tool_call_retrieve_memories_tool():
    """Test retrieving memories through a simulated tool call."""
    async with SynthiansClient() as client:
        try:
            # First, create a memory we can retrieve
            timestamp = int(time.time())
            content = f"Retrievable memory for tool test at {timestamp}"
            
            memory_resp = await client.process_memory(content=content)
            memory_id = memory_resp.get("memory_id")
            
            # Wait briefly
            await asyncio.sleep(0.5)
            
            # Now retrieve it using the tool call endpoint
            retrieval = await client.retrieve_memories_tool(
                query=f"tool test at {timestamp}",
                top_k=3
            )
            
            # Verify successful retrieval
            assert retrieval.get("success") is True, f"Tool call memory retrieval failed: {retrieval.get('error')}"
            assert "memories" in retrieval, "No memories returned from tool call"
            
            # Check if our memory was found
            memories = retrieval.get("memories", [])
            memory_ids = [m.get("id") for m in memories]
            
            print(f"Retrieved memory IDs through tool: {memory_ids}")
            print(f"Expected memory ID: {memory_id}")
            assert memory_id in memory_ids, "Memory not found via tool retrieval"
            
            # Verify tool-specific formatting (if implemented)
            if "tool_format" in retrieval:
                assert retrieval["tool_format"] == "formatted_for_llm", "Tool-specific formatting not applied"
            
        except Exception as e:
            # The API might not support the tool_call parameter yet
            print(f"Tool call memory retrieval test failed: {str(e)}")
            print("Tool-specific endpoint might not be implemented yet.")

@pytest.mark.asyncio
async def test_tool_call_detect_contradictions_tool():
    """Test contradiction detection through a simulated tool call."""
    async with SynthiansClient() as client:
        try:
            # Create contradicting memories
            timestamp = int(time.time())
            
            # First statement
            await client.process_memory(
                content=f"The meeting is scheduled for Tuesday at 2pm. {timestamp}",
                metadata={"contradiction_test": True}
            )
            
            # Contradicting statement
            await client.process_memory(
                content=f"The meeting is scheduled for Wednesday at 3pm. {timestamp}",
                metadata={"contradiction_test": True}
            )
            
            # Wait briefly
            await asyncio.sleep(1)
            
            # Check for contradictions using the tool call
            result = await client.detect_contradictions_tool(
                query=f"meeting schedule {timestamp}",
                threshold=0.7
            )
            
            # Verify successful detection
            assert result.get("success") is True, f"Tool call contradiction detection failed: {result.get('error')}"
            
            # If contradictions were found, they should be in the result
            if "contradictions" in result:
                contradictions = result.get("contradictions", [])
                print(f"Detected {len(contradictions)} contradictions through tool call")
                print(f"Contradiction results: {json.dumps(contradictions, indent=2)}")
                
                # There should be at least one contradiction
                if len(contradictions) > 0:
                    assert "memory_pairs" in contradictions[0], "Contradiction missing memory pairs"
                    assert "contradiction_type" in contradictions[0], "Contradiction missing type"
            
        except Exception as e:
            # The API might not support the contradiction detection yet
            print(f"Tool call contradiction detection test failed: {str(e)}")
            print("Contradiction detection feature might not be implemented yet.")

@pytest.mark.asyncio
async def test_tool_call_feedback_tool():
    """Test providing feedback through a simulated tool call."""
    async with SynthiansClient() as client:
        try:
            # First, create a memory we can provide feedback on
            timestamp = int(time.time())
            content = f"Memory for feedback test at {timestamp}"
            
            memory_resp = await client.process_memory(content=content)
            memory_id = memory_resp.get("memory_id")
            
            # Now provide feedback through the tool call
            # Add a method for this if not available
            if not hasattr(client, "provide_feedback_tool"):
                async def provide_feedback_tool(self, memory_id, similarity_score, was_relevant):
                    payload = {
                        "memory_id": memory_id,
                        "similarity_score": similarity_score,
                        "was_relevant": was_relevant,
                        "tool_call": True  # Identify this as coming from a tool call
                    }
                    async with self.session.post(
                        f"{self.base_url}/provide_feedback", json=payload
                    ) as response:
                        return await response.json()
                
                client.provide_feedback_tool = provide_feedback_tool.__get__(client, SynthiansClient)
            
            # Use the feedback tool
            feedback_resp = await client.provide_feedback_tool(
                memory_id=memory_id,
                similarity_score=0.92,
                was_relevant=True
            )
            
            # Verify successful feedback
            assert feedback_resp.get("success") is True, f"Tool call feedback failed: {feedback_resp.get('error')}"
            assert "new_threshold" in feedback_resp, "Threshold adjustment information missing"
            
            print(f"Feedback through tool call: {json.dumps(feedback_resp, indent=2)}")
            
        except Exception as e:
            # The API might not support the tool call parameter yet
            print(f"Tool call feedback test failed: {str(e)}")
            print("Tool-specific endpoint might not be implemented yet.")

```

# tests\test_transcription_voice_flow.py

```py
import pytest
import asyncio
import json
import time
from datetime import datetime
from synthians_memory_core.api.client.client import SynthiansClient

# Add process_transcription method to SynthiansClient if not already present
async def process_transcription(self, text: str, audio_metadata: dict = None, embedding=None):
    """Process transcription data and store it in the memory system."""
    payload = {
        "text": text,
        "audio_metadata": audio_metadata or {},
        "embedding": embedding
    }
    async with self.session.post(
        f"{self.base_url}/process_transcription", json=payload
    ) as response:
        return await response.json()

# Add the method to the client class if not present
if not hasattr(SynthiansClient, "process_transcription"):
    SynthiansClient.process_transcription = process_transcription

@pytest.mark.asyncio
async def test_transcription_feature_extraction():
    """Test that transcription processing extracts relevant features."""
    async with SynthiansClient() as client:
        # Create a transcription with rich metadata
        text = "This is a test transcription with some pauses... and rhythm changes."
        audio_metadata = {
            "duration_sec": 5.2,
            "avg_volume": 0.75,
            "speaking_rate": 2.1,  # Words per second
            "pauses": [
                {"start": 1.2, "duration": 0.5},
                {"start": 3.5, "duration": 0.8}
            ]
        }
        
        # Process the transcription
        result = await client.process_transcription(
            text=text,
            audio_metadata=audio_metadata
        )
        
        # Verify successful processing
        assert result.get("success") is True, f"Transcription processing failed: {result.get('error')}"
        assert "memory_id" in result, "No memory ID returned for transcription"
        
        # Check metadata enrichment
        metadata = result.get("metadata", {})
        
        # Basic metadata verification
        assert "timestamp" in metadata, "No timestamp in metadata"
        assert "speaking_rate" in metadata, "Speaking rate not captured in metadata"
        assert "duration_sec" in metadata, "Duration not captured in metadata"
        
        # Advanced feature extraction verification (if implemented)
        if "pause_count" in metadata:
            assert metadata["pause_count"] >= 2, "Expected at least 2 pauses to be detected"
        
        if "speech_features" in metadata:
            assert isinstance(metadata["speech_features"], dict), "Speech features not properly structured"
        
        print(f"Transcription metadata: {json.dumps(metadata, indent=2)}")

@pytest.mark.asyncio
async def test_interrupt_metadata_enrichment():
    """Test that interruption metadata is properly stored and processed."""
    async with SynthiansClient() as client:
        # Create a transcription with interruption data
        text = "I was talking about- wait, let me restart. This is what I meant to say."
        audio_metadata = {
            "duration_sec": 7.5,
            "was_interrupted": True,
            "interruptions": [
                {"timestamp": 2.1, "duration": 0.3, "type": "self"}
            ],
            "user_interruptions": 1
        }
        
        # Process the transcription
        result = await client.process_transcription(
            text=text,
            audio_metadata=audio_metadata
        )
        
        # Verify successful processing
        assert result.get("success") is True, "Transcription processing failed"
        
        # Check interruption metadata
        metadata = result.get("metadata", {})
        assert "was_interrupted" in metadata, "Interruption flag not in metadata"
        assert metadata.get("was_interrupted") is True, "Interruption flag not preserved"
        
        if "interruption_count" in metadata:
            assert metadata["interruption_count"] >= 1, "Expected at least 1 interruption to be counted"
        
        if "user_interruptions" in metadata:
            assert metadata["user_interruptions"] >= 1, "User interruptions not preserved in metadata"
        
        print(f"Interruption metadata: {json.dumps(metadata, indent=2)}")

@pytest.mark.asyncio
async def test_session_level_memory():
    """Test that multiple utterances within a session are properly linked."""
    async with SynthiansClient() as client:
        # Generate a unique session ID
        session_id = f"test-session-{int(time.time())}"
        
        # Create first utterance in session
        text1 = "This is the first part of a multi-utterance conversation."
        metadata1 = {
            "session_id": session_id,
            "utterance_index": 1,
            "timestamp": time.time()
        }
        
        result1 = await client.process_memory(
            content=text1,
            metadata=metadata1
        )
        
        assert result1.get("success") is True, "First utterance processing failed"
        memory_id1 = result1.get("memory_id")
        
        # Wait briefly
        await asyncio.sleep(0.5)
        
        # Create second utterance in same session
        text2 = "This is the second part, continuing from what I said before."
        metadata2 = {
            "session_id": session_id,
            "utterance_index": 2,
            "timestamp": time.time(),
            "previous_memory_id": memory_id1  # Link to previous utterance
        }
        
        result2 = await client.process_memory(
            content=text2,
            metadata=metadata2
        )
        
        assert result2.get("success") is True, "Second utterance processing failed"
        memory_id2 = result2.get("memory_id")
        
        # Wait briefly
        await asyncio.sleep(0.5)
        
        # Create third utterance in same session
        text3 = "This is the third and final part of my conversation."
        metadata3 = {
            "session_id": session_id,
            "utterance_index": 3,
            "timestamp": time.time(),
            "previous_memory_id": memory_id2  # Link to previous utterance
        }
        
        result3 = await client.process_memory(
            content=text3,
            metadata=metadata3
        )
        
        assert result3.get("success") is True, "Third utterance processing failed"
        
        # Retrieve memories from this session
        # This assumes the API has a way to filter by session_id
        # If not, we can query by the unique session ID in the content
        retrieval_resp = await client.retrieve_memories(
            query=f"session:{session_id}",
            top_k=10
        )
        
        # Check if all three memories were retrieved
        memories = retrieval_resp.get("memories", [])
        memory_ids = [m.get("id") for m in memories]
        
        print(f"Retrieved session memories: {json.dumps(memory_ids, indent=2)}")
        
        # Check for session links in metadata (if implemented)
        for memory in memories:
            if "metadata" in memory and "session_id" in memory["metadata"]:
                assert memory["metadata"]["session_id"] == session_id, "Session ID not preserved"

@pytest.mark.asyncio
async def test_voice_state_tracking():
    """Test that voice state transitions are properly tracked in memory metadata."""
    async with SynthiansClient() as client:
        # Create a transcription with voice state metadata
        text = "This is a test of voice state tracking."
        audio_metadata = {
            "voice_state": "SPEAKING",
            "state_duration": 3.2,
            "previous_state": "LISTENING",
            "state_transition_count": 5,
            "last_state_transition_time": time.time() - 3.2
        }
        
        try:
            # Process the transcription (if endpoint exists)
            result = await client.process_transcription(
                text=text,
                audio_metadata=audio_metadata
            )
            
            # Verify successful processing
            assert result.get("success") is True, "Voice state tracking test failed"
            
            # Check if voice state metadata was preserved
            metadata = result.get("metadata", {})
            if "voice_state" in metadata:
                assert metadata["voice_state"] == "SPEAKING", "Voice state not preserved"
            
            if "state_transition_count" in metadata:
                assert metadata["state_transition_count"] == 5, "State transition count not preserved"
            
            print(f"Voice state metadata: {json.dumps(metadata, indent=2)}")
            
        except Exception as e:
            # This test may fail if the API doesn't support voice state tracking yet
            print(f"Voice state tracking test failed: {str(e)}")
            print("This feature may not be implemented yet.")

```

# tests\test_variant_mac.py

```py
# tests/test_variant_mac.py

import pytest
import pytest_asyncio
import asyncio
import json
import os
import time
from typing import Dict, List, Any

# Import our variant testing fixtures
from variant_conftest import api_clients, create_test_memories

# Get current variant for logging and assertions
CURRENT_VARIANT = os.environ.get('TITANS_VARIANT', 'UNKNOWN')
if CURRENT_VARIANT != 'MAC':
    pytest.skip(f"Skipping MAC tests since current variant is {CURRENT_VARIANT}", allow_module_level=True)

# Test functions specifically for MAC variant
@pytest.mark.asyncio
async def test_mac_variant_memory_processing(api_clients):
    """Test MAC variant's basic memory processing capabilities."""
    session, mc_client = api_clients
    
    # 1. Create test memories
    test_content = f"This is a MAC variant test memory created at {time.time()}"
    memory_ids = await create_test_memories(mc_client, count=3, 
                                          prefix=f"MAC-Variant-Test")
    
    # 2. Wait briefly for asynchronous processing
    await asyncio.sleep(1)  # Allow processing to complete
    
    # 3. Call CCE to process a related memory through the MAC variant
    async with session.post(
        "http://localhost:8002/process_memory",
        json={
            "content": f"This is a follow-up to the MAC-Variant-Test memories",
            "embedding": [float(i) / 100 for i in range(384)],  # Simple test embedding
            "metadata": {
                "source": "variant_test",
                "variant": "MAC"
            }
        }
    ) as response:
        assert response.status == 200, f"Failed to process memory via CCE: {await response.text()}"
        result = await response.json()
        assert "memory_id" in result, "No memory_id in response"
        cce_memory_id = result["memory_id"]
    
    # 4. Wait for CCE to process the memory (MAC model needs time to process)
    await asyncio.sleep(3)  # Allow sufficient time for MAC processing
    
    # 5. Verify the CCE-processed memory exists in Memory Core
    retrieved_memory = await mc_client.get_memory(cce_memory_id)
    assert retrieved_memory is not None, f"Could not retrieve memory {cce_memory_id}"
    assert "metadata" in retrieved_memory, "No metadata in retrieved memory"
    
    # MAC specific: Memory should have been processed by MAC variant
    # The processing_info is deeply nested, so we need to handle it carefully
    metadata = retrieved_memory.get("metadata", {})
    processing_info = metadata.get("processing_info", {})
    
    # Check for MAC-specific indicators in the memory
    # Note: The exact structure depends on your implementation
    assert processing_info.get("variant") == "MAC" or \
           processing_info.get("titans_variant") == "MAC" or \
           metadata.get("titans_variant") == "MAC", \
           f"Memory not processed by MAC variant: {metadata}"
    
    # Clean up test memories
    for memory_id in memory_ids + [cce_memory_id]:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mac_variant_retrieval(api_clients):
    """Test MAC variant's retrieval behavior."""
    session, mc_client = api_clients
    
    # 1. Create a series of memories with known semantic relationships
    memory_contents = [
        "Artificial intelligence models require large datasets for training",
        "Neural networks have many interconnected layers of neurons", 
        "Deep learning systems process information similarly to the human brain",
        "Machine learning algorithms improve with more training data",
        "Gradient descent is used to optimize neural network weights"
    ]
    
    memory_ids = []
    for i, content in enumerate(memory_contents):
        memory_entry = {
            "content": content,
            "embedding": [float(j) / 100 for j in range(384)],  # Simple test embedding
            "metadata": {
                "source": "mac_variant_test",
                "test_id": i,
                "variant": "MAC"
            }
        }
        
        result = await mc_client.process_memory(memory_entry)
        memory_ids.append(result["memory_id"])
    
    # 2. Wait for processing
    await asyncio.sleep(1)
    
    # 3. Query through CCE with MAC variant
    query = "How do neural networks process information?"
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": query,
            "max_memories": 3
        }
    ) as response:
        assert response.status == 200, f"Failed to retrieve memories: {await response.text()}"
        result = await response.json()
        
        # 4. Verify MAC-specific retrieval behavior
        # MAC should have specific associative characteristics
        assert "memories" in result, "No memories in response"
        assert len(result["memories"]) > 0, "No memories retrieved"
        
        # Look for memories that mention neural networks
        found_neural = False
        for memory in result["memories"]:
            if "neural" in memory["content"].lower():
                found_neural = True
                break
        
        assert found_neural, "MAC variant did not retrieve relevant neural network memories"
    
    # 5. Verify Memory Core can directly retrieve our test memories
    retrieved = await mc_client.retrieve_memories(
        query=query,
        max_memories=5
    )
    assert len(retrieved["memories"]) > 0, "No memories retrieved directly from Memory Core"
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mac_variant_state(api_clients):
    """Test MAC variant state management and internal memory structure."""
    session, mc_client = api_clients
    
    # 1. Check Neural Memory server state to confirm MAC model is active
    async with session.get("http://localhost:8001/status") as response:
        assert response.status == 200, "Failed to get Neural Memory status"
        nm_status = await response.json()
        
        # The status response format depends on your implementation
        # Look for MAC-specific indicators
        assert "model_info" in nm_status, "No model info in Neural Memory status"
        model_info = nm_status["model_info"]
        
        # Verify it's running MAC variant
        if "variant" in model_info:
            assert model_info["variant"] == "MAC", f"Wrong variant: {model_info['variant']}"
        elif "architecture" in model_info:
            assert "MAC" in model_info["architecture"], f"MAC not in architecture: {model_info['architecture']}"
    
    # 2. Check context-cascade-engine status
    async with session.get("http://localhost:8002/status") as response:
        assert response.status == 200, "Failed to get CCE status"
        cce_status = await response.json()
        
        # Verify CCE is also configured for MAC
        # The exact path depends on your CCE status response format
        titan_config = cce_status.get("config", {}).get("titan", {})
        if titan_config:
            assert titan_config.get("variant") == "MAC" or \
                   titan_config.get("titans_variant") == "MAC", \
                   f"CCE not configured for MAC: {titan_config}"
        
        # Alternative check locations depending on implementation
        titans_variant = cce_status.get("titans_variant") or \
                        cce_status.get("config", {}).get("titans_variant")
        if titans_variant is not None:
            assert titans_variant == "MAC", f"Wrong CCE variant: {titans_variant}"

@pytest.mark.asyncio
async def test_mac_memory_characteristics(api_clients):
    """Test MAC-specific memory characteristics and behaviors."""
    session, mc_client = api_clients
    
    # MAC variant is expected to have specific characteristics:
    # 1. It operates more like traditional associative memory
    # 2. Its QuickRecall values may differ from other variants
    # 3. Its retrievals should show specific patterns
    
    # Create a memory sequence to test associations
    test_sequence = [
        "The capital of France is Paris.",
        "Paris is known for the Eiffel Tower.",
        "The Eiffel Tower was built in 1889.",
        "The year 1889 was in the 19th century."
    ]
    
    # Process these in sequence through CCE
    memory_ids = []
    for content in test_sequence:
        async with session.post(
            "http://localhost:8002/process_memory",
            json={
                "content": content,
                "metadata": {"test": "mac_characteristics"}
            }
        ) as response:
            result = await response.json()
            memory_ids.append(result["memory_id"])
        # Brief pause between memories to ensure sequential processing
        await asyncio.sleep(0.5)
    
    # Allow processing to complete
    await asyncio.sleep(2)
    
    # Test associative retrieval with CCE
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": "What is Paris known for?",
            "max_memories": 2
        }
    ) as response:
        result = await response.json()
        memories = result.get("memories", [])
        
        # MAC should find related memories about Paris
        paris_memory = next((m for m in memories if "paris" in m["content"].lower()), None)
        assert paris_memory is not None, "MAC variant didn't retrieve Paris-related memory"
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

```

# tests\test_variant_mag.py

```py
# tests/test_variant_mag.py

import pytest
import pytest_asyncio
import asyncio
import json
import os
import time
from typing import Dict, List, Any

# Import our variant testing fixtures
from variant_conftest import api_clients, create_test_memories

# Get current variant for logging and assertions
CURRENT_VARIANT = os.environ.get('TITANS_VARIANT', 'UNKNOWN')
if CURRENT_VARIANT != 'MAG':
    pytest.skip(f"Skipping MAG tests since current variant is {CURRENT_VARIANT}", allow_module_level=True)

# Test functions specifically for MAG variant
@pytest.mark.asyncio
async def test_mag_variant_memory_processing(api_clients):
    """Test MAG variant's basic memory processing capabilities."""
    session, mc_client = api_clients
    
    # 1. Create test memories
    test_content = f"This is a MAG variant test memory created at {time.time()}"
    memory_ids = await create_test_memories(mc_client, count=3, 
                                          prefix=f"MAG-Variant-Test")
    
    # 2. Wait briefly for asynchronous processing
    await asyncio.sleep(1)  # Allow processing to complete
    
    # 3. Call CCE to process a related memory through the MAG variant
    async with session.post(
        "http://localhost:8002/process_memory",
        json={
            "content": f"This is a follow-up to the MAG-Variant-Test memories",
            "embedding": [float(i) / 100 for i in range(384)],  # Simple test embedding
            "metadata": {
                "source": "variant_test",
                "variant": "MAG"
            }
        }
    ) as response:
        assert response.status == 200, f"Failed to process memory via CCE: {await response.text()}"
        result = await response.json()
        assert "memory_id" in result, "No memory_id in response"
        cce_memory_id = result["memory_id"]
    
    # 4. Wait for CCE to process the memory (MAG model might need more time for gating)
    await asyncio.sleep(3)  # Allow sufficient time for MAG processing
    
    # 5. Verify the CCE-processed memory exists in Memory Core
    retrieved_memory = await mc_client.get_memory(cce_memory_id)
    assert retrieved_memory is not None, f"Could not retrieve memory {cce_memory_id}"
    assert "metadata" in retrieved_memory, "No metadata in retrieved memory"
    
    # MAG specific: Memory should have been processed by MAG variant
    # The processing_info is deeply nested, so we need to handle it carefully
    metadata = retrieved_memory.get("metadata", {})
    processing_info = metadata.get("processing_info", {})
    
    # Check for MAG-specific indicators in the memory
    # Note: The exact structure depends on your implementation
    assert processing_info.get("variant") == "MAG" or \
           processing_info.get("titans_variant") == "MAG" or \
           metadata.get("titans_variant") == "MAG", \
           f"Memory not processed by MAG variant: {metadata}"
    
    # Clean up test memories
    for memory_id in memory_ids + [cce_memory_id]:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mag_variant_retrieval(api_clients):
    """Test MAG variant's retrieval behavior with gating characteristics."""
    session, mc_client = api_clients
    
    # 1. Create memories for testing MAG's gating behavior
    # Include some high emotional memories and some neutral ones
    memory_contents = [
        {"content": "Today was an amazing day with perfect weather!", "emotion": "joy", "intensity": 0.9},
        {"content": "I learned about neural network architecture today", "emotion": "neutral", "intensity": 0.2},
        {"content": "The accident on the highway was terrible", "emotion": "sadness", "intensity": 0.8},
        {"content": "The conference presentation was informative", "emotion": "neutral", "intensity": 0.3},
        {"content": "I'm extremely frustrated with the software bugs", "emotion": "anger", "intensity": 0.75}
    ]
    
    memory_ids = []
    for i, memory_data in enumerate(memory_contents):
        memory_entry = {
            "content": memory_data["content"],
            "embedding": [float(j) / 100 for j in range(384)],  # Simple test embedding
            "metadata": {
                "source": "mag_variant_test",
                "test_id": i,
                "variant": "MAG",
                "dominant_emotion": memory_data["emotion"],
                "emotion_intensity": memory_data["intensity"]
            }
        }
        
        result = await mc_client.process_memory(memory_entry)
        memory_ids.append(result["memory_id"])
    
    # 2. Wait for processing
    await asyncio.sleep(1)
    
    # 3. Query through CCE with different emotional states
    # First with emotional query that should activate gating
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": "Tell me about emotional experiences",
            "max_memories": 3,
            "query_metadata": {
                "current_emotion": "joy",  # Current emotional state
                "emotion_intensity": 0.7    # High intensity
            }
        }
    ) as response:
        assert response.status == 200, f"Failed to retrieve memories: {await response.text()}"
        result = await response.json()
        
        # 4. Verify MAG-specific retrieval behavior (emotional gating)
        assert "memories" in result, "No memories in response"
        assert len(result["memories"]) > 0, "No memories retrieved"
        
        # MAG should prioritize emotionally congruent memories (joy in this case)
        # At least one high-joy memory should be present in the results
        found_joy = False
        for memory in result["memories"]:
            memory_emotion = memory.get("metadata", {}).get("dominant_emotion")
            if memory_emotion == "joy":
                found_joy = True
                break
        
        assert found_joy, "MAG variant did not retrieve emotionally congruent memories"
    
    # 5. Now query with neutral state - MAG should show different behavior
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": "Tell me about informative content",
            "max_memories": 3,
            "query_metadata": {
                "current_emotion": "neutral",  # Neutral emotional state
                "emotion_intensity": 0.2       # Low intensity
            }
        }
    ) as response:
        assert response.status == 200
        result = await response.json()
        
        # MAG should prioritize neutral memories with low emotional content
        neutral_memories = []
        for memory in result.get("memories", []):
            memory_emotion = memory.get("metadata", {}).get("dominant_emotion")
            if memory_emotion == "neutral":
                neutral_memories.append(memory)
                
        assert len(neutral_memories) > 0, "MAG didn't retrieve neutral memories with neutral query"
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mag_variant_state(api_clients):
    """Test MAG variant state management and internal gating structure."""
    session, mc_client = api_clients
    
    # 1. Check Neural Memory server state to confirm MAG model is active
    async with session.get("http://localhost:8001/status") as response:
        assert response.status == 200, "Failed to get Neural Memory status"
        nm_status = await response.json()
        
        # Look for MAG-specific indicators
        assert "model_info" in nm_status, "No model info in Neural Memory status"
        model_info = nm_status["model_info"]
        
        # Verify it's running MAG variant
        if "variant" in model_info:
            assert model_info["variant"] == "MAG", f"Wrong variant: {model_info['variant']}"
        elif "architecture" in model_info:
            assert "MAG" in model_info["architecture"], f"MAG not in architecture: {model_info['architecture']}"
    
    # 2. Check context-cascade-engine status
    async with session.get("http://localhost:8002/status") as response:
        assert response.status == 200, "Failed to get CCE status"
        cce_status = await response.json()
        
        # Verify CCE is also configured for MAG
        titan_config = cce_status.get("config", {}).get("titan", {})
        if titan_config:
            assert titan_config.get("variant") == "MAG" or \
                   titan_config.get("titans_variant") == "MAG", \
                   f"CCE not configured for MAG: {titan_config}"
        
        # Alternative check locations depending on implementation
        titans_variant = cce_status.get("titans_variant") or \
                        cce_status.get("config", {}).get("titans_variant")
        if titans_variant is not None:
            assert titans_variant == "MAG", f"Wrong CCE variant: {titans_variant}"

@pytest.mark.asyncio
async def test_mag_emotion_gating_influence(api_clients):
    """Test the impact of MAG's emotion gating mechanism on memory retrieval."""
    session, mc_client = api_clients
    
    # Create emotion-tagged memories
    emotions = ["joy", "anger", "sadness", "fear", "neutral"]
    memory_ids = []
    
    # Create memories with different emotions
    for emotion in emotions:
        for i in range(2):  # 2 memories per emotion
            content = f"This is a {emotion} memory {i} for testing MAG's emotion gating"
            memory_entry = {
                "content": content,
                "embedding": [float(j) / 100 for j in range(384)],  # Simple test embedding
                "metadata": {
                    "source": "mag_emotion_test",
                    "dominant_emotion": emotion,
                    "emotion_intensity": 0.8 if emotion != "neutral" else 0.2
                }
            }
            
            result = await mc_client.process_memory(memory_entry)
            memory_ids.append(result["memory_id"])
    
    # Allow time for processing
    await asyncio.sleep(2)
    
    # Test the gating effect with different emotional contexts
    emotion_queries = [
        {"emotion": "joy", "query": "Tell me about happy memories"},
        {"emotion": "anger", "query": "What makes people upset?"},
        {"emotion": "neutral", "query": "Give me factual information"}
    ]
    
    for eq in emotion_queries:
        # Query with specific emotional context
        async with session.post(
            "http://localhost:8002/retrieve_memories",
            json={
                "query": eq["query"],
                "max_memories": 4,
                "query_metadata": {
                    "current_emotion": eq["emotion"],
                    "emotion_intensity": 0.7 if eq["emotion"] != "neutral" else 0.2
                }
            }
        ) as response:
            result = await response.json()
            memories = result.get("memories", [])
            
            # Count emotion matches in retrieved memories
            matching_emotions = 0
            for memory in memories:
                memory_emotion = memory.get("metadata", {}).get("dominant_emotion")
                if memory_emotion == eq["emotion"]:
                    matching_emotions += 1
            
            # MAG should prioritize emotion-congruent memories
            # At least 50% of retrieved memories should match the query emotion
            assert matching_emotions >= len(memories) * 0.5, \
                   f"MAG gating not working for {eq['emotion']} emotion. " \
                   f"Only {matching_emotions}/{len(memories)} memories matched."
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

```

# tests\test_variant_mal.py

```py
# tests/test_variant_mal.py

import pytest
import pytest_asyncio
import asyncio
import json
import os
import time
from typing import Dict, List, Any

# Import our variant testing fixtures
from variant_conftest import api_clients, create_test_memories

# Get current variant for logging and assertions
CURRENT_VARIANT = os.environ.get('TITANS_VARIANT', 'UNKNOWN')
if CURRENT_VARIANT != 'MAL':
    pytest.skip(f"Skipping MAL tests since current variant is {CURRENT_VARIANT}", allow_module_level=True)

# Test functions specifically for MAL variant
@pytest.mark.asyncio
async def test_mal_variant_memory_processing(api_clients):
    """Test MAL variant's basic memory processing capabilities."""
    session, mc_client = api_clients
    
    # 1. Create test memories
    test_content = f"This is a MAL variant test memory created at {time.time()}"
    memory_ids = await create_test_memories(mc_client, count=3, 
                                          prefix=f"MAL-Variant-Test")
    
    # 2. Wait briefly for asynchronous processing
    await asyncio.sleep(1)  # Allow processing to complete
    
    # 3. Call CCE to process a related memory through the MAL variant
    async with session.post(
        "http://localhost:8002/process_memory",
        json={
            "content": f"This is a follow-up to the MAL-Variant-Test memories",
            "embedding": [float(i) / 100 for i in range(384)],  # Simple test embedding
            "metadata": {
                "source": "variant_test",
                "variant": "MAL"
            }
        }
    ) as response:
        assert response.status == 200, f"Failed to process memory via CCE: {await response.text()}"
        result = await response.json()
        assert "memory_id" in result, "No memory_id in response"
        cce_memory_id = result["memory_id"]
    
    # 4. Wait for CCE to process the memory (MAL model needs time for latent memory processing)
    await asyncio.sleep(3)  # Allow sufficient time for MAL processing
    
    # 5. Verify the CCE-processed memory exists in Memory Core
    retrieved_memory = await mc_client.get_memory(cce_memory_id)
    assert retrieved_memory is not None, f"Could not retrieve memory {cce_memory_id}"
    assert "metadata" in retrieved_memory, "No metadata in retrieved memory"
    
    # MAL specific: Memory should have been processed by MAL variant
    # The processing_info is deeply nested, so we need to handle it carefully
    metadata = retrieved_memory.get("metadata", {})
    processing_info = metadata.get("processing_info", {})
    
    # Check for MAL-specific indicators in the memory
    # Note: The exact structure depends on your implementation
    assert processing_info.get("variant") == "MAL" or \
           processing_info.get("titans_variant") == "MAL" or \
           metadata.get("titans_variant") == "MAL", \
           f"Memory not processed by MAL variant: {metadata}"
    
    # Clean up test memories
    for memory_id in memory_ids + [cce_memory_id]:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mal_variant_retrieval(api_clients):
    """Test MAL variant's unique latent memory retrieval behavior."""
    session, mc_client = api_clients
    
    # 1. Create semantically related memories for testing MAL's latent connecting abilities
    memory_contents = [
        "Quantum computing uses qubits instead of classical bits",
        "Superposition allows qubits to be in multiple states simultaneously",
        "Quantum entanglement is a phenomenon where particles become correlated",
        "Einstein called quantum entanglement 'spooky action at a distance'",
        "Richard Feynman was a pioneer in quantum electrodynamics"
    ]
    
    memory_ids = []
    for i, content in enumerate(memory_contents):
        memory_entry = {
            "content": content,
            "embedding": [float(j) / 100 for j in range(384)],  # Simple test embedding
            "metadata": {
                "source": "mal_variant_test",
                "test_id": i,
                "variant": "MAL"
            }
        }
        
        result = await mc_client.process_memory(memory_entry)
        memory_ids.append(result["memory_id"])
    
    # 2. Wait for processing - MAL needs time to develop latent connections
    await asyncio.sleep(3)
    
    # 3. Query with a term related to but not explicitly mentioned in our memories
    query = "What did Einstein think about quantum physics?"
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": query,
            "max_memories": 3
        }
    ) as response:
        assert response.status == 200, f"Failed to retrieve memories: {await response.text()}"
        result = await response.json()
        
        # 4. Verify MAL-specific retrieval behavior (latent connections)
        assert "memories" in result, "No memories in response"
        assert len(result["memories"]) > 0, "No memories retrieved"
        
        # MAL should find the Einstein reference through latent connections
        found_einstein = False
        for memory in result["memories"]:
            if "einstein" in memory["content"].lower():
                found_einstein = True
                break
        
        assert found_einstein, "MAL variant didn't retrieve Einstein-related memory through latent connections"
    
    # 5. Try another query that should benefit from MAL's latent space
    query = "Tell me about quantum phenomena"
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": query,
            "max_memories": 3
        }
    ) as response:
        assert response.status == 200
        result = await response.json()
        
        # Should retrieve memories about superposition or entanglement
        found_quantum_phenomenon = False
        for memory in result.get("memories", []):
            content = memory["content"].lower()
            if "superposition" in content or "entanglement" in content:
                found_quantum_phenomenon = True
                break
                
        assert found_quantum_phenomenon, "MAL didn't retrieve appropriate quantum phenomena memories"
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

@pytest.mark.asyncio
async def test_mal_variant_state(api_clients):
    """Test MAL variant state management and internal memory structure."""
    session, mc_client = api_clients
    
    # 1. Check Neural Memory server state to confirm MAL model is active
    async with session.get("http://localhost:8001/status") as response:
        assert response.status == 200, "Failed to get Neural Memory status"
        nm_status = await response.json()
        
        # The status response format depends on your implementation
        # Look for MAL-specific indicators
        assert "model_info" in nm_status, "No model info in Neural Memory status"
        model_info = nm_status["model_info"]
        
        # Verify it's running MAL variant
        if "variant" in model_info:
            assert model_info["variant"] == "MAL", f"Wrong variant: {model_info['variant']}"
        elif "architecture" in model_info:
            assert "MAL" in model_info["architecture"], f"MAL not in architecture: {model_info['architecture']}"
    
    # 2. Check context-cascade-engine status
    async with session.get("http://localhost:8002/status") as response:
        assert response.status == 200, "Failed to get CCE status"
        cce_status = await response.json()
        
        # Verify CCE is also configured for MAL
        # The exact path depends on your CCE status response format
        titan_config = cce_status.get("config", {}).get("titan", {})
        if titan_config:
            assert titan_config.get("variant") == "MAL" or \
                   titan_config.get("titans_variant") == "MAL", \
                   f"CCE not configured for MAL: {titan_config}"
        
        # Alternative check locations depending on implementation
        titans_variant = cce_status.get("titans_variant") or \
                        cce_status.get("config", {}).get("titans_variant")
        if titans_variant is not None:
            assert titans_variant == "MAL", f"Wrong CCE variant: {titans_variant}"

@pytest.mark.asyncio
async def test_mal_latent_memory_formation(api_clients):
    """Test MAL's ability to form latent memories from sequential inputs."""
    session, mc_client = api_clients
    
    # MAL variant is expected to develop latent representations
    # when processing a sequence of related memories
    
    # 1. Create a sequence of related but indirect memories
    test_sequence = [
        "Machine learning models require training data.",
        "Large datasets help improve model accuracy.",
        "Data preprocessing is an important step in machine learning.",
        "Feature engineering can significantly impact model performance.",
        "Hyperparameter tuning optimizes model configuration."
    ]
    
    # Process these in sequence through CCE to allow MAL to build latent space
    memory_ids = []
    for content in test_sequence:
        async with session.post(
            "http://localhost:8002/process_memory",
            json={
                "content": content,
                "metadata": {"test": "mal_latent_formation"}
            }
        ) as response:
            result = await response.json()
            memory_ids.append(result["memory_id"])
        # MAL may need more time between memories to form latent connections
        await asyncio.sleep(1)
    
    # 2. Allow processing to complete and latent space to develop
    await asyncio.sleep(5)
    
    # 3. Query with a concept not directly mentioned but latently related
    query = "How can we improve AI models?"
    async with session.post(
        "http://localhost:8002/retrieve_memories",
        json={
            "query": query,
            "max_memories": 3
        }
    ) as response:
        result = await response.json()
        memories = result.get("memories", [])
        
        # 4. MAL should retrieve memories about data, preprocessing, or tuning
        # even though "AI models" wasn't explicitly mentioned
        assert len(memories) > 0, "MAL variant didn't retrieve any memories"
        
        # Check if retrieved memories are relevant to improving models
        relevant_count = 0
        for memory in memories:
            content = memory["content"].lower()
            if any(term in content for term in ["data", "accuracy", "performance", "tuning", "preprocessing"]):
                relevant_count += 1
        
        # At least one memory should be relevant through latent connections
        assert relevant_count > 0, "MAL didn't form effective latent connections"
    
    # 5. Test Memory Core can directly retrieve the memories we created
    retrieved = await mc_client.retrieve_memories(
        query="Tell me about machine learning",
        max_memories=5
    )
    assert len(retrieved["memories"]) > 0, "No memories retrieved directly from Memory Core"
    
    # Clean up
    for memory_id in memory_ids:
        await mc_client.delete_memory(memory_id)

```

# tests\test_vector_index.py

```py
import pytest
import asyncio
import json
import time
import numpy as np
import os
import sys
import logging
from datetime import datetime
from synthians_memory_core.api.client.client import SynthiansClient
from synthians_memory_core.vector_index import MemoryVectorIndex

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("test_vector_index")

@pytest.mark.asyncio
async def test_faiss_vector_index_creation():
    """Test the creation and basic functionality of the FAISS vector index."""
    # Create a test vector index with a specific dimension
    dimension = 768
    index = MemoryVectorIndex({
        'embedding_dim': dimension,
        'storage_path': os.path.join(os.getcwd(), 'test_index'),
        'index_type': 'L2',
        'use_gpu': True  # This will use GPU if available, otherwise fall back to CPU
    })
    
    # Verify the index was created with the right parameters
    assert index.embedding_dim == dimension, f"Expected dimension {dimension}, got {index.embedding_dim}"
    logger.info(f"Created vector index with dimension {index.embedding_dim}, GPU usage: {index.is_using_gpu}")
    
    # Create some test embeddings
    num_vectors = 100
    test_vectors = np.random.random((num_vectors, dimension)).astype('float32')
    
    # Add vectors to the index
    for i in range(num_vectors):
        memory_id = f"test_memory_{i}"
        index.add(memory_id, test_vectors[i])
    
    # Verify the index contains the expected number of vectors
    assert index.count() == num_vectors, f"Expected {num_vectors} vectors in index, got {index.count()}"
    
    # Test search functionality
    query_vector = np.random.random(dimension).astype('float32')
    k = 10
    results = index.search(query_vector, k)
    
    # Verify search results format
    assert len(results) <= k, f"Expected at most {k} results, got {len(results)}"
    assert all(isinstance(r, tuple) and len(r) == 2 for r in results), "Results should be (memory_id, score) tuples"
    
    # Test index persistence
    index_path = os.path.join(index.storage_path, 'test_index.faiss')
    index.save(index_path)
    assert os.path.exists(index_path), f"Index file not found at {index_path}"
    
    # Test index loading
    new_index = MemoryVectorIndex({
        'embedding_dim': dimension,
        'storage_path': os.path.join(os.getcwd(), 'test_index'),
        'index_type': 'L2'
    })
    new_index.load(index_path)
    
    # Verify loaded index has the same vectors
    assert new_index.count() == index.count(), "Loaded index has different vector count"
    
    # Clean up
    if os.path.exists(index_path):
        os.remove(index_path)
    logger.info("Vector index creation and persistence test completed successfully")

@pytest.mark.asyncio
async def test_dimension_mismatch_handling():
    """Test the handling of embedding dimension mismatches."""
    # Create a vector index with specific dimension
    dimension = 768
    index = MemoryVectorIndex({
        'embedding_dim': dimension,
        'storage_path': os.path.join(os.getcwd(), 'test_index'),
        'index_type': 'L2'
    })
    
    # Create vectors with different dimensions
    smaller_dim = 384
    larger_dim = 1024
    
    standard_vector = np.random.random(dimension).astype('float32')
    smaller_vector = np.random.random(smaller_dim).astype('float32')
    larger_vector = np.random.random(larger_dim).astype('float32')
    
    # Add vectors with different dimensions
    index.add("standard_vector", standard_vector)
    index.add("smaller_vector", smaller_vector)  # Should be padded
    index.add("larger_vector", larger_vector)    # Should be truncated
    
    # Verify all vectors were added
    assert index.count() == 3, f"Expected 3 vectors in index, got {index.count()}"
    
    # Test search with different dimension vectors
    standard_results = index.search(standard_vector, 3)
    smaller_results = index.search(smaller_vector, 3)
    larger_results = index.search(larger_vector, 3)
    
    # Verify search results contain expected entries
    assert any(r[0] == "standard_vector" for r in standard_results), "Standard vector not found in results"
    assert any(r[0] == "smaller_vector" for r in smaller_results), "Smaller vector not found in results"
    assert any(r[0] == "larger_vector" for r in larger_results), "Larger vector not found in results"
    
    logger.info("Dimension mismatch handling test completed successfully")

@pytest.mark.asyncio
async def test_malformed_embedding_handling():
    """Test the handling of malformed embeddings (NaN/Inf values)."""
    # Create a vector index
    dimension = 768
    index = MemoryVectorIndex({
        'embedding_dim': dimension,
        'storage_path': os.path.join(os.getcwd(), 'test_index'),
        'index_type': 'L2'
    })
    
    # Create a normal vector and malformed vectors
    normal_vector = np.random.random(dimension).astype('float32')
    
    # Vector with NaN values
    nan_vector = np.random.random(dimension).astype('float32')
    nan_vector[10:20] = np.nan
    
    # Vector with Inf values
    inf_vector = np.random.random(dimension).astype('float32')
    inf_vector[30:40] = np.inf
    
    # Add vectors - the malformed ones should be handled gracefully
    index.add("normal_vector", normal_vector)
    
    # These should be handled by replacing with zeros or normalized vectors
    index.add("nan_vector", nan_vector)
    index.add("inf_vector", inf_vector)
    
    # Verify we can search without errors
    results = index.search(normal_vector, 3)
    assert len(results) > 0, "No results returned from search"
    
    # Search with malformed query vectors should also work
    nan_query = np.random.random(dimension).astype('float32')
    nan_query[5:15] = np.nan
    
    nan_results = index.search(nan_query, 3)
    assert len(nan_results) > 0, "No results returned from search with NaN query"
    
    logger.info("Malformed embedding handling test completed successfully")

@pytest.mark.asyncio
async def test_end_to_end_vector_retrieval():
    """End-to-end test of vector indexing and retrieval through the API."""
    async with SynthiansClient() as client:
        # Step 1: Create distinct test memories
        timestamp = datetime.now().isoformat()
        
        memory1 = await client.process_memory(
            content=f"FAISS vector index test memory Alpha at {timestamp}",
            metadata={"test_group": "vector_index", "category": "alpha"}
        )
        
        memory2 = await client.process_memory(
            content=f"FAISS vector index test memory Beta at {timestamp}",
            metadata={"test_group": "vector_index", "category": "beta"}
        )
        
        memory3 = await client.process_memory(
            content=f"FAISS vector index test memory Gamma at {timestamp}",
            metadata={"test_group": "vector_index", "category": "gamma"}
        )
        
        # Allow time for processing and indexing
        await asyncio.sleep(1)
        
        # Step 2: Retrieve with exact match
        alpha_query = f"Alpha at {timestamp}"
        alpha_results = await client.retrieve_memories(alpha_query, top_k=3)
        
        # Verify retrieval accuracy
        assert alpha_results.get("success") is True, "Retrieval failed"
        alpha_memories = alpha_results.get("memories", [])
        alpha_ids = [m.get("id") for m in alpha_memories]
        
        # Memory1 should be retrieved
        assert memory1.get("memory_id") in alpha_ids, "Alpha memory not found in retrieval results"
        
        # Step 3: Test with lower threshold to ensure retrieval works
        general_query = f"vector index test at {timestamp}"
        low_threshold_results = await client.retrieve_memories(
            general_query, 
            top_k=10, 
            threshold=0.3  # Lower threshold as per the memory improvement
        )
        
        all_memories = low_threshold_results.get("memories", [])
        all_ids = [m.get("id") for m in all_memories]
        
        # All memories should be retrieved with a lower threshold
        assert memory1.get("memory_id") in all_ids, "Memory 1 not found with low threshold"
        assert memory2.get("memory_id") in all_ids, "Memory 2 not found with low threshold"
        assert memory3.get("memory_id") in all_ids, "Memory 3 not found with low threshold"
        
        logger.info(f"Retrieved {len(all_memories)} memories with low threshold")
        logger.info("End-to-end vector retrieval test completed successfully")

if __name__ == "__main__":
    # For manual test execution
    asyncio.run(test_faiss_vector_index_creation())
    asyncio.run(test_dimension_mismatch_handling())
    asyncio.run(test_malformed_embedding_handling())
    asyncio.run(test_end_to_end_vector_retrieval())

```

# tests\variant_conftest.py

```py
# tests/variant_conftest.py

import pytest
import pytest_asyncio
import asyncio
import aiohttp
import os
import httpx # Using httpx for simpler async requests in checks

# Assuming SynthiansClient is available and targets the Memory Core API (e.g., port 5010)
from synthians_memory_core.api.client.client import SynthiansClient

MC_URL = "http://localhost:5010"
NM_URL = "http://localhost:8001"
CCE_URL = "http://localhost:8002"

# Fixture to provide API clients to tests
@pytest_asyncio.fixture
async def api_clients():
    # Provides aiohttp session for CCE/NM and dedicated client for MC
    async with aiohttp.ClientSession() as session, \
               SynthiansClient(base_url=MC_URL) as mc_client:
        # Optional: Add a quick health check *here* before yielding if desired
        # await check_services_quick(session)
        yield session, mc_client

# Optional: Quick check before each test function
@pytest_asyncio.fixture(autouse=True)
async def check_services_quick_fixture():
    """Quickly check if services seem responsive before each test"""
    async with httpx.AsyncClient(timeout=5.0) as client:
        try:
            resp_mc = await client.get(f"{MC_URL}/health")
            resp_nm = await client.get(f"{NM_URL}/health")
            resp_cce = await client.get(f"{CCE_URL}/") # Basic check
            resp_mc.raise_for_status()
            resp_nm.raise_for_status()
            resp_cce.raise_for_status()
        except (httpx.RequestError, httpx.HTTPStatusError) as e:
            pytest.fail(f"Service unresponsive before test: {e}")
        except Exception as e:
             pytest.fail(f"Unexpected error checking services: {e}")

# Keep other useful fixtures for variant tests
# Helper functions for test utilities
async def create_test_memories(client, count=5, prefix="Test memory"):
    """Create a batch of test memories for testing."""
    memory_ids = []
    for i in range(count):
        content = f"{prefix} {i}"
        memory_id = f"test_variant_{os.environ.get('TITANS_VARIANT', 'UNKNOWN')}_{i}"
        
        # Create a test memory with random embedding
        embedding = [float(j) / 100 for j in range(384)]  # 384-dimensional embedding
        
        # Use the API to create the memory
        memory_entry = {
            "content": content,
            "embedding": embedding,
            "metadata": {
                "source": "test",
                "test_id": i,
                "test_batch": prefix,
                "variant": os.environ.get('TITANS_VARIANT', 'UNKNOWN')
            }
        }
        
        # Store the memory in the database
        await client.process_memory(memory_entry, memory_id)
        memory_ids.append(memory_id)
    
    return memory_ids

```

# tools\__init__.py

```py


```

# tools\lucidia_think_trace.py

```py
#!/usr/bin/env python3
"""
Lucidia Think Trace - Cognitive Flow Diagnostic Utility

This utility enables end-to-end tracing of Lucidia's cognitive process:
1. Submits a query to Lucidia's ContextCascadeEngine
2. Captures the IntentGraph and cognitive trace
3. Retrieves and formats diagnostic metrics
4. Provides a visual representation of the cognitive flow

Usage:
    python lucidia_think_trace.py --query "What were the key design principles behind the Titans paper?"

Author: Lucidia Team
Created: 2025-03-28
"""

import argparse
import asyncio
import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

# Adjust Python path to find the proper modules
root_dir = str(Path(__file__).resolve().parent.parent)
if root_dir not in sys.path:  # Avoid adding duplicates
    sys.path.insert(0, root_dir)
    print(f"Added {root_dir} to sys.path")
else:
    print(f"{root_dir} already in sys.path")

import aiohttp
import numpy as np
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.tree import Tree
from rich import print as rprint

# --- Use ABSOLUTE IMPORTS ---
try:
    # Import directly from the package root
    from synthians_memory_core.geometry_manager import GeometryManager
    from synthians_memory_core.orchestrator.context_cascade_engine import ContextCascadeEngine
    from synthians_memory_core.synthians_trainer_server.metrics_store import get_metrics_store
except ImportError as e:
    print(f"Error importing modules: {e}")
    print(f"Current Python path: {sys.path}")
    print(f"Attempted to import from root: {root_dir}")
    print("Ensure synthians_memory_core and its submodules are correctly structured and importable.")
    sys.exit(1)

# Initialize rich console for pretty printing
console = Console()


async def run_diagnostic_test(query: str, emotion: Optional[str] = None,
                        metadata: Optional[Dict[str, Any]] = None,
                        memory_core_url: str = "http://localhost:5010",
                        neural_memory_url: str = "http://localhost:8001",
                        diagnostics_url: str = "http://localhost:8001/diagnose_emoloop",
                        window: str = "last_100") -> Dict[str, Any]:
    """
    Run a complete diagnostic test of Lucidia's cognitive process
    
    Args:
        query: The query to process
        emotion: Optional user emotion
        metadata: Optional metadata
        memory_core_url: URL of the Memory Core service
        neural_memory_url: URL of the Neural Memory Server
        diagnostics_url: URL of the diagnostics endpoint
        window: Window for diagnostics (last_100, last_hour, etc.)
        
    Returns:
        Dictionary with test results
    """
    # Prepare metadata
    if metadata is None:
        metadata = {}
    
    if emotion and "emotion" not in metadata:
        metadata["emotion"] = emotion
        
    metadata["session"] = metadata.get("session", f"diagnostic_{int(time.time())}")
    metadata["timestamp"] = datetime.now(timezone.utc).isoformat()
    
    # Initialize ContextCascadeEngine with geometry manager for proper embedding handling
    console.print("[bold blue]Initializing ContextCascadeEngine[/bold blue]")
    
    try:
        # Initialize GeometryManager with specific configuration for handling dimension mismatches
        # This ensures both 384 and 768 dimension embeddings can be handled safely
        geometry_manager = GeometryManager(config={
            'alignment_strategy': 'truncate',  # or 'pad' - truncate larger vectors to match smaller ones
            'normalization_enabled': True,      # ensure vectors are normalized before comparison
            'embedding_dim': 768               # default dimension, will be adjusted if needed
        })
        
        engine = ContextCascadeEngine(
            memory_core_url=memory_core_url,
            neural_memory_url=neural_memory_url,
            geometry_manager=geometry_manager,
            metrics_enabled=True
        )
    except Exception as e:
        console.print(f"[bold red]Error initializing ContextCascadeEngine:[/bold red] {e}")
        return {"error": str(e), "status": "initialization_failed"}
    
    # Process input
    console.print(f"[bold green]Processing query:[/bold green] {query}")
    start_time = time.time()
    try:
        # Safe processing that handles dimension mismatches and malformed embeddings
        response = await engine.process_new_input(
            content=query,
            embedding=None,  # Let the system generate the embedding
            metadata=metadata
        )
        process_time = time.time() - start_time
    except Exception as e:
        console.print(f"[bold red]Error processing input:[/bold red] {e}")
        return {"error": str(e), "status": "processing_failed", "process_time_ms": (time.time() - start_time) * 1000}
    
    # Get intent graph
    intent_id = response.get("intent_id")
    intent_graph = None
    intent_graph_path = None
    
    if intent_id:
        # Try to find the intent graph file
        intent_graphs_dir = Path("logs/intent_graphs")
        if intent_graphs_dir.exists():
            for file in intent_graphs_dir.glob(f"*{intent_id}*.json"):
                intent_graph_path = file
                try:
                    with open(file, 'r') as f:
                        intent_graph = json.load(f)
                except json.JSONDecodeError:
                    console.print(f"[bold yellow]Warning: Could not parse intent graph file:[/bold yellow] {file}")
                break
    
    # Get diagnostics
    diagnostics = None
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{diagnostics_url}?window={window}") as resp:
                if resp.status == 200:
                    diagnostics = await resp.json()
    except Exception as e:
        console.print(f"[bold red]Error getting diagnostics:[/bold red] {e}")
    
    # Format diagnostics as table if metrics_store is available
    formatted_diagnostics = None
    try:
        metrics_store = get_metrics_store()
        if metrics_store and diagnostics:
            formatted_diagnostics = metrics_store.format_diagnostics_as_table(diagnostics)
    except Exception as e:
        console.print(f"[bold yellow]Warning: Could not format diagnostics:[/bold yellow] {e}")
    
    return {
        "response": response,
        "intent_id": intent_id,
        "intent_graph": intent_graph,
        "intent_graph_path": str(intent_graph_path) if intent_graph_path else None,
        "diagnostics": diagnostics,
        "formatted_diagnostics": formatted_diagnostics,
        "process_time_ms": process_time * 1000
    }


def display_cognitive_trace(results: Dict[str, Any]) -> None:
    """
    Display a visual representation of the cognitive trace
    
    Args:
        results: Results from run_diagnostic_test
    """
    response = results.get("response", {})
    intent_graph = results.get("intent_graph")
    
    # Display response summary
    console.print("\n[bold cyan]RESPONSE SUMMARY[/bold cyan]")
    summary_table = Table(show_header=True)
    summary_table.add_column("Key", style="dim")
    summary_table.add_column("Value")
    
    summary_table.add_row("Memory ID", response.get("memory_id", "N/A"))
    summary_table.add_row("Intent ID", response.get("intent_id", "N/A"))
    summary_table.add_row("Status", response.get("status", "N/A"))
    summary_table.add_row("Time", f"{results.get('process_time_ms', 0):.2f} ms")
    
    # Add surprise metrics if available
    surprise = response.get("surprise_metrics", {})
    if surprise:
        loss = surprise.get("loss")
        grad_norm = surprise.get("grad_norm")
        boost = surprise.get("boost_calculated")
        
        if loss is not None:
            summary_table.add_row("Loss", f"{loss:.6f}")
        if grad_norm is not None:
            summary_table.add_row("Gradient Norm", f"{grad_norm:.6f}")
        if boost is not None:
            summary_table.add_row("QuickRecal Boost", f"{boost:.6f}")
    
    console.print(summary_table)
    
    # Display intent graph tree if available
    if intent_graph:
        console.print("\n[bold magenta]INTENT GRAPH TRACE[/bold magenta]")
        
        # Create tree structure
        tree = Tree(f"[bold]🧠 Cognitive Trace[/bold] ({response.get('intent_id', 'unknown')})")
        
        # Memory trace
        memory_trace = intent_graph.get("memory_trace", {})
        if memory_trace:
            mem_node = tree.add("[bold yellow]Memory Operations[/bold yellow]")
            
            # Memory storage
            storage = memory_trace.get("storage", [])
            if storage:
                storage_node = mem_node.add(f"[yellow]Storage ({len(storage)} operations)[/yellow]")
                for i, item in enumerate(storage[:3]):  # Show first 3
                    memory_id = item.get("memory_id", "unknown")
                    score = item.get("quickrecal_score", 0)
                    storage_node.add(f"Memory {i+1}: ID={memory_id}, QR={score:.4f}")
                if len(storage) > 3:
                    storage_node.add(f"... {len(storage)-3} more")
            
            # Memory retrievals
            retrieved = memory_trace.get("retrieved", [])
            if retrieved:
                retrieval_node = mem_node.add(f"[yellow]Retrievals ({len(retrieved)} operations)[/yellow]")
                for i, item in enumerate(retrieved[:3]):  # Show first 3
                    memory_id = item.get("memory_id", "unknown")
                    emotion = item.get("dominant_emotion", "neutral")
                    retrieval_node.add(f"Memory {i+1}: ID={memory_id}, Emotion={emotion}")
                if len(retrieved) > 3:
                    retrieval_node.add(f"... {len(retrieved)-3} more")
        
        # Neural memory trace
        neural_trace = intent_graph.get("neural_memory_trace", {})
        if neural_trace:
            neural_node = tree.add("[bold blue]Neural Memory Operations[/bold blue]")
            
            # Updates
            updates = neural_trace.get("updates", [])
            if updates:
                update_node = neural_node.add(f"[blue]Updates ({len(updates)} operations)[/blue]")
                for i, item in enumerate(updates[:3]):  # Show first 3
                    loss = item.get("loss", 0)
                    grad = item.get("grad_norm", 0)
                    update_node.add(f"Update {i+1}: Loss={loss:.6f}, GradNorm={grad:.6f}")
                if len(updates) > 3:
                    update_node.add(f"... {len(updates)-3} more")
        
        # Reasoning steps
        steps = intent_graph.get("reasoning_steps", [])
        if steps:
            reasoning_node = tree.add("[bold green]Reasoning Steps[/bold green]")
            for i, step in enumerate(steps):
                step_desc = step.get("description", "Unknown step")
                # Truncate if too long
                if len(step_desc) > 70:
                    step_desc = step_desc[:67] + "..."
                reasoning_node.add(f"Step {i+1}: {step_desc}")
        
        # Final output
        output = intent_graph.get("final_output", "No output recorded")
        result_node = tree.add("[bold cyan]Final Output[/bold cyan]")
        if isinstance(output, str) and len(output) > 100:
            result_node.add(f"{output[:97]}...")
        else:
            result_node.add(str(output))
        
        # Print the tree
        console.print(tree)
        
        # Print path to intent graph file
        if results.get("intent_graph_path"):
            console.print(f"\nFull intent graph saved to: [italic]{results['intent_graph_path']}[/italic]")
    
    # Display diagnostics if available
    if results.get("formatted_diagnostics"):
        console.print("\n[bold cyan]COGNITIVE DIAGNOSTICS[/bold cyan]")
        console.print(results["formatted_diagnostics"])
    elif results.get("diagnostics"):
        console.print("\n[bold cyan]COGNITIVE DIAGNOSTICS (raw)[/bold cyan]")
        console.print(json.dumps(results["diagnostics"], indent=2))


async def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Lucidia Think Trace - Cognitive Flow Diagnostic Utility")
    parser.add_argument("--query", type=str, required=True, help="Query to process")
    parser.add_argument("--emotion", type=str, default=None, help="User emotion (e.g., curiosity, confusion)")
    parser.add_argument("--memcore-url", type=str, default="http://localhost:5010", help="Memory Core URL")
    parser.add_argument("--neural-url", type=str, default="http://localhost:8001", help="Neural Memory Server URL")
    parser.add_argument("--window", type=str, default="last_100", help="Diagnostics window")
    parser.add_argument("--topic", type=str, default=None, help="Topic tag for metadata")
    parser.add_argument("--session", type=str, default=None, help="Session ID for metadata")
    parser.add_argument("--output-json", type=str, default=None, help="Save results to JSON file")
    
    args = parser.parse_args()
    
    # Prepare metadata
    metadata = {
        "source": "lucidia_think_trace"
    }
    
    if args.emotion:
        metadata["emotion"] = args.emotion
    
    if args.topic:
        metadata["topic"] = args.topic
        
    if args.session:
        metadata["session"] = args.session
    
    # Run diagnostic test
    console.print(Panel.fit(
        f"[bold]LUCIDIA THINK TRACE[/bold]\n\nQuery: {args.query}",
        title="🧠 Cognitive Flow Diagnostics",
        border_style="cyan"
    ))
    
    results = await run_diagnostic_test(
        query=args.query,
        emotion=args.emotion,
        metadata=metadata,
        memory_core_url=args.memcore_url,
        neural_memory_url=args.neural_url,
        window=args.window
    )
    
    # Display results
    display_cognitive_trace(results)
    
    # Save results to file if requested
    if args.output_json:
        # Remove formatted_diagnostics as it's not JSON serializable
        results_copy = {k: v for k, v in results.items() if k != "formatted_diagnostics"}
        with open(args.output_json, 'w') as f:
            json.dump(results_copy, f, indent=2)
        console.print(f"\nResults saved to: [italic]{args.output_json}[/italic]")
    
    return results


if __name__ == "__main__":
    try:
        results = asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[bold red]Interrupted by user[/bold red]")
        sys.exit(1)
    except Exception as e:
        console.print(f"\n[bold red]Error:[/bold red] {e}")
        import traceback
        console.print(traceback.format_exc())
        sys.exit(1)

```

# tools\repair_index.py

```py
#!/usr/bin/env python

"""
Repair index utility for Synthians Memory Core.

This script repairs the vector index by recreating ID mappings from persistent storage.
"""

import argparse
import asyncio
import logging
import os
import sys
from pathlib import Path

# Add parent directory to path to allow importing modules
parent_dir = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(parent_dir))

from synthians_memory_core.synthians_memory_core import SynthiansMemoryCore

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("repair_index")


async def repair_index(repair_type: str = "recreate_mapping", config_path: str = None):
    """Repair the vector index.
    
    Args:
        repair_type: Type of repair to perform (recreate_mapping, rebuild)
        config_path: Path to custom config file
    """
    # Use default config
    config = None
    
    if config_path and os.path.exists(config_path):
        import json
        with open(config_path, 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded custom config from {config_path}")
    
    # Initialize memory core
    logger.info(f"Initializing SynthiansMemoryCore with repair_type={repair_type}")
    memory_core = SynthiansMemoryCore(config)
    
    # Run repair
    logger.info(f"Running repair of type '{repair_type}'...")
    success = await memory_core.repair_index(repair_type)
    
    if success:
        logger.info(f"✅ Repair of type '{repair_type}' completed successfully")
        # Verify the index integrity
        is_consistent, diagnostics = memory_core.vector_index.verify_index_integrity()
        logger.info(f"Index consistency after repair: {is_consistent}")
        logger.info(f"Index diagnostics: {diagnostics}")
    else:
        logger.error(f"❌ Repair of type '{repair_type}' failed")
    
    return success


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Repair the Synthians Memory Core vector index")
    parser.add_argument(
        "--repair-type", 
        type=str, 
        default="auto", 
        choices=["auto", "recreate_mapping", "rebuild"],
        help="Type of repair to perform"
    )
    parser.add_argument(
        "--config", 
        type=str, 
        default=None,
        help="Path to custom config file"
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    asyncio.run(repair_index(args.repair_type, args.config))

```

# tools\repair_mapping.py

```py
#!/usr/bin/env python

"""
Repair ID mapping utility for Synthians Memory Core.

This script specifically fixes the ID mapping inconsistency where FAISS count > 0 but Mapping count = 0.
"""

import os
import sys
import json
import logging
import hashlib
import numpy as np
from pathlib import Path

# Add parent directory to path to allow importing modules
parent_dir = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(parent_dir))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("repair_mapping")


def scan_memory_files(memory_dir):
    """Scan all memory files in the directory to rebuild ID mapping.
    
    Args:
        memory_dir: Directory containing memory files
        
    Returns:
        dict: Dictionary mapping memory IDs to their numeric IDs
    """
    id_mapping = {}
    memory_ids = []
    
    # Find all memory files
    for root, _, files in os.walk(memory_dir):
        for file in files:
            if file.endswith('.json') and file.startswith('mem_'):
                memory_id = file.split('.')[0]  # Remove .json extension
                memory_ids.append(memory_id)
    
    logger.info(f"Found {len(memory_ids)} memory files")
    
    # Generate numeric IDs for all memory IDs
    for memory_id in memory_ids:
        numeric_id = int(hashlib.md5(memory_id.encode()).hexdigest(), 16) % (2**63-1)
        id_mapping[memory_id] = numeric_id
    
    return id_mapping


def save_mapping(id_mapping, storage_path):
    """Save ID mapping to a JSON file.
    
    Args:
        id_mapping: Dictionary mapping memory IDs to their numeric IDs
        storage_path: Path to save the mapping file
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        mapping_path = os.path.join(storage_path, 'faiss_index.bin.mapping.json')
        
        # Create a serializable copy of the mapping
        serializable_mapping = {}
        for k, v in id_mapping.items():
            # Convert any non-string keys to strings for JSON serializability
            key = str(k)
            # Convert any special numeric types to standard Python types
            if isinstance(v, (np.int64, np.int32, np.int16, np.int8)):
                value = int(v)
            else:
                value = v
            serializable_mapping[key] = value
        
        # Write the mapping to a file
        with open(mapping_path, 'w') as f:
            json.dump(serializable_mapping, f, indent=2)
        
        logger.info(f"Saved {len(serializable_mapping)} ID mappings to {mapping_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving ID mapping: {str(e)}")
        return False


def find_storage_path():
    """Find the storage path by looking for common directory structures."""
    possible_config_paths = [
        os.path.join(parent_dir, 'synthians_memory_core', 'config', 'core_config.json'),
        os.path.join(parent_dir, 'config', 'core_config.json'),
        os.path.join(parent_dir, 'core_config.json')
    ]
    
    # Try to find a config file
    for config_path in possible_config_paths:
        if os.path.exists(config_path):
            logger.info(f"Found config file at {config_path}")
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                storage_path = config.get('storage_path')
                if storage_path:
                    return storage_path
            except Exception as e:
                logger.warning(f"Error reading config file {config_path}: {str(e)}")
    
    # If no config file found, try common storage paths
    possible_storage_paths = [
        os.path.join(parent_dir, 'storage'),
        os.path.join(parent_dir, 'synthians_memory_core', 'storage'),
        os.path.join(parent_dir, 'data', 'storage'),
    ]
    
    for path in possible_storage_paths:
        if os.path.exists(path):
            logger.info(f"Found storage directory at {path}")
            return path
    
    # Last resort: Just use a path in the current directory
    default_path = os.path.join(parent_dir, 'storage')
    os.makedirs(default_path, exist_ok=True)
    logger.warning(f"No storage path found, using default: {default_path}")
    return default_path


def main():
    """Main function to repair ID mapping."""
    try:
        # Find storage path without relying on config
        storage_path = find_storage_path()
        logger.info(f"Using storage path: {storage_path}")
        
        # Look for memories directory
        memories_path = os.path.join(storage_path, 'memories')
        if not os.path.exists(memories_path):
            # Try to find the memories directory
            for root, dirs, _ in os.walk(storage_path):
                for dir_name in dirs:
                    if dir_name == 'memories':
                        memories_path = os.path.join(root, dir_name)
                        break
                if os.path.exists(memories_path):
                    break
        
        if not os.path.exists(memories_path):
            logger.warning("Could not find 'memories' directory, creating one")
            os.makedirs(memories_path, exist_ok=True)
        
        logger.info(f"Using memories path: {memories_path}")
        
        # Scan memory files to rebuild ID mapping
        id_mapping = scan_memory_files(memories_path)
        
        # Save mapping
        success = save_mapping(id_mapping, storage_path)
        
        if success:
            logger.info(f"Successfully rebuilt ID mapping with {len(id_mapping)} entries")
            
            # Print instructions for restarting the server
            logger.info("""\n===========================================================\n
\
ID mapping has been repaired. Please restart the server to load the fixed mapping.\n\
If problems persist, run the following in Python:\n\
    from synthians_memory_core.synthians_memory_core import SynthiansMemoryCore\n\
    import asyncio\n\
    asyncio.run(SynthiansMemoryCore().repair_index('recreate_mapping'))\n\
===========================================================\n""")
        else:
            logger.error("Failed to rebuild ID mapping")
    
    except Exception as e:
        logger.error(f"Error during ID mapping repair: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

```

# utils\__init__.py

```py
# synthians_memory_core/utils/__init__.py

from .transcription_feature_extractor import TranscriptionFeatureExtractor

__all__ = ['TranscriptionFeatureExtractor']

```

# utils\transcription_feature_extractor.py

```py
import numpy as np
from typing import Dict, Any, Optional, List, Union
import logging
import asyncio

from ..custom_logger import logger

class TranscriptionFeatureExtractor:
    """
    Extracts emotion and semantic features from transcribed voice input.
    Uses an emotion analyzer and optional keyword extractor to enrich transcription metadata.
    
    This class is designed to work with the EmotionAnalyzer and KeyBERT, but can be
    used with any compatible analyzers that follow the same interface.
    """

    def __init__(self, emotion_analyzer, keyword_extractor=None, config: Optional[Dict] = None):
        self.emotion_analyzer = emotion_analyzer  # EmotionAnalyzer instance
        self.keyword_extractor = keyword_extractor  # KeyBERT or similar
        self.config = config or {}
        
        # Default configuration with fallbacks
        self.top_n_keywords = self.config.get('top_n_keywords', 5)
        self.min_keyword_score = self.config.get('min_keyword_score', 0.3)
        self.include_ngrams = self.config.get('include_ngrams', True)
        
        logger.info("TranscriptionFeatureExtractor", "Initialized with" + 
                   f" emotion_analyzer={emotion_analyzer is not None}" +
                   f" keyword_extractor={keyword_extractor is not None}")
        
        # Lazy-load KeyBERT if not provided but needed
        self._keybert = None
    
    async def extract_features(self, transcript: str, meta: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Extract features from a transcript and return them as metadata.
        
        Args:
            transcript: The text transcript to analyze
            meta: Optional metadata about the audio (duration, etc.)
            
        Returns:
            A dictionary of extracted features suitable for metadata
        """
        if not transcript or not isinstance(transcript, str) or len(transcript.strip()) == 0:
            logger.warning("TranscriptionFeatureExtractor", "Empty or invalid transcript provided")
            return {"input_modality": "spoken", "source": "transcription", "error": "Empty transcript"}
        
        metadata = {}
        
        # Tag basic information about the input
        metadata["input_modality"] = "spoken"
        metadata["source"] = "transcription"
        metadata["word_count"] = len(transcript.split())
        
        # 1. Emotion Analysis
        emotion_features = await self._extract_emotion_features(transcript)
        if emotion_features:
            metadata.update(emotion_features)
        
        # 2. Keyword Extraction
        keyword_features = await self._extract_keyword_features(transcript)
        if keyword_features:
            metadata.update(keyword_features)
            
        # 3. Speech Metadata
        if meta:
            speech_features = self._extract_speech_features(transcript, meta)
            if speech_features:
                metadata.update(speech_features)
        
        logger.info("TranscriptionFeatureExtractor", 
                   f"Extracted {len(metadata)} features from transcript")
        return metadata
    
    async def _extract_emotion_features(self, text: str) -> Dict[str, Any]:
        """
        Extract emotion features using the emotion analyzer.
        """
        features = {}
        
        if self.emotion_analyzer is None:
            logger.warning("TranscriptionFeatureExtractor", "No emotion analyzer available")
            return features
        
        try:
            # Use our emotion analyzer to get emotion data
            emotion = await self.emotion_analyzer.analyze(text)
            
            # Extract the core emotional features
            features["dominant_emotion"] = emotion.get("dominant_emotion", "neutral")
            features["emotions"] = emotion.get("emotions", {})
            
            # Calculate derived features
            if "emotions" in emotion and emotion["emotions"]:
                # Get intensity (highest emotion score)
                features["intensity"] = max(emotion["emotions"].values())
                
                # Calculate sentiment value (-1 to 1 scale)
                pos_emotions = ["joy", "happiness", "excitement", "love", "optimism", "admiration"]
                neg_emotions = ["sadness", "anger", "fear", "disgust", "disappointment"]
                
                sentiment = 0.0
                for emotion_name, score in emotion["emotions"].items():
                    if emotion_name in pos_emotions:
                        sentiment += score
                    elif emotion_name in neg_emotions:
                        sentiment -= score
                
                # Normalize to [-1, 1]
                features["sentiment_value"] = max(min(sentiment, 1.0), -1.0)
            else:
                features["intensity"] = 0.5
                features["sentiment_value"] = 0.0
            
            # Create emotional_context for compatibility with other systems
            features["emotional_context"] = {
                "dominant_emotion": features["dominant_emotion"],
                "emotions": features["emotions"],
                "intensity": features["intensity"],
                "sentiment_value": features["sentiment_value"]
            }
            
        except Exception as e:
            logger.error("TranscriptionFeatureExtractor", f"Error in emotion analysis: {str(e)}")
            features["dominant_emotion"] = "neutral"
            features["intensity"] = 0.5
            features["sentiment_value"] = 0.0
        
        return features
    
    async def _extract_keyword_features(self, text: str) -> Dict[str, Any]:
        """
        Extract keyword features using KeyBERT or a similar keyword extractor.
        Lazy-loads KeyBERT if needed and not provided.
        """
        features = {}
        
        # Ensure we have a keyword extractor
        if self.keyword_extractor is None:
            # Try to lazy-load KeyBERT if possible
            if self._keybert is None:
                try:
                    loop = asyncio.get_event_loop()
                    self._keybert = await loop.run_in_executor(None, self._load_keybert)
                    if self._keybert is None:
                        logger.warning("TranscriptionFeatureExtractor", "Failed to load KeyBERT")
                        return features
                except Exception as e:
                    logger.error("TranscriptionFeatureExtractor", f"Error loading KeyBERT: {str(e)}")
                    return features
            
            # Use the lazy-loaded KeyBERT
            self.keyword_extractor = self._keybert
        
        # Extract keywords if we have an extractor
        if self.keyword_extractor:
            try:
                # Run in executor to avoid blocking
                loop = asyncio.get_event_loop()
                keywords = await loop.run_in_executor(
                    None, 
                    lambda: self.keyword_extractor.extract_keywords(
                        text, 
                        top_n=self.top_n_keywords,
                        keyphrase_ngram_range=(1, 3) if self.include_ngrams else (1, 1),
                        stop_words='english',
                        use_mmr=True,
                        diversity=0.7
                    )
                )
                
                # Filter by minimum score
                keywords = [(kw, score) for kw, score in keywords if score >= self.min_keyword_score]
                
                # Save as separate lists for keywords and scores
                features["keywords"] = [kw for kw, _ in keywords]
                features["keyword_scores"] = {kw: score for kw, score in keywords}
                
                # Also save as topic tags for compatibility
                features["topic_tags"] = features["keywords"][:3] if len(features["keywords"]) > 3 else features["keywords"]
                
            except Exception as e:
                logger.error("TranscriptionFeatureExtractor", f"Error extracting keywords: {str(e)}")
                features["keywords"] = []
                features["topic_tags"] = []
        
        return features
    
    def _extract_speech_features(self, text: str, meta: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract features related to speech patterns from metadata.
        """
        features = {}
        
        # Extract duration and calculate speaking rate
        duration = meta.get("duration_sec", None)
        if duration is not None and duration > 0:
            word_count = len(text.split())
            features["speaking_rate"] = round(word_count / duration, 2)  # words per second
            features["duration_sec"] = round(duration, 2)
        
        # Add interruption metadata if available
        features["user_interruptions"] = meta.get("user_interruptions", 0)
        features["was_interrupted"] = meta.get("was_interrupted", False)
        
        # Add timestamps if available
        if "interruption_timestamps" in meta and isinstance(meta["interruption_timestamps"], list):
            features["interruption_timestamps"] = meta["interruption_timestamps"]
            
        # Add conversation flow metrics
        if features["was_interrupted"]:
            # Flag for reflection triggers during retrieval
            features["requires_reflection"] = True
            
            # Add analysis of interruption severity
            if features["user_interruptions"] > 5:
                features["interruption_severity"] = "high"
            elif features["user_interruptions"] > 2:
                features["interruption_severity"] = "medium"
            else:
                features["interruption_severity"] = "low"
        
        # Add other speech-related metadata if available
        for key in ["speaker_id", "confidence", "language", "timestamp", "session_id"]:
            if key in meta:
                features[key] = meta[key]
        
        return features
    
    def _load_keybert(self):
        """
        Attempt to lazy-load KeyBERT if it's available.
        Returns None if KeyBERT can't be loaded.
        """
        try:
            from keybert import KeyBERT
            logger.info("TranscriptionFeatureExtractor", "Lazy-loading KeyBERT")
            return KeyBERT()
        except ImportError:
            logger.warning("TranscriptionFeatureExtractor", 
                         "KeyBERT not installed. Install with: pip install keybert")
            return None

```

# vector_index.py

```py
import logging
import os
import threading
import time
import numpy as np
import faiss
import json
from typing import Dict, List, Tuple, Any, Optional, Union
import hashlib
import uuid

logger = logging.getLogger(__name__)

# Dynamic FAISS import with fallback installation capability
try:
    import faiss
    logger.info("FAISS import successful")
    # Explicitly check for GPU support
    try:
        res = faiss.StandardGpuResources()
        logger.info("FAISS GPU support available")
    except Exception as e:
        logger.warning(f"FAISS GPU support not available: {e}")
except ImportError:
    logger.warning("FAISS not found, attempting to install")
    try:
        import subprocess
        
        # Try to detect GPU and install appropriate version
        try:
            # First check if CUDA is available via nvidia-smi
            nvidia_smi_output = subprocess.check_output(["nvidia-smi"], stderr=subprocess.STDOUT)
            # If we get here, nvidia-smi worked, so install GPU version
            logger.info("NVIDIA GPU detected, installing FAISS with GPU support")
            result = subprocess.run(["pip", "install", "faiss-gpu"], check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            # No GPU available, install CPU version
            logger.info("No NVIDIA GPU detected, installing FAISS CPU version")
            result = subprocess.run(["pip", "install", "faiss-cpu"], check=True)
        
        # Now try importing again
        import faiss
        logger.info("FAISS installed and imported successfully")
    except Exception as e:
        logger.error(f"Failed to install FAISS: {e}")
        raise

class MemoryVectorIndex:
    """A vector index for storing and retrieving memory embeddings."""

    def __init__(self, config: Dict[str, Any]):
        """Initialize the vector index.
        
        Args:
            config: A dictionary containing configuration options for the vector index.
                - embedding_dim: The dimension of the embeddings to store (int)
                - storage_path: The path to store the index (str)
                - index_type: The type of index to use (str, e.g., 'L2' or 'IP')
                - use_gpu: Whether to use GPU for the index (bool, default: False)
                - gpu_timeout_seconds: Seconds to wait for GPU init before fallback (int, default: 10)
        """
        self.config = config
        self.embedding_dim = config.get('embedding_dim', 768)
        self.storage_path = config.get('storage_path', './faiss_index')
        self.index_type = config.get('index_type', 'L2')
        self.use_gpu = config.get('use_gpu', False)
        self.gpu_timeout_seconds = config.get('gpu_timeout_seconds', 10)
        self.id_to_index = {}  # Maps memory IDs to their indices in the FAISS index
        self.is_using_gpu = False  # Will be set to True if GPU init succeeds
        
        # Initialize the index based on the configuration
        self._initialize_index()

    def _initialize_index(self, force_cpu=False, use_id_map=False):
        """Initialize the FAISS index for the vector store.
        
        Args:
            force_cpu (bool): If True, forces CPU usage even if GPU is requested (for incompatible index types)
            use_id_map (bool): If True, use IndexIDMap for the index
        """
        try:
            # Create a flat index for L2 or IP distance
            if self.index_type.upper() == 'L2':
                base_index = faiss.IndexFlatL2(self.embedding_dim)
            else: # Inner Product or Cosine
                base_index = faiss.IndexFlatIP(self.embedding_dim)
                
            # For GPU usage, try to create a GPU version of the index
            # IMPORTANT: FAISS GPU indexes don't support add_with_ids, so we need CPU for IndexIDMap
            if self.use_gpu and not force_cpu:
                try:
                    # Create GPU resources
                    if not hasattr(faiss, 'StandardGpuResources'):
                        logger.warning("GPU FAISS not available. Falling back to CPU.")
                        self.is_using_gpu = False
                    else:
                        self.gpu_resources = faiss.StandardGpuResources()
                        # Convert the index to a GPU index
                        base_index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, base_index)
                        self.is_using_gpu = True
                        logger.info(f"Using GPU FAISS index")
                except Exception as e:
                    logger.warning(f"Failed to initialize GPU index: {str(e)}. Falling back to CPU.")
                    self.is_using_gpu = False
                    # Re-create the base index since the conversion may have failed
                    if self.index_type.upper() == 'L2':
                        base_index = faiss.IndexFlatL2(self.embedding_dim)
                    else: # Inner Product or Cosine
                        base_index = faiss.IndexFlatIP(self.embedding_dim)
            else:
                self.is_using_gpu = False
                
            # Wrap the base index with an IndexIDMap to handle custom IDs
            # NOTE: ID map is incompatible with GPU indexes, so we need to use CPU
            if use_id_map and hasattr(faiss, 'IndexIDMap'):
                # If we're using GPU but need IndexIDMap, fall back to CPU
                if self.is_using_gpu:
                    logger.warning("IndexIDMap is incompatible with GPU indexes. Falling back to CPU.")
                    # Re-create the base index on CPU
                    if self.index_type.upper() == 'L2':
                        base_index = faiss.IndexFlatL2(self.embedding_dim)
                    else: # Inner Product or Cosine
                        base_index = faiss.IndexFlatIP(self.embedding_dim)
                    self.is_using_gpu = False
                
                self.index = faiss.IndexIDMap(base_index)
                logger.info(f"Created IndexIDMap with {self.index_type} base index, dimension {self.embedding_dim}")
            else:
                # Fallback if IndexIDMap is not available
                self.index = base_index
                logger.warning("IndexIDMap is not available. Using base index instead.")
                
            return True
        except Exception as e:
            logger.error(f"Error initializing index: {str(e)}")
            return False

    def add(self, memory_id: str, embedding: np.ndarray) -> bool:
        """Add a memory vector to the index.
        
        Args:
            memory_id: The unique ID of the memory
            embedding: The embedding vector
            
        Returns:
            bool: Whether the add was successful
        """
        try:
            # Validate the embedding
            if not self._validate_embedding(embedding):
                logger.warning(f"Invalid embedding for memory {memory_id}, skipping")
                return False
                
            # Ensure embedding has correct shape
            if len(embedding.shape) == 1:
                embedding = embedding.reshape(1, -1)
                
            # Generate a numeric ID for this memory if needed
            numeric_id = self._get_numeric_id(memory_id)
            
            # Different add approach based on index type
            if hasattr(self.index, 'add_with_ids'):
                # If using IndexIDMap
                try:
                    self.index.add_with_ids(embedding, np.array([numeric_id]))
                    self.id_to_index[memory_id] = numeric_id
                    # Backup id mapping after each add for better recovery
                    self._backup_id_mapping()
                    return True
                except Exception as e:
                    logger.error(f"Failed to add with IDs: {str(e)}")
                    # Fall back to standard add method
                    pass
                    
            # If not using IDMap or if add_with_ids failed
            if not hasattr(self.index, 'add_with_ids'):
                # Standard add approach
                index_before = self.count()
                self.index.add(embedding)
                if self.count() > index_before:
                    self.id_to_index[memory_id] = index_before  # First new index
                    # Backup id mapping after each add for better recovery
                    self._backup_id_mapping()
                    return True
                else:
                    logger.warning(f"Failed to add embedding for memory {memory_id}")
            
            return False
                
        except Exception as e:
            logger.error(f"Error adding memory to index: {str(e)}")
            return False

    def _backup_id_mapping(self) -> bool:
        """Backup the ID mapping to a JSON file for recovery purposes.
        
        Returns:
            bool: True if successful, False otherwise
        """
        try:
            mapping_path = os.path.join(self.storage_path, 'faiss_index.bin' + '.mapping.json')
            
            # Create a serializable copy of the mapping
            serializable_mapping = {}
            for k, v in self.id_to_index.items():
                # Convert any non-string keys to strings for JSON serializability
                key = str(k)
                # Convert any special numeric types to standard Python types
                if isinstance(v, (np.int64, np.int32, np.int16, np.int8)):
                    value = int(v)
                else:
                    value = v
                serializable_mapping[key] = value
            
            # Write the mapping to a file
            with open(mapping_path, 'w') as f:
                json.dump(serializable_mapping, f, indent=2)
            
            return True
        except Exception as e:
            logger.error(f"Error backing up ID mapping: {str(e)}")
            return False

    def search(self, query_embedding: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
        """Search the index for similar embeddings.

        Args:
            query_embedding: The query embedding
            k: The number of results to return

        Returns:
            List[Tuple[str, float]]: A list of memory IDs and similarity scores in descending order of similarity
        """
        try:
            # Validate the query embedding
            validated_query = self._validate_embedding(query_embedding)
            if validated_query is None:
                logger.error("Invalid query embedding")
                return []
                
            # Log the state of the index and mappings
            current_count = self.count()
            logger.debug(f"Searching index with {current_count} vectors and {len(self.id_to_index)} id mappings")
            
            if current_count == 0:
                logger.warning("Search called on empty index")
                return []
                
            # Ensure k is not larger than the number of items in the index
            k = min(k, current_count)
            if k <= 0:
                logger.warning(f"Search k value adjusted to 0 or less ({k}), returning empty list.")
                return []
                
            # Different search approach based on index type
            if hasattr(self.index, 'search_and_reconstruct'):
                # For IndexIDMap, we get IDs directly
                distances, numeric_ids, _ = self.index.search_and_reconstruct(np.array([validated_query], dtype=np.float32), k)
                
                # Log raw results for debugging
                logger.debug(f"FAISS raw results - distances: {distances}, IDs: {numeric_ids}")
                
                # Convert the results to memory IDs and scores
                results = []
                if len(numeric_ids) > 0 and len(distances) > 0:  # Check if search returned anything
                    # Ensure indices are flattened and valid
                    valid_ids = [idx for idx in numeric_ids[0] if idx >= 0]  # Filter out -1 indices
                    valid_distances = [distances[0][i] for i, idx in enumerate(numeric_ids[0]) if idx >= 0]  # Filter corresponding distances
                    
                    logger.debug(f"Valid IDs after filtering -1: {valid_ids}")
                    
                    # Create reverse mapping from numeric_id to memory_id
                    numeric_to_memory_id = {v: k for k, v in self.id_to_index.items()}
                    
                    for i, numeric_id in enumerate(valid_ids):
                        dist = valid_distances[i]
                        
                        # Convert distances to similarity scores based on index type
                        if self.index_type.upper() == 'L2':
                            similarity = float(np.exp(-dist))  # Convert distance to similarity
                        else:  # IP or Cosine (already similarities)
                            similarity = float(dist)
                        
                        # Find the memory ID for this numeric ID
                        memory_id = numeric_to_memory_id.get(int(numeric_id))
                        
                        if memory_id is not None:
                            results.append((memory_id, similarity))
                            logger.debug(f"Candidate: ID={memory_id}, NumericID={numeric_id}, Similarity={similarity:.4f}")
                        else:
                            # Enhanced diagnostics for missing mappings
                            logger.warning(f"No memory ID found for numeric ID {numeric_id}")
            else:
                # Fallback for non-IDMap indices - use legacy approach
                distances, indices = self.index.search(np.array([validated_query], dtype=np.float32), k)
                
                # Convert the results to memory IDs and scores using the old mapping approach
                index_to_id = {idx: mid for mid, idx in self.id_to_index.items()}
                results = []
                
                if len(indices) > 0 and len(distances) > 0:  # Check if search returned anything
                    valid_indices = [idx for idx in indices[0] if idx >= 0]  # Filter out -1 indices
                    valid_distances = [distances[0][i] for i, idx in enumerate(indices[0]) if idx >= 0]  # Filter corresponding distances
                    
                    for i, idx in enumerate(valid_indices):
                        dist = valid_distances[i]
                        
                        # Convert distances to similarity scores based on index type
                        if self.index_type.upper() == 'L2':
                            similarity = float(np.exp(-dist))  # Convert distance to similarity
                        else:  # IP or Cosine (already similarities)
                            similarity = float(dist)
                        
                        # Find the memory ID for this index using the reverse mapping
                        memory_id = index_to_id.get(int(idx))
                        
                        if memory_id is not None:
                            results.append((memory_id, similarity))
                            logger.debug(f"Legacy candidate: ID={memory_id}, Index={idx}, Similarity={similarity:.4f}")
                        else:
                            logger.warning(f"No memory ID found for index {idx}")
            
            logger.info(f"FAISS search returning {len(results)} raw candidates")
            return results
        except Exception as e:
            logger.error(f"Error searching index: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _validate_embedding(self, embedding: Union[np.ndarray, list, tuple]) -> Optional[np.ndarray]:
        """Validate and normalize an embedding vector.
        
        This handles several common issues:
        1. Converts lists/tuples to numpy arrays
        2. Ensures the embedding is 1D
        3. Checks for NaN or Inf values
        4. Ensures the embedding has the correct dimension
        
        Args:
            embedding: The embedding vector to validate
            
        Returns:
            np.ndarray: A validated embedding vector, or None if invalid
        """
        try:
            # Handle case where embedding is a dict (common error)
            if isinstance(embedding, dict):
                logger.error("Embedding is a dict, not a vector. You may have passed a structured payload instead.")
                return None
                
            # Convert to numpy array if not already
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding, dtype=np.float32)
            
            # Ensure embedding is 1D
            if len(embedding.shape) > 1:
                # If it's a 2D array with only one row, flatten it
                if len(embedding.shape) == 2 and embedding.shape[0] == 1:
                    embedding = embedding.flatten()
                else:
                    logger.error(f"Expected 1D embedding, got shape {embedding.shape}")
                    return None
            
            # Check for NaN or Inf values
            if np.isnan(embedding).any() or np.isinf(embedding).any():
                logger.warning("Embedding contains NaN or Inf values. Replacing with zeros.")
                embedding = np.where(np.isnan(embedding) | np.isinf(embedding), 0.0, embedding)
            
            # Check dimension
            if len(embedding) != self.embedding_dim:
                logger.warning(f"Embedding dimension mismatch: expected {self.embedding_dim}, got {len(embedding)}")
                # Resize the embedding to match expected dimension
                if len(embedding) < self.embedding_dim:
                    # Pad with zeros
                    padding = np.zeros(self.embedding_dim - len(embedding), dtype=np.float32)
                    embedding = np.concatenate([embedding, padding])
                else:
                    # Truncate
                    embedding = embedding[:self.embedding_dim]
            
            # Ensure dtype is float32 for FAISS
            embedding = embedding.astype(np.float32)
            
            return embedding
        except Exception as e:
            logger.error(f"Error validating embedding: {str(e)}")
            return None

    def count(self) -> int:
        """Get the number of embeddings in the index.
        
        Returns:
            int: The number of embeddings in the index
        """
        try:
            index_count = self.index.ntotal if hasattr(self.index, 'ntotal') else 0
            mapping_count = len(self.id_to_index) if hasattr(self, 'id_to_index') else 0
            
            # Check for inconsistencies
            if index_count != mapping_count:
                logger.warning(f"Vector index inconsistency detected! FAISS count: {index_count}, Mapping count: {mapping_count}")
                
            return index_count
        except Exception as e:
            logger.error(f"Error getting index count: {str(e)}")
            return 0

    def reset(self) -> bool:
        """Reset the index, removing all embeddings.
        
        Returns:
            bool: True if the index was reset successfully, False otherwise
        """
        try:
            # Re-initialize the index
            self._initialize_index()
            self.id_to_index = {}
            return True
        except Exception as e:
            logger.error(f"Error resetting index: {str(e)}")
            return False

    def save(self, filepath: Optional[str] = None) -> bool:
        """Save the index to disk.
        
        When using IndexIDMap, we save the entire index including the ID mappings in a single file.
        
        Args:
            filepath: The filepath to save the index to. If None, use the storage_path.
            
        Returns:
            bool: True if the index was saved successfully, False otherwise
        """
        try:
            # Create storage directory if it doesn't exist
            os.makedirs(self.storage_path, exist_ok=True)
            
            if filepath is None:
                filepath = os.path.join(self.storage_path, 'faiss_index.bin')
            
            # Prepare the index for saving
            index_to_save = self.index
            
            # If using GPU, extract CPU index first
            if self.is_using_gpu:
                try:
                    index_to_save = faiss.index_gpu_to_cpu(self.index)
                    logger.info("Successfully converted GPU index to CPU for saving")
                except Exception as e:
                    logger.warning(f"Could not extract CPU index from GPU index: {e}. Saving with default method.")
            
            # Save the FAISS index
            faiss.write_index(index_to_save, filepath)
            
            # Check if we need to save the id_to_index mapping separately
            # With IndexIDMap this might not be necessary, but we keep it for backward compatibility
            # and as a safety backup
            mapping_path = filepath + '.mapping.json'
            try:
                with open(mapping_path, 'w') as f:
                    # Convert any non-string keys to strings for JSON serialization
                    mapping_serializable = {str(k): v for k, v in self.id_to_index.items()}
                    json.dump(mapping_serializable, f)
                logger.info(f"Saved backup ID mapping to {mapping_path} with {len(self.id_to_index)} entries")
            except Exception as map_e:
                logger.warning(f"Failed to save ID mapping: {map_e}. Index saved but mapping may be lost.")
            
            logger.info(f"Successfully saved index to {filepath} with {self.count()} vectors")
            return True
        except Exception as e:
            logger.error(f"Error saving index: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def load(self, filepath: Optional[str] = None) -> bool:
        """Load the index from disk.
        
        Args:
            filepath: The filepath to load the index from. If None, use the storage_path.
            
        Returns:
            bool: True if the index was loaded successfully, False otherwise
        """
        try:
            if filepath is None:
                filepath = os.path.join(self.storage_path, 'faiss_index.bin')
                mapping_path = filepath + '.mapping.json'
            else:
                # If custom filepath, derive mapping path by adding .mapping.json extension
                mapping_path = filepath + '.mapping.json'
            
            if not os.path.exists(filepath):
                logger.warning(f"Index file not found at {filepath}. Starting fresh.")
                self._initialize_index()  # Initialize an empty index
                self.id_to_index = {}
                return False  # Indicate load didn't happen, but state is clean
            
            if os.path.isdir(filepath):
                logger.error(f"Expected a file but got a directory: {filepath}")
                return False
                
            # --- Load the index data from disk ---
            logger.info(f"Loading FAISS index data from {filepath}")
            loaded_cpu_index = faiss.read_index(filepath)
            logger.info(f"Successfully loaded CPU index data, ntotal={loaded_cpu_index.ntotal}")

            # --- Check if the loaded index uses IndexIDMap ---
            is_index_id_map = hasattr(loaded_cpu_index, 'id_map')
            logger.info(f"Loaded index is{'not' if not is_index_id_map else ''} an IndexIDMap")
            
            # --- Handle the loaded index (CPU or GPU) ---
            if self.use_gpu and hasattr(faiss, 'StandardGpuResources'):
                logger.info("Attempting to move loaded index data to GPU...")
                try:
                    res = faiss.StandardGpuResources()
                    self.index = faiss.index_cpu_to_gpu(res, 0, loaded_cpu_index)
                    self.is_using_gpu = True
                    logger.info(f"Successfully moved loaded index to GPU, ntotal={self.index.ntotal}")
                except Exception as e:
                    logger.error(f"Failed to move loaded index to GPU: {e}. Falling back to CPU.")
                    self.index = loaded_cpu_index  # Fallback to the loaded CPU index
                    self.is_using_gpu = False
            else:
                # If not using GPU, assign the loaded CPU index directly
                self.index = loaded_cpu_index
                self.is_using_gpu = False
                logger.info(f"Using loaded CPU index, ntotal={self.index.ntotal}")
            
            # --- Attempt to load or rebuild the ID-to-index mapping ---
            self.id_to_index = {}  # Reset mapping before loading
            
            # If the index is an IndexIDMap, we can extract IDs directly
            if is_index_id_map:
                # For IndexIDMap, we need to rebuild the id_to_index from the index itself
                # This will be done later in a full rebuild if needed
                logger.info("Loaded index is an IndexIDMap, will extract IDs directly for operations")
            
            # Optionally load the backup mapping file (even for IndexIDMap as it has string->numeric mapping)
            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r') as f:
                        mapping_data = json.load(f)
                        
                    if isinstance(mapping_data, dict):
                        # Convert string keys back to their original type if needed
                        self.id_to_index = {k: int(v) if isinstance(v, str) and v.isdigit() else v 
                                           for k, v in mapping_data.items()}
                        logger.info(f"Successfully loaded {len(self.id_to_index)} memory mappings from {mapping_path}")
                except Exception as map_e:
                    logger.warning(f"Error loading mapping file {mapping_path}: {map_e}. May need to rebuild mapping.")
            else:
                logger.warning(f"Mapping file not found at {mapping_path}. Will rely on IndexIDMap internal mapping if available.")
            
            # For consistency checking and backup purposes
            if not is_index_id_map and (self.index.ntotal != len(self.id_to_index)):
                logger.warning(f"Mismatch after load: FAISS index has {self.index.ntotal} vectors, mapping has {len(self.id_to_index)} entries.")
            
            logger.info("Index load process completed.")
            return True
        except Exception as e:
            logger.error(f"General error loading index: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            # Reset index on general error
            self._initialize_index()
            self.id_to_index = {}
            return False

    def verify_index_integrity(self) -> Tuple[bool, Dict[str, Any]]:
        """Verify the integrity of the index and the ID mapping.
        
        This method performs a thorough check of the index to identify any inconsistencies.
        
        Returns:
            Tuple[bool, Dict]: A tuple containing a boolean indicating whether the index is consistent,
                              and a dictionary with diagnostic information
        """
        try:
            # Initialize diagnostics
            diagnostics = {
                "faiss_count": 0,
                "id_mapping_count": 0,
                "is_index_id_map": False,
                "index_implementation": "Unknown",
                "is_consistent": False,
                "backup_mapping_exists": False,
                "backup_mapping_count": 0
            }
            
            # Check if the index is empty
            if self.index is None:
                return False, {**diagnostics, "error": "Index is None"}
                
            # Get the index type
            index_type = type(self.index).__name__
            diagnostics["index_implementation"] = index_type
            
            # Check if the index is an IndexIDMap
            is_index_id_map = hasattr(self.index, 'id_map')
            diagnostics["is_index_id_map"] = is_index_id_map
            
            # Count the number of vectors in the index
            faiss_count = self.count()
            diagnostics["faiss_count"] = faiss_count
            
            # Count the number of ID mappings
            id_mapping_count = len(self.id_to_index)
            diagnostics["id_mapping_count"] = id_mapping_count
            
            # Check if the ID mapping count matches the FAISS count
            if faiss_count != id_mapping_count:
                logger.warning(f"Vector index inconsistency detected! FAISS count: {faiss_count}, Mapping count: {id_mapping_count}")
                
                # Try to recover from backup mapping file if available
                mapping_path = os.path.join(self.storage_path, 'faiss_index.bin' + '.mapping.json')
                if os.path.exists(mapping_path):
                    diagnostics["backup_mapping_exists"] = True
                    try:
                        with open(mapping_path, 'r') as f:
                            mapping_data = json.load(f)
                        
                        if isinstance(mapping_data, dict):
                            diagnostics["backup_mapping_count"] = len(mapping_data)
                            
                            if id_mapping_count == 0 and len(mapping_data) > 0:
                                logger.warning("Empty ID mapping detected with available backup - recommend running repair_index")
                    except Exception as e:
                        logger.error(f"Error checking backup mapping file: {str(e)}")
            
            # The index is consistent if the counts match
            is_consistent = (faiss_count == id_mapping_count) or (is_index_id_map and id_mapping_count > 0)
            diagnostics["is_consistent"] = is_consistent
            
            return is_consistent, diagnostics
        
        except Exception as e:
            logger.error(f"Error verifying index integrity: {str(e)}")
            return False, {"error": str(e)}

    def migrate_to_idmap(self, force_cpu: bool = True) -> bool:
        """Migrate from a standard index to an IndexIDMap index.
        
        This method extracts all vectors from the current index,
        creates a new IndexIDMap, and adds all vectors with their IDs.
        
        Args:
            force_cpu: Whether to force CPU usage during migration
            
        Returns:
            bool: True if migration was successful, False otherwise
        """
        try:
            # Check if we already have an IndexIDMap
            if hasattr(self.index, 'id_map'):
                logger.info("Index is already using IndexIDMap, no migration needed")
                return True
                
            logger.info(f"Starting migration to IndexIDMap (force_cpu={force_cpu})")
            
            # Save the current index and mappings
            old_index = self.index
            old_id_to_index = self.id_to_index.copy()  # Copy to avoid modifying during iteration
            
            # Get the current vector count
            original_count = old_index.ntotal
            logger.info(f"Index contains {original_count} vectors before migration")
            
            if original_count == 0:
                logger.info("Empty index, creating fresh IndexIDMap")
                self._initialize_index(force_cpu=force_cpu, use_id_map=True)
                return True
                
            # Check mapping consistency
            if len(old_id_to_index) != original_count:
                logger.warning(f"Inconsistent ID mapping during migration: {len(old_id_to_index)} mappings for {original_count} vectors")
                # Continue anyway as migration might actually help fix this
                
            # Create a list to hold all the vectors and their IDs
            vectors = []
            ids = []
            id_mapping = {}
            next_id = 0
            
            # Special case: If we have vectors but no ID mapping, we need a special approach
            if original_count > 0 and len(old_id_to_index) == 0:
                logger.info("Using sequential extraction for index with no ID mappings")
                try:
                    # Extract vectors directly using a sequential approach
                    memory_ids = []
                    # Try to find memory files to get real memory IDs
                    memory_path = os.path.join(os.path.dirname(self.storage_path), 'memories')
                    
                    # Fallback paths if standard path doesn't exist
                    if not os.path.exists(memory_path):
                        alt_paths = [
                            os.path.join(self.storage_path, 'memories'),
                            os.path.join(os.path.dirname(os.path.dirname(self.storage_path)), 'memories')
                        ]
                        for path in alt_paths:
                            if os.path.exists(path):
                                memory_path = path
                                logger.info(f"Found memories directory at: {memory_path}")
                                break
                    
                    # If memory directory found, read memory IDs from files
                    if os.path.exists(memory_path):
                        for root, _, files in os.walk(memory_path):
                            for file in files:
                                if file.endswith('.json') and file.startswith('mem_'):
                                    memory_id = file.split('.')[0]  # Remove .json extension
                                    memory_ids.append(memory_id)
                        logger.info(f"Found {len(memory_ids)} memory files")
                    
                    # If we have memory_ids from files and they match the count, use them
                    if len(memory_ids) >= original_count:
                        logger.info(f"Using real memory IDs from files for extraction: {len(memory_ids)} available")
                        memory_ids = memory_ids[:original_count]  # Limit to number of vectors
                    else:
                        # Generate synthetic memory IDs if we couldn't find them or counts don't match
                        logger.warning(f"Generating synthetic memory IDs for {original_count} vectors (found only {len(memory_ids)} real IDs)")
                        memory_ids = [f"mem_{uuid.uuid4().hex[:12]}" for _ in range(original_count)]
                    
                    # Extract vectors using index.reconstruct with sequential indices
                    for i in range(original_count):
                        try:
                            memory_id = memory_ids[i]
                            numeric_id = self._get_numeric_id(memory_id)
                            
                            # Extract vector - approach depends on index type
                            if hasattr(old_index, 'reconstruct'):
                                vector = old_index.reconstruct(i)
                                vectors.append(vector)
                                ids.append(numeric_id)
                                id_mapping[memory_id] = numeric_id
                                next_id += 1
                            elif hasattr(old_index, 'xb'):
                                vector = old_index.xb[i * old_index.d: (i + 1) * old_index.d].reshape(1, -1)
                                vectors.append(vector.reshape(-1))
                                ids.append(numeric_id)
                                id_mapping[memory_id] = numeric_id
                                next_id += 1
                        except Exception as e:
                            logger.warning(f"Error extracting vector at index {i}: {str(e)}")
                    
                    logger.info(f"Extracted {len(vectors)} vectors using sequential extraction")
                except Exception as e:
                    logger.error(f"Error during sequential extraction: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            # If no vectors extracted yet and we have ID mappings, try the standard approaches
            if not vectors and len(old_id_to_index) > 0:
                # Approach 1: If the old index allows reconstruction
                if hasattr(old_index, 'reconstruct'):
                    logger.info("Using vector reconstruction from original index")
                    try:
                        # For indices that support reconstruction
                        for memory_id, idx in old_id_to_index.items():
                            vector = old_index.reconstruct(idx)
                            
                            # Generate a consistent numeric ID for this memory_id
                            numeric_id = self._get_numeric_id(memory_id)
                            
                            vectors.append(vector)
                            ids.append(numeric_id)
                            id_mapping[memory_id] = numeric_id
                            next_id += 1
                    except Exception as e:
                        logger.error(f"Error during vector reconstruction: {str(e)}")
                        return False
                # Approach 2: For CPU indices, directly access the storage
                elif not force_cpu and hasattr(old_index, 'xb'):
                    logger.info("Using direct vector access from CPU index")
                    try:
                        # For CPU indices, we can directly access the vectors
                        for memory_id, idx in old_id_to_index.items():
                            if 0 <= idx < old_index.ntotal:
                                vector = old_index.xb[idx * old_index.d: (idx + 1) * old_index.d].reshape(1, -1)
                                
                                # Generate a consistent numeric ID for this memory_id
                                numeric_id = self._get_numeric_id(memory_id)
                                
                                vectors.append(vector.reshape(-1))
                                ids.append(numeric_id)
                                id_mapping[memory_id] = numeric_id
                                next_id += 1
                            else:
                                logger.warning(f"Invalid index {idx} for memory ID {memory_id}, skipping")
                    except Exception as e:
                        logger.error(f"Error during direct vector access: {str(e)}")
                        return False
                else:
                    logger.error("Cannot extract vectors from current index type for migration")
                    return False
                    
            # Check if we successfully extracted vectors
            if not vectors:
                logger.error("Failed to extract any vectors for migration")
                return False
                
            logger.info(f"Extracted {len(vectors)} vectors for migration")
            
            # Create a new IndexIDMap with CPU backend (GPU doesn't support add_with_ids)
            self._initialize_index(force_cpu=True, use_id_map=True)
            
            # Add all vectors with their IDs
            if vectors and ids:
                # Convert to numpy arrays
                vectors_array = np.vstack(vectors).astype(np.float32)
                ids_array = np.array(ids, dtype=np.int64)
                
                # Add to the new index
                self.index.add_with_ids(vectors_array, ids_array)
                
                # Update the ID mapping
                self.id_to_index = id_mapping
                
                # Verify the migration was successful
                if self.index.ntotal != len(vectors):
                    logger.error(f"Migration verification failed: expected {len(vectors)} vectors, got {self.index.ntotal}")
                    return False
                    
                # Backup ID mapping after successful migration
                self._backup_id_mapping()
                
                logger.info(f"Successfully migrated {self.index.ntotal} vectors to IndexIDMap")
                return True
            else:
                logger.error("No vectors to migrate")
                return False
                
        except Exception as e:
            logger.error(f"Error migrating to IndexIDMap: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def recreate_mapping(self) -> bool:
        """Recreate the ID mapping from persistent storage.
        
        This is useful when the index is intact but the ID mapping is lost or corrupted.
        It attempts to reconstruct the ID mappings by:
        1. Reading the backup mapping file if available
        2. Reading memory IDs from persistence layer
        3. Generating consistent numeric IDs for all memories
        
        Returns:
            bool: True if reconstruction was successful, False otherwise
        """
        try:
            logger.info("Starting ID mapping reconstruction...")
            
            # First check if we're using IndexIDMap
            if not hasattr(self.index, 'id_map'):
                logger.warning("Index is not using IndexIDMap, migrating first...")
                success = self.migrate_to_idmap()
                if not success:
                    logger.error("Migration to IndexIDMap failed, cannot recreate mapping.")
                    return False
            
            # 1. Try to read the backup mapping file first
            mapping_path = os.path.join(self.storage_path, 'faiss_index.bin' + '.mapping.json')
            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r') as f:
                        mapping_data = json.load(f)
                        
                    if isinstance(mapping_data, dict):
                        # Convert string keys to appropriate types if needed
                        self.id_to_index = {k: int(v) if isinstance(v, str) and v.isdigit() else v 
                                           for k, v in mapping_data.items()}
                        logger.info(f"Loaded {len(self.id_to_index)} ID mappings from backup file")
                        return True
                    else:
                        logger.warning("Mapping file has invalid format, cannot use it for reconstruction.")
                except Exception as e:
                    logger.error(f"Error reading mapping file: {str(e)}")
            else:
                logger.warning("No mapping backup file found, will try to reconstruct from memory directories.")
                
            # If we have no mappings at this point, we need to rebuild from scratch
            if len(self.id_to_index) == 0:
                # 2. Try to reconstruct from memory directories
                logger.info("Attempting to reconstruct ID mapping from memory directories...")
                memory_path = os.path.join(self.storage_path, 'memories')
                
                if not os.path.exists(memory_path):
                    # Look one level up
                    parent_path = os.path.dirname(self.storage_path)
                    potential_memory_path = os.path.join(parent_path, 'memories')
                    if os.path.exists(potential_memory_path):
                        memory_path = potential_memory_path
                    else:
                        # Search for memories directory
                        for root, dirs, _ in os.walk(os.path.dirname(self.storage_path)):
                            if 'memories' in dirs:
                                memory_path = os.path.join(root, 'memories')
                                logger.info(f"Found memories directory at {memory_path}")
                                break
                
                if os.path.exists(memory_path):
                    try:
                        # Scan memory directory for memory files
                        memory_ids = []
                        for root, _, files in os.walk(memory_path):
                            for file in files:
                                if file.endswith('.json') and file.startswith('mem_'):
                                    memory_id = file.split('.')[0]  # Remove .json extension
                                    memory_ids.append(memory_id)
                        
                        logger.info(f"Found {len(memory_ids)} memory files")
                        
                        # Generate numeric IDs for all memory IDs
                        total_count = self.index.ntotal
                        logger.info(f"FAISS index contains {total_count} vectors")
                        
                        # Two strategies - try both for best results:
                        # 1. Generate consistent IDs for all memories and update mapping
                        new_mapping = {}
                        for memory_id in memory_ids:
                            numeric_id = self._get_numeric_id(memory_id)
                            new_mapping[memory_id] = numeric_id
                        
                        # Only update if we found a reasonable number of memories
                        # and not wildly more than the vectors in the index
                        if 0 < len(new_mapping) <= total_count * 1.5:  # Allow some buffer
                            self.id_to_index = new_mapping
                            logger.info(f"Reconstructed {len(self.id_to_index)} ID mappings from memory files")
                            
                            # Backup the reconstructed mapping
                            self._backup_id_mapping()
                            return True
                        else:
                            logger.warning(f"Memory count ({len(new_mapping)}) doesn't match index count ({total_count})")
                    except Exception as e:
                        logger.error(f"Error scanning memory directory: {str(e)}")
                else:
                    logger.warning(f"Memories directory not found at {memory_path}, cannot reconstruct from files")
                    
                # 3. Last-resort fallback: Auto-generate sequential mappings
                if total_count > 0:
                    logger.warning("Using last-resort fallback: Generating sequential ID mappings")
                    # This is a complete guess but might recover some functionality
                    # Generate UUIDs for the vectors in the index
                    for i in range(total_count):
                        memory_id = f"mem_{uuid.uuid4().hex[:12]}"
                        self.id_to_index[memory_id] = i
                    
                    logger.warning(f"Generated {len(self.id_to_index)} sequential mappings - CAUTION: these are not the original IDs")
                    # Backup these mappings for future use
                    self._backup_id_mapping()
                    return True
            
            return len(self.id_to_index) > 0
            
        except Exception as e:
            logger.error(f"Error recreating mapping: {str(e)}")
            return False
            
    def destroy_index(self) -> bool:
        """Completely remove the index from storage and memory.
        
        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Clear in-memory structures
            self.id_to_index = {}
            
            # Reset the index
            if self.index_type.upper() == 'L2':
                self.index = faiss.IndexFlatL2(self.embedding_dim)
            else:  # IP or Cosine
                self.index = faiss.IndexFlatIP(self.embedding_dim)
                
            # If the index file exists, remove it
            index_path = os.path.join(self.storage_path, 'faiss_index.bin')
            if os.path.exists(index_path):
                os.remove(index_path)
                
            # Also remove mapping file if it exists
            mapping_path = os.path.join(self.storage_path, 'faiss_index.bin' + '.mapping.json')
            if os.path.exists(mapping_path):
                os.remove(mapping_path)
                
            logger.info("Index successfully destroyed and reset")
            return True
            
        except Exception as e:
            logger.error(f"Error destroying index: {str(e)}")
            return False

    def _get_numeric_id(self, memory_id: str) -> int:
        """Generate a consistent numeric ID from a memory_id string.
        
        This ensures that each memory_id always maps to the same numeric ID,
        which is important for index consistency across restarts.
        
        Args:
            memory_id: The string memory ID
            
        Returns:
            int: A numeric ID derived from the memory_id
        """
        # Convert memory_id to a numeric ID using a hash function
        # We use a large prime to reduce collision chances
        # Note: we mask to 63 bits to avoid int64 overflow issues
        numeric_id = int(hashlib.md5(memory_id.encode()).hexdigest(), 16) % (2**63-1)
        return numeric_id

```

